import os
import re
import json
import logging
import requests
from time import sleep
from typing import Any
from collections.abc import Iterator
from requests.adapters import HTTPAdapter, Retry

from interlinked.core.tool import Tool
from interlinked.core.config import Config
from interlinked.core.utilities import Utilities
from interlinked.core.clients.baseaiclient import BaseAIClient, AIModel, AIClientResponse, ToolCall

logger = logging.getLogger(__name__)


class OllamaClient(BaseAIClient):
	"""
	Ollama client to interact with local AI models

	Get Ollama from [here](ollama.com)
	"""

	ROLE_USER: str = 'user'
	ROLE_TOOL: str = 'tool'
	ROLE_SYSTEM: str = 'system'
	ROLE_ASSISTANT: str = 'assistant'

	# how long, in seconds, this client should wait for responses
	TIMEOUT: int = 1200

	# the maximum number of retries
	MAX_RETRIES: int = 4
	KEEP_ALIVE: str = '20m'

	model_name: str = None
	options: dict[str, Any] = None
	embedding_model_name: str = None

	def __init__(self, model_name: str, embedding_model_name: str = None,
				 base_url: str = None, options: dict[str, Any] = None):
		"""
		Initializes this AI client to use with `AI.ask(…, client=OllamaClient(…))`

		@param model_name(str): The default model name to use to chat (e.g. `mixtral`)
		@param embedding_model_name(str): (optional) The default model name to use to embeddings
		@param base_url(str): (optional) If you host Ollama in Bolt, SimCloud, or a server, this would be the URL + port
		@param options(dict): (optional) Any default options you'd like to set for the model (github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)
		"""

		# prefix `base_url` if it doesn't have `http://…`
		if base_url and not base_url.startswith('http'):
			base_url = f'http://{base_url}'

		self.model_name = model_name
		self.base_url = base_url or Config.current.OLLAMA_BASE_URL
		self.embedding_model_name = embedding_model_name or model_name
		self.configure_session()

		if not self.model_name:
			raise Exception('please pass `OllamaClient(model_name=\'…\')')

		elif re.findall(r'llama[0-2]+', self.model_name):
			raise Exception('llama models (version 1 through 2) are not approved to be used internally due to its restrictive license.'
							' Try `mistral`, `mixtral`, `gemma`, and any model you have approval for')

		self.options = {**{

			'top_p': 0.9,
			'temperature': 0.6,
			'frequency_penalty': 1,
		}, **(options or {})}

		# make sure Ollama is installed and server is active
		try:
			self.get_models()

		except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError) as exception:
			logger.warning('⚠️ Ollama server is not available. To use on-device models, please install and run Ollama from https://ollama.ai')

	def configure_session(self) -> None:
		"""
		[Internal]
		Configures the session
		"""

		self.session = requests.session()

		retry: Retry = Retry(total=8, backoff_factor=1, connect=1, status_forcelist=[502, 503, 504])
		self.session.mount('https://', HTTPAdapter(max_retries=retry))

	def get_models(self, dsid: int = None, _retry_count: int = 0) -> list[AIModel]:
		"""
		Returns a list of `AIModel` containing models available
		"""

		cookies: dict[str, Any] = None
		headers: dict[str, Any] = {'Content-Type': 'application/json'}

		response = self.session.get(f'{self.base_url}/api/tags', headers=headers, cookies=cookies, timeout=5)

		if not response.ok:
			logger.error(f'could not get models: {response.text=}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		return [AIModel(name=model.get('name'), description=model.get('details').get('parameter_size'), raw=model)
				for model in response_json.get('models')]

	def pull_model(self, model_name: str, insecure: bool = False) -> None:
		"""
		Download a model from Ollama. Called automatically by `AI.ask(…)` if you use a model that
		you haven't downloaded yet

		@param model_name(str): The name of the model to pull (e.g. `mixtral`)
		@param insecure(bool): (Optional) Whether to allow insecure connections (useful for development)
		"""

		url: str = f'{self.base_url}/api/pull'
		payload: dict[str, Any] = {'name': model_name, 'insecure': insecure}
		headers: dict[str, str] = {'Content-Type': 'application/json'}

		response = self.session.post(url, json=payload, headers=headers, timeout=None)

		if not response.ok:
			logger.error(response.text)

		response.raise_for_status()

		# if the model name is invalid, the response may not raise a non-200 HTTP error
		if 'does not exist' in response.text:
			raise Exception(f'"{model_name}" is not a valid model name')

	@Utilities.dynamic
	def get_response(self, messages: str|list[dict], tools: list[Tool] = None, model_name: str = None,
					 temperature: float = None, stream: bool = False, format_: str = None, _retry_count: int = 0,
					 **kwargs) -> AIClientResponse | Iterator[AIClientResponse]:
		"""
		Returns chat response for a given messages.
		It is recommended to use `AI.ask(…, client=OllamaClient(model_name=…))` instead of using this API directly.

		@param messages(str|list): A list of messages or a prompt string (which gets converted to a message)
		@param tools(list): (Optional) A list of functions/tools to give AI access to
		@param model_name(str): (Optional) The name of the model to use. Defaults to the model you use in `AI.ask(…)`
		@param temperature(float): (Optional) The temperature (controls creativeness). Defaults to the value in `AI.ask(…)`,
								   which is typically 0.6
		@param stream(bool): (Optional) Whether the response should be streamed chunk by chunk. When set, this function will return an
							 iterator that yields an `AIClientResponse`, which has the word/chunk (e.g. `observation.content`)
		@param format_(str): (Optional) The desired format of the output (e.g. "json". See available options in `Template`)
		@return (tuple): a response
		"""

		if not model_name:
			model_name = self.model_name

		messages = messages if isinstance(messages, list) else [{'role': self.ROLE_USER, 'content': messages}]

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		supports_tools: bool = kwargs.get('supports_tools', True)

		# if tools are passed, add the tools to the system prompt
		if tools and not supports_tools:

			messages = messages.copy()

			# if there's an existing system prompt, append the tools to it
			if messages[0].get('role') == self.ROLE_SYSTEM:
				messages[0]['content'] = f'{messages[0]}\n\n{self.get_message_for_tools(tools=tools)["content"]}'

			else:
				messages.insert(0, self.get_message_for_tools(tools=tools))

			# change all role: `tool` to `user`
			for message in messages:

				if message.get('role') == self.ROLE_TOOL:
					message['role'] = self.ROLE_USER

		if stream:
			return self.get_response_stream(messages=messages, tools=tools, model_name=model_name, options=options, format_=format_, **kwargs)

		data: dict[str, Any] = {

			'stream': False,
			'options': options,
			'model': model_name,
			'messages': messages,
			'keep_alive': self.KEEP_ALIVE,
			'context': kwargs.get('context'),
		}

		if (tools and not supports_tools) or format_ == 'json':
			data['format'] = 'json'

		if supports_tools and tools:
			data['tools'] = [tool.to_dict(client=self) for tool in tools]

		headers: dict[str, Any] = {'Content-Type': 'application/json'}

		try:
			response = self.session.post(f'{self.base_url}/api/chat', data=json.dumps(data),
										 headers=headers, timeout=self.TIMEOUT, stream=False)

		except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError) as exception:

			if _retry_count <= self.MAX_RETRIES:

				logger.warning('request timed out. trying again…')

				sleep(_retry_count + 5)
				return self.get_response(messages=messages, tools=tools, model_name=model_name, temperature=temperature, stream=stream,
										 format_=format_, _retry_count=_retry_count + 1, **kwargs)

			else:

				logger.error('request timed out too many times')
				raise exception

		if response.status_code == 404 and 'not found' in response.json().get('error') and _retry_count < 2:

			logger.info(f'⬇️ downloading {model_name}…')
			self.pull_model(model_name)

			return self.get_response(messages=messages, tools=tools, model_name=model_name, temperature=temperature, stream=stream,
									 format_=format_, _retry_count=_retry_count + 1, **kwargs)

		elif not response.ok and 'does not support tools' in response.json().get('error'):

			if len(messages) == 1:
				logger.warning(f'ℹ️ "{model_name}" does not officially support `tools=[…]`. results may be unexpected')

			return self.get_response(messages=messages, tools=tools, model_name=model_name, temperature=temperature, stream=stream,
									 format_=format_, _retry_count=_retry_count + 1, supports_tools=False, **kwargs)

		if not response.ok:
			logger.error(f'{response.text=}, {messages=}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		message: dict[str, Any] = response_json.get('message')
		content: str = message.get('content')
		tool_calls: list[ToolCall] = []

		if tools:

			if supports_tools:
				tool_calls = [ToolCall(id=raw_tool_call.get('id'), name=raw_tool_call.get('function', {}).get('name'),
									   raw_arguments=raw_tool_call.get('function', {}).get('arguments')) for raw_tool_call in message.get('tool_calls', [])]

			else:

				if 'name' in content and 'arguments' in content:

					matches: list[str] = re.findall(r'{.*?}', content, flags=re.MULTILINE | re.DOTALL)
					matches = [match for match in matches if 'name' in match and 'arguments' in match]

					for match in matches:

						tool_call_dict: dict[str, Any] = Tool.get_dictionary_from_string(string=match.replace('\'', ''))

						# only add if the tool_call is a dict
						if not isinstance(tool_call_dict, dict):
							raise Exception(f'{model_name} responded with an invalid tool call ({tool_call_dict=}, {content=})')

						tool_calls.append(ToolCall(id=None, name=tool_call_dict.get('name'), raw_arguments=json.dumps(tool_call_dict.get('arguments'))))

		return AIClientResponse(raw=message, content=content, tool_calls=tool_calls)

	def get_response_stream(self, messages: str|list[dict], model_name: str, options: dict[str, Any],
							tools: list[Tool] = None, format_: str = None, _retry_count: int = 0, **kwargs) -> Iterator[AIClientResponse]:
		"""
		[Internal]
		To use streaming, please use `AI.ask(…, stream=True)` or `OllamaClient().get_response(…, stream=True)`

		Calls OllamaClient's /api/chat endpoint and returns raw JSON response
		"""

		if _retry_count > self.MAX_RETRIES:
			raise Exception('retried too many times')

		supports_tools: bool = kwargs.get('supports_tools', True)

		data: dict[str, Any] = {

			'stream': True,
			'options': options,
			'model': model_name,
			'messages': messages,
			'keep_alive': self.KEEP_ALIVE,
			'context': kwargs.get('context'),
			'format': 'json' if (tools and not supports_tools) or format_ == 'json' else None,
		}

		if supports_tools and tools:
			data['tools'] = [tool.to_dict(client=self) for tool in tools]

		headers: dict[str, Any] = {'Content-Type': 'application/json'}
		with self.session.post(f'{self.base_url}/api/chat', data=json.dumps(data),
							   headers=headers, timeout=self.TIMEOUT, stream=True) as response:

			for line in response.iter_lines():

				partial = json.loads(line)

				if error := partial.get('error'):

					# if the model is not downloaded, download it automatically
					if 'not found' in error:

						logger.info(f'⬇️ downloading {model_name}…')
						self.pull_model(model_name=model_name)

						return self.generate_stream(messages=messages, model_name=model_name, options=options,
													_retry_count=_retry_count, **kwargs)

					raise Exception(error)

				# if the response is finished, return so we don't yield an empty response
				if partial.get('done'):
					return

				yield AIClientResponse(raw=partial.get('message'), content=partial.get('message').get('content'), tool_calls=[])

	@Utilities.dynamic
	def get_completion(self, prompt: str, model_name: str = None,
					   temperature: float = None, stream: bool = False, **kwargs) -> AIClientResponse | Iterator[AIClientResponse]:
		"""
		Returns a completion for a given text/code

		@param prompt(str): a text prompt/code to complete
		@param model_name(str): the name of the model to use
		@param temperature(float): the temperature to use (creativeness)
		@param stream(bool): (Optional) Whether the response should be streamed chunk by chunk. When set, this function will return an
							 iterator that yields an `AIClientResponse`, which has the word/chunk (e.g. `observation.content`)
		@return (AIClientResponse): a response (the completion will be in `.content`)
		"""

		if not model_name:
			model_name = self.model_name

		data: dict[str, Any] = {

			'stream': stream,
			'prompt': prompt,
			'model': model_name,
			'keep_alive': self.KEEP_ALIVE,
		}
		headers: dict[str, Any] = {'Content-Type': 'application/json'}

		if stream:

			def _get_completion_stream():

				with self.session.post(f'{self.base_url}/api/generate', data=json.dumps(data),
							   headers=headers, timeout=self.TIMEOUT, stream=True) as response:

					for line in response.iter_lines():

						partial = json.loads(line)

						if error := partial.get('error'):

							# if the model is not downloaded, download it automatically
							if 'not found' in error:

								logger.info(f'⬇️ downloading {model_name}…')
								self.pull_model(model_name=model_name)

								return self.generate_stream(messages=messages, model_name=model_name, options=options,
															_retry_count=_retry_count, **kwargs)

							raise Exception(error)

						# if the response is finished, return so we don't yield an empty response
						if partial.get('done'):
							return

						yield AIClientResponse(raw=partial, content=partial.get('response'))

			return _get_completion_stream()

		response = self.session.post(f'{self.base_url}/api/generate', data=json.dumps(data),
									 headers=headers, timeout=self.TIMEOUT, stream=False)

		if response.status_code == 404 and 'not found' in response.json().get('error'):

			logger.info(f'⬇️ downloading {model_name}…')
			self.pull_model(model_name)

			return self.get_completion(prompt=prompt, model_name=model_name, temperature=temperature, **kwargs)

		if not response.ok:
			logger.error(f'{response.text=}, {prompt=}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		content: str = response_json.get('response')

		return AIClientResponse(raw=response_json, content=content)

	def get_embedding(self, input: str, model_name: str = None, _retry_count: int = 0) -> list[float]:
		"""
		Returns embeddings for a given input string

		@param input(str): The text/string you would like to generate embedding for
		@param model_name(str): (optional) The name of the model to use
		"""

		if not model_name:
			model_name = self.embedding_model_name

		data: dict[str, Any] = {

			'prompt': input,
			'model': model_name,
			'keep_alive': self.KEEP_ALIVE,
		}

		headers: dict[str, Any] = {'Content-Type': 'application/json'}

		try:
			response = self.session.post(f'{self.base_url}/api/embeddings',
										 data=json.dumps(data), headers=headers,
										 timeout=self.TIMEOUT)

		except requests.exceptions.ReadTimeout:

			sleep(1)
			return self.get_embedding(input=input, model_name=model_name)

		if not response.ok:
			logger.error(f'{response.text} ({input=})')

		response.raise_for_status()
		return response.json().get('embedding')

	""" Utilities """

	def get_message_for_tools(self, tools: list[Tool]) -> dict[str, Any]:
		"""
		[Internal]
		Returns a system message for a given list of tools
		"""

		tools: str = '\n'.join([json.dumps(tool.to_dict(self), indent=4) for tool in tools])
		message: dict[str, Any] = self.create_message(role=self.ROLE_SYSTEM, content=f'{TOOLS_SYSTEM_PROMPT}\n{tools}')
		return message

	def create_message(self, role: str, content: str, files: list[str|bytes] = None, **kwargs) -> dict[str, Any]:
		"""
		Creates a message that matches the format used by the client that inherits
		this class

		@param role(str): the value of any of the `ROLE_` properties
		@param content(str): the text content of the message
		@param files(list): a list of file paths or bytes
		"""

		message: dict[str, Any] = {'role': role, 'content': content}

		if files:
			message['images'] = [self.encode_file(path=file) if isinstance(file, str) else self.encode_bytes(bytes_=file) for file in files]

		# if this is a message from a tool/function, add the tool arguments
		if role == self.ROLE_TOOL:

			message['name'] = kwargs['name']
			message['tool_call_id'] = kwargs['tool_call_id']

		return message

	""" Ollama-specific functions """

	def fine_tune(self, *args, **kwargs) -> str:
		raise NotImplementedError('Please use MLXClient instead for fine-tuning')

TOOLS_SYSTEM_PROMPT: str =  \
'''
You are an assistant with access to the functions below. If the user's prompt requires using any of the tools below, respond with the exact JSON format below:
{"name": "function_name", "arguments": {}}

If you respond with the above, a function will be called and its response will be returned to you. Your task is to either:
- Repeat the response back to the user or
- Call the next function if necessary

---
'''