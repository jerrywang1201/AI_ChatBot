try:
	from openai import OpenAI

except ImportError:

	# importing `openai` is optional
	class OpenAI:
		pass


import os
import re
import json
import httpx
import base64
import inspect
import logging
import requests
import websockets
from time import sleep
from pydantic import BaseModel
from typing import Any, Callable
from collections.abc import Iterator
from requests.adapters import HTTPAdapter, Retry

from interlinked.core.tool import Tool
from interlinked.core.config import Config
from interlinked.core.utilities import Utilities
from interlinked.core.clients.baseaiclient import BaseAIClient, AIModel, AIClientResponse, ToolCall, Call, File, Event, File

logger = logging.getLogger(__name__)
logging.getLogger('httpx').setLevel(logging.WARNING)


class OpenAIClient(OpenAI, BaseAIClient):
	"""
	OpenAI client to interact with OpenAI's APIs
	"""

	ROLE_USER: str = 'user'
	ROLE_TOOL: str = 'tool'
	ROLE_SYSTEM: str = 'system'
	ROLE_ASSISTANT: str = 'assistant'

	# realtime events
	EVENT_SESSION_UPDATE: str = 'session.update'

	EVENT_TEXT_DONE: str = 'response.text.done'
	EVENT_TEXT_DELTA: str = 'response.text.delta'

	EVENT_AUDIO_DONE: str = 'response.audio.done'
	EVENT_AUDIO_DELTA: str = 'response.audio.delta'

	EVENT_SESSION_CREATED: str = 'session.created'
	EVENT_SESSION_UPDATED: str = 'session.updated'

	EVENT_RESPONSE_DONE: str = 'response.done'
	EVENT_RESPONSE_CANCEL: str = 'response.cancel'
	EVENT_RESPONSE_CREATE: str = 'response.create'
	EVENT_RESPONSE_CREATED: str = 'response.created'

	EVENT_ITEM_CREATE: str = 'conversation.item.create'
	EVENT_ITEM_CREATED: str = 'conversation.item.created'

	EVENT_RATE_LIMITS_UPDATED: str = 'rate_limits.updated'

	EVENT_OUTPUT_ITEM_DONE: str = 'response.output_item.done'
	EVENT_OUTPUT_ITEM_ADDED: str = 'response.output_item.added'

	EVENT_CONTENT_PART_DONE: str = 'response.content_part.done'
	EVENT_CONTENT_PART_ADDED: str = 'response.content_part.added'

	EVENT_AUDIO_TRANSCRIPT_DONE: str = 'response.audio_transcript.done'
	EVENT_AUDIO_TRANSCRIPT_DELTA: str = 'response.audio_transcript.delta'

	EVENT_AUDIO_APPEND: str = 'input_audio_buffer.append'
	EVENT_AUDIO_COMMITTED: str = 'input_audio_buffer.committed'
	EVENT_AUDIO_SPEECH_STARTED: str = 'input_audio_buffer.speech_started'
	EVENT_AUDIO_SPEECH_STOPPED: str = 'input_audio_buffer.speech_stopped'

	EVENT_FUNCTION_CALL_DONE: str = 'response.function_call_arguments.done'
	EVENT_FUNCTION_CALL_DELTA: str = 'response.function_call_arguments.delta'

	EVENT_AUDIO_TRANSCRIPTION_COMPLETED: str = 'conversation.item.input_audio_transcription.completed'

	# Interlinked's built-in types
	EVENT_TOOL_CALL = EVENT_FUNCTION_CALL_DONE

	SKIPPED_EVENT_TYPES: list[str] = {

		EVENT_SESSION_CREATED, EVENT_SESSION_UPDATED,
		EVENT_RESPONSE_CREATED, EVENT_ITEM_CREATED,
		EVENT_RATE_LIMITS_UPDATED, EVENT_OUTPUT_ITEM_ADDED,
		EVENT_CONTENT_PART_ADDED, EVENT_RESPONSE_DONE,
		EVENT_CONTENT_PART_DONE, EVENT_OUTPUT_ITEM_DONE,
		EVENT_TEXT_DONE, EVENT_AUDIO_DONE, EVENT_FUNCTION_CALL_DELTA,
		EVENT_AUDIO_TRANSCRIPT_DONE, EVENT_AUDIO_COMMITTED,
		EVENT_AUDIO_SPEECH_STOPPED, EVENT_AUDIO_TRANSCRIPTION_COMPLETED,
	}

	# the input rate OpenAI uses for audio playback
	INPUT_RATE: int = 24000

	model_name: str = None
	embedding_model_name: str = None

	options: dict[str, Any] = None

	def __init__(self, model_name: str = None, embedding_model_name: str = None,
				 api_key: str = None, base_url: str = None, options: dict[str, Any] = None, http_client: httpx.Client = None, **kwargs):
		"""
		Initializes this AI client to use with `AI.ask(…, client=OpenAIClient(…))`

		@param model_name(str): (optional) The default model name to use to chat
		@param embedding_model_name(str): (optional) The default model name to use to embeddings
		@param base_url(str): (optional) Override the OpenAI backend URL
		@param api_key(str): (optional) Your OpenAI API key. If not set, `INTERLINKED_OPENAI_API_KEY` environment variable will be read
		"""

		if not api_key:
			api_key = Config.current.OPENAI_API_KEY

		if not api_key:
			raise Exception(f'Missing a valid `OpenAIClient(api_key=\'{api_key}\')`')

		self.api_key = api_key
		self.model_name = model_name or 'gpt-4o'
		self.embedding_model_name = embedding_model_name or 'text-embedding-3-large'

		http_client: httpx.Client = http_client

		if not Config.current.is_development:

			if base_url and 'floodgate' in base_url and 'cert' in kwargs and 'dsid' in kwargs:

				headers: dict[str, Any] = {'X-Narrative-Dsid': kwargs['dsid']}

				if _headers := kwargs.get('headers'):
					headers = {**headers, **_headers}

				http_client = httpx.Client(base_url=base_url, cert=kwargs['cert'], headers=headers)

			else:

				http_client = httpx.Client(mounts={
					'http://': httpx.HTTPTransport(proxy='http://dps.iso.apple.com:80'),
					'https://': httpx.HTTPTransport(proxy='http://dps.iso.apple.com:443')
				})

		self.options = {**{'temperature': 0.6}, **(options or {})}

		self.configure_session()
		super(OpenAIClient, self).__init__(api_key=api_key, http_client=http_client, base_url=base_url)

	def configure_session(self) -> None:
		"""
		[Internal]
		Configures the session
		"""

		self.session = requests.session()

		retry: Retry = Retry(total=8, backoff_factor=1, status_forcelist=[502, 503, 504])
		self.session.mount('https://', HTTPAdapter(max_retries=retry))

	def get_models(self, dsid: int = None) -> list[AIModel]:
		"""
		Returns a list of `AIModel` containing models available
		"""
		return [AIModel(name=model.id, is_fine_tuned=model.id.startswith('ft:'), raw=model) for model in self.models.list() if model.id.startswith((
			'gpt-', 'ft:', 'aws:'))]

	def get_embedding(self, input: str|list[str], model_name: str = None) -> list[float]|list[list[float]]:
		"""
		Returns embeddings for a given input string

		@param input(str): The text/string you would like to generate embedding for
		@param model_name(str): (optional) The name of the model to use
		"""

		if not model_name:
			model_name = self.embedding_model_name

		data: list[dict] = self.embeddings.create(input=input, model=model_name).data

		if isinstance(input, list):
			return [_data.embedding for _data in data]

		return data[0].embedding

	def get_response(self, messages: str|list[dict], tools: list[Tool] = None, model_name: str = None,
					 temperature: float = None, stream: bool = False, format_: str | dict | BaseModel = None,
					 output_media: bool = False, **kwargs) -> AIClientResponse:
		"""
		Returns chat response for a given messages

		@param messages(str|list): A list of messages or a prompt string (which gets converted to a message)
		@param tools(list): (Optional) A list of functions/tools to give AI access to
		@param model_name(str): (Optional) The name of the model to use. Defaults to the model you use in `AI.ask(…)`
		@param temperature(float): (Optional) The temperature (controls creativeness). Defaults to the value in `AI.ask(…)`,
								   which is typically 0.6
		@param format_(str|dict|BaseModel): (Optional) if set, the response will be in JSON
		@param modalities(list): (Optional) Control the output type (e.g. `text`, `audio`)
		@return (tuple): a response
		"""
		if not model_name:
			model_name = self.model_name

		if 'dall' in model_name:
			return self.get_response_media(messages=messages, model_name=model_name, **kwargs)

		messages = messages if isinstance(messages, list) else [self.create_message(role=self.ROLE_USER, content=messages)]

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		is_base_model: bool = False

		if format_:

			if isinstance(format_, dict):
				options['response_format'] = format_

			elif inspect.isclass(format_) and issubclass(format_, BaseModel):

				is_base_model = True
				options['response_format'] = format_

			elif format_ == 'json':
				options['response_format'] = {'type': 'json_object'}

		# modalities + audio
		if 'modalities' in kwargs:
			options['modalities'] = kwargs.get('modalities')

		if 'audio' in kwargs:
			options['audio'] = kwargs.get('audio')

		# `o*-` models do not support `temperature`
		if ('temperature' in options and options['temperature'] is None) or re.findall(r'^o[1-9]+.*', model_name) or 'search' in model_name:
			del options['temperature']

		# add `tools` to `options`
		if tools:
			options['tools'] = [tool.to_dict(self) for tool in tools]

		if stream:

			# not supported yet with `BaseModel`
			if is_base_model:
				raise NotImplementedError('Streaming with a `BaseModel` is not supported yet')

			def _get_ai_client_response_for_generator(generator: Iterator) -> AIClientResponse:
				"""
				Internal function that converts OpenAI response to `AIClientResponse`
				"""

				tool_calls: list[ToolCall] = []
				first_delta: Any = None

				for completion in generator:

					# if the response is finished, return so we don't yield an empty response
					if completion.choices[0].finish_reason == 'stop':
						return

					delta = completion.choices[0].delta

					# if we have tool calls, the first one may only
					# have a name with no arguments, so we cache it and wait for all arguments
					if raw_tool_calls := delta.tool_calls:

						if len(raw_tool_calls) > 1:
							raise NotImplementedError('missing implementation for streaming with multiple tool calls')

						raw_tool_call: Any = raw_tool_calls[0]
						tool_call: ToolCall = tool_calls[-1] if tool_calls else ToolCall(name=None, raw_arguments='')

						if not tool_call.name and raw_tool_call.function.name:

							tool_call.id = raw_tool_call.id
							tool_call.name = raw_tool_call.function.name
							first_delta = delta

							# streamed deltas with tool calls don't have a role for some reason
							if not first_delta.role:
								first_delta.role = self.ROLE_ASSISTANT

						if (not raw_tool_call.function.name or tool_call.name == raw_tool_call.function.name) and raw_tool_call.function.arguments:
							tool_call.raw_arguments = f'{tool_call.raw_arguments}{raw_tool_call.function.arguments}'

						if tool_calls:
							tool_calls[-1] = tool_call
						else:
							tool_calls = [tool_call]

					else:

						yield AIClientResponse(raw=(first_delta or delta).model_dump(exclude_none=True), content=(first_delta or delta).content or '', tool_calls=tool_calls)
						first_delta = None

			generator: Iterator = self.chat.completions.create(messages=messages, model=model_name, stream=stream, **options)
			return _get_ai_client_response_for_generator(generator=generator)

		message: Any = None

		if is_base_model:

			completion = self.chat.completions.parse(messages=messages, model=model_name, **options)
			message = completion.choices[0].message

		else:

			completion = self.chat.completions.create(messages=messages, model=model_name, **options)
			message = completion.choices[0].message

		# when setting raw, exclude `None` values so we don't send them back to the API in replies
		# since the API does not allow `None` values
		return self.get_ai_client_response_for_message(message=message.model_dump(exclude_none=True))

	def get_response_media(self, messages: str|list[dict], model_name: str = None,
							_retry_count: int = 0, **kwargs) -> AIClientResponse:
		"""
		Returns media generated by model for given message like image generation

		@param messages(str|list): a messages or string
		@param model_name(str): the name of the model to use
		@return (tuple): a response
		"""
		if _retry_count > self.MAX_RETRIES:
			raise Exception('Retried too many times')

		message = messages[-1]['content'] if isinstance(messages, list) else messages
		options: dict[str, Any] = {**{'size': '1024x1024', 'quality': 'standard', 'n': 1}, **(self.options or {})}

		if 'temperature' in options:
			del options['temperature']

		response: dict[str, Any] = self.images.generate(model=model_name, prompt=message, **options)
		return self.get_ai_client_response_for_media(message=response)

	def fine_tune(self, data: list[dict[str, Any]], validation_data: list[dict[str, Any]] = None,
				  model_name: str = None, options: dict[str, Any] = None) -> str:
		"""
		[Internal]
		Fine-tunes an on-device model.
		Use `AI.learn(…, fine_tune=True)` instead.

		@param data(list): A list of messages (user question and response).
		@param validation_data(list): (Optional) A list of messages (user question and response) to use
									  for validating the fine-tuned model.
		@param model_name(str): (Optional) The name of the model to fine-tune. Defaults to the one from `OpenAIClient(model_name=…)`
		@param options(dict): (Optional) Custom arguments to pass to OpenAI (such as `grad_checkpoint`)
		@return (str): The name of the new fine-tuned model with the adapter path
		"""

		import io
		from datetime import datetime

		model_name = model_name or self.model_name
		file_id: str = self.files.create(file=io.BytesIO(('\n'.join(json.dumps(line) for line in data)).encode()), purpose='fine-tune').id
		fine_tuning_job: Any = self.fine_tuning.jobs.create(training_file=file_id, model=model_name,
															**(self.options.get('fine_tune', {})), **(options or {}))

		if not fine_tuning_job or (fine_tuning_job.error and fine_tuning_job.error.code):
			raise Exception(f'Could not fine-tune "{model_name}": {fine_tuning_job.error.message if fine_tuning_job else "Unknown"}')

		logger.section(f'fine-tuning {model_name}…')
		logger.info(f'check live status on platform.openai.com/finetune/{fine_tuning_job.id}')

		while not fine_tuning_job.finished_at and not (fine_tuning_job.error and fine_tuning_job.error.code):

			fine_tuning_job = self.fine_tuning.jobs.retrieve(fine_tuning_job.id)

			if fine_tuning_job.estimated_finish:
				logging.info(f'finishing in {Utilities.get_time_until(target_time=datetime.utcfromtimestamp(fine_tuning_job.estimated_finish))}…')

			sleep(5)

		if fine_tuning_job.error and fine_tuning_job.error.code:
			raise Exception(f'Could not fine-tune "{model_name}": {fine_tuning_job.error.message}')

		timeout: int = 0

		while not fine_tuning_job.fine_tuned_model:

			if timeout > 10:
				break

			sleep(2)
			timeout += 1

		return fine_tuning_job.fine_tuned_model

	""" Realtime """

	async def start_call(self, model_name: str = None,
						 tools: list[Tool] = None, options: dict = None) -> Call:
		"""
		Starts a Realtime session/call (a call with AI)

		@param model_name(str): The name of the model to use (e.g. gpt-4o-realtime-preview)
		@param options(dict): Session configuration
		"""

		options = options or self.options
		model_name = model_name or self.model_name

		url: str = str(self.base_url).replace('https://', 'wss://')
		url = f'{url}/realtime?model={model_name}'

		headers: dict[str, Any] = {

			'OpenAI-Beta': 'realtime=v1',
			'Authorization': f'Bearer {self.api_key}',
		}

		call: Call = Call(websocket=await websockets.connect(url, proxy=None if Config.current.is_development else 'http://dps.iso.apple.com:443',
															 additional_headers=headers), client=self)

		# add tools/Function Calling (if any)
		if tools:

			if not options:
				options = {}

			options['tools'] = [tool.to_dict(self) for tool in tools]
			options['tools'] = [{'type': 'function', **tool['function']} for tool in options['tools']]

		if options:
			await call.send(event={'type': self.EVENT_SESSION_UPDATE, 'session': options})

		return call

	""" Utilities """

	def get_ai_client_response_for_message(self, message: dict[str, Any]) -> AIClientResponse:
		"""
		[Internal]
		Converts a raw response message to `AIClientResponse`

		@param message(dict): e.g. `{'role': 'user', 'content': 'Lorem ipsum'}`
		@return (AIClientResponse): A response
		"""

		audio: dict = message.get('audio')
		content: str = message.get('content')

		if audio:

			# there is no content when the message has audio
			if not content:
				content = audio.get('transcript')

		if content:

			content = content.replace('As a language model, ', '')
			content = content.replace('As an AI language model, ', '')
			content = content.replace(', but as an AI language model', '')

		tool_calls: list[ToolCall] = [ToolCall(id=raw_tool_call.get('id'), name=raw_tool_call.get('function', {}).get('name'), raw_arguments=raw_tool_call.get('function', {}).get('arguments'))
									  for raw_tool_call in message.get('tool_calls', [])]

		# there is a bug in the OpenAI API where it sometimes returns an internal `ToolCall`
		# instead of the actual functions. We work around this bug by replacing it with the actual functions
		has_invalid_function: bool = False
		INVALID_TOOL_NAME: str = 'multi_tool_use.parallel'

		for tool_call in tool_calls[:]:

			if tool_call.name == INVALID_TOOL_NAME:

				has_invalid_function = True

				for raw_tool_call_index, raw_tool_call in enumerate(tool_call.arguments.get('tool_uses', [])):

					new_tool_call: ToolCall = ToolCall(id=f'{tool_call.id}_{raw_tool_call_index}',
													   name=raw_tool_call.get('recipient_name', '').split('functions.')[-1],
													   raw_arguments=json.dumps(raw_tool_call.get('parameters', {})))
					tool_calls.append(new_tool_call)

					# append the tool call to the raw message (which gets passed back to the API later)
					message['tool_calls'].append({'id': new_tool_call.id, 'type': 'function',
												  'function': {'arguments': new_tool_call.raw_arguments, 'name': new_tool_call.name}})

		if has_invalid_function:

			# remove the invalid `ToolCall`
			tool_calls = [tool_call for tool_call in tool_calls if tool_call.name != INVALID_TOOL_NAME]
			message['tool_calls'] = [raw_tool_call for raw_tool_call in message['tool_calls'] if raw_tool_call.get('function', {}).get('name') != INVALID_TOOL_NAME]

		# handle file responses
		files: list[bytes] = []

		if audio:

			file: File = File(id=audio.get('id'), content=base64.b64decode(audio.get('data')), raw=audio)
			files.append(file)

		return AIClientResponse(raw=message, content=content, tool_calls=tool_calls, files=files)

	def get_ai_client_event_for_event(self, raw: dict|str) -> Event:
		"""
		Creates an Event given a raw event

		@param raw(dict|str): A raw event from the OpenAI API (via `.start_call()`)
		"""

		if isinstance(raw, str):
			raw = json.loads(raw)

		type: str = raw.get('type')

		response: dict[str, Any] = raw.get('response', {})
		status_details: dict[str, Any] = response.get('status_details', {}) if response else None
		error: dict|str = status_details.get('error') if status_details else None

		tool_call: ToolCall = None

		if type == self.EVENT_FUNCTION_CALL_DONE:
			tool_call = ToolCall(id=raw.get('call_id'), name=raw.get('name'), raw_arguments=raw.get('arguments'))

		return Event(id=response.get('id'), type=type, tool_calls=[tool_call] if tool_call else None, error=error, raw=raw,
					 is_speech_started=type == self.EVENT_AUDIO_SPEECH_STARTED,
					 is_speech_stopped=type == self.EVENT_AUDIO_SPEECH_STOPPED,
					 text=raw.get('delta') if type in {self.EVENT_TEXT_DELTA, self.EVENT_AUDIO_TRANSCRIPT_DELTA} else None,
					 audio_bytes=base64.b64decode(raw.get('delta')) if type == self.EVENT_AUDIO_DELTA else None)

	def get_ai_client_response_for_media(self, message: dict[str, Any]) -> AIClientResponse:
		"""
		Converts a raw response message to `AIClientResponse`

		@param message(dict): e.g. `{'data': [{'url': '...'}] }`
		@return (AIClientResponse): a response
		"""

		files: list[File] = []
		for data in message.data:

			if url := data.url:

				response = requests.get(url)

				if response.ok:

					content_type = response.headers.get("Content-Type")
					encoded_image = base64.b64encode(response.content).decode("utf-8")
					content = base64.b64decode(encoded_image)
					raw: str = response.content
					files.append(File(raw=raw, content=content, content_type=content_type, id=url))

		return AIClientResponse(raw=message, files=files)

	def create_message(self, role: str, content: str, files: list[str|bytes] = None, **kwargs) -> dict[str, Any]:
		"""
		Creates a message that matches the format used by OpenAI

		@param role(str): the value of any of the `ROLE_` properties
		@param content(str): the text content of the message
		@param realtime(bool): If this is a Realtime/call
		@param files(list): a list of file paths or bytes
		"""

		message: dict[str, Any] = {'role': role}

		if files:

			# change content type to a list (which contains the message text and files)
			content: list[dict[str, Any]] = [{'type': 'text', 'text': content}]

			for file in files:

				if isinstance(file, str):

					file_base64: str = self.encode_file(path=file)

					if file.lower().endswith('.pdf'):

						file_name = os.path.basename(file)
						file_data: dict[str, Any] = {
							'type': 'file',
							'file': {
								'filename': file_name,
								'file_data': f'data:application/pdf;base64,{file_base64}'
							}
						}

					else:

						file_data: dict[str, Any] = {
							'type': 'image_url',
							'image_url': {
								'url': f'data:image/jpeg;base64,{file_base64}'
							}
						}

				else:

					file_base64: str = self.encode_bytes(bytes_=file)

					if file[:4] == b'%PDF':

						file_data: dict[str, Any] = {
							'type': 'file',
							'file': {
								'filename': 'file.pdf',
								'file_data': f'data:application/pdf;base64,{file_base64}'
							}
						}

					else:

						file_data: dict[str, Any] = {
							'type': 'image_url',
							'image_url': {
								'url': f'data:image/jpeg;base64,{file_base64}'
							}
						}

				content.append(file_data)

		# if this is a message from a tool/function, add the tool arguments
		if role == self.ROLE_TOOL:

			message['name'] = kwargs['name']
			message['tool_call_id'] = kwargs['tool_call_id']

		message['content'] = content
		return message

	def create_events(self, type: str, is_partial: bool = False, **kwargs) -> list[dict[str, Any]]:
		"""
		Creates events that matches the format used by OpenAI

		@param type(str): The type of message
		"""

		events: list[Event] = []

		# tool/function response
		if type == self.EVENT_TOOL_RESPONSE:

			events.append(Event(raw={

				'type': self.EVENT_ITEM_CREATE,
				'item': {

					'type': 'function_call_output',
					'output': kwargs.get('content'),
					'call_id': kwargs.get('tool_call').id,
				}
			}, type=type))

		# reply to AI via text
		elif type == self.EVENT_ITEM_CREATE:

			events.append(Event(raw={

				'type': self.EVENT_ITEM_CREATE,
				'item': {

					'type': 'message',
					'role': kwargs.get('role'),
					'content': [{'type': 'input_text', 'text': kwargs.get('content')}]
				}
			}, type=type))

		# stream audio to AI
		elif type == self.EVENT_AUDIO_APPEND:

			events.append(Event(raw={

				'type': self.EVENT_AUDIO_APPEND,
				'audio': base64.b64encode(kwargs.get('audio_bytes')).decode()
			}, type=type))

		# cancel response (interrupt AI's response)
		elif type == self.EVENT_RESPONSE_CANCEL:
			return [Event(raw={'type': self.EVENT_RESPONSE_CANCEL}, type=type)]

		else:
			raise NotImplementedError(f'Unsupported event type ({type=}, {kwargs=})')

		# if we're not partial (not adding any more events), add the done event
		if not is_partial:
			events.append(Event(raw={'type': self.EVENT_RESPONSE_CREATE}, type=self.EVENT_RESPONSE_CREATE))

		return events

	def get_should_skip_event(self, event: Event) -> bool:
		"""
		Returns whether `AI.ask(…)` should skip a given `Event`

		Some events are irrelevant for the user to listen to

		@param event(Event): the event from the AI model
		@return (bool): Whether the user should skip this event (not an important event)
		"""
		return event.type in self.SKIPPED_EVENT_TYPES

