import os
import re
import json
import uuid
import random
import logging

import requests
from time import sleep
from typing import Any
from pathlib import Path
from collections.abc import Iterator
from requests.adapters import HTTPAdapter, Retry

from interlinked.core.tool import Tool
from interlinked.core.config import Config
from interlinked.core.utilities import Utilities
from interlinked.core.clients.baseaiclient import BaseAIClient, AIModel, AIClientResponse, ToolCall

logger = logging.getLogger(__name__)


class MLXClient(BaseAIClient):
	"""
	MLX client to interact with local AI models
	"""

	ROLE_USER: str = 'user'
	ROLE_TOOL: str = 'tool'
	ROLE_SYSTEM: str = 'system'
	ROLE_ASSISTANT: str = 'assistant'

	STOP_WORDS: list[str] = ['<|end|>', '<|user|>', '<|assistant|>']

	MODELS_PATH: str = f'{Path.home()}/.cache/huggingface/hub'

	# how long, in seconds, this client should wait for responses
	TIMEOUT: int = 1200

	model_name: str = None
	adapter_path: str = None
	options: dict[str, Any] = None
	embedding_model_name: str = None

	def __init__(self, model_name: str, embedding_model_name: str = None,
				 base_url: str = None, options: dict[str, Any] = None):
		"""
		Initializes this AI client to use with `AI.ask(â€¦, client=MLXClient(â€¦))`

		@param model_name(str): the default model name to use to chat
		@param embedding_model_name(str): (optional) the default model name to use to embeddings
		@param base_url(str): (optional) If you host MLXServer in Bolt, SimCloud, or a server, this would be the URL + port
		@param options(dict): (optional) any default options you'd like to set for the model (github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/SERVER.md)
		"""

		if not model_name:
			raise Exception('please pass `MLXClient(model_name=\'â€¦\')')

		if model_name.startswith('fine-tuned--'):

			self.adapter_path = f'{self.MODELS_PATH}/{model_name.replace("/", "--")}'
			model_name = model_name.replace('fine-tuned--', '').replace('--', '/').rsplit('-', 1)[0]

		self.model_name = model_name
		self.base_url = base_url or Config.current.MLX_BASE_URL
		self.embedding_model_name = embedding_model_name or model_name
		self.configure_session()

		self.options = {**{

			'top_p': 0.9,
			'temperature': 0.6,
			'stop': self.STOP_WORDS,
			'repetition_penalty': 1.0,

		}, **(options or {})}

	def configure_session(self) -> None:
		"""
		[Internal]
		Configures the session
		"""

		self.session = requests.session()

		retry: Retry = Retry(total=8, backoff_factor=1, status_forcelist=[502, 503, 504])
		self.session.mount('https://', HTTPAdapter(max_retries=retry))

	def get_models(self) -> list[AIModel]:
		"""
		Returns a list of `AIModel` containing models available
		"""

		models: list[AIModel] = []

		for model_name in os.listdir(self.MODELS_PATH):

			if not model_name.startswith('models--'):
				continue

			model_name = model_name.replace('models--', '')
			model_name = model_name.replace('--', '/')
			models.append(AIModel(name=model_name))

		# include fine-tuned models (adapters)
		for adapter_name in os.listdir(self.MODELS_PATH):

			if not adapter_name.startswith('fine-tuned--'):
				continue

			adapter_name = adapter_name.replace('fine-tuned--', '')
			model_name = adapter_name.replace('--', '/')
			models.append(AIModel(name=f'fine-tuned--{model_name}', is_fine_tuned=True))

		return models
 
	def pull_model(self, model_name: str) -> None:
		"""
		Download a model from HuggingFace

		@param model_name: Name of the model to pull
		"""

		from huggingface_hub import snapshot_download
		snapshot_download(model_name)

	@Utilities.dynamic
	def get_response(self, messages: str|list[dict], tools: list[dict[str, Any]] = None, model_name: str = None,
					 temperature: float = None, stream: bool = False, format_: str = None, _retry_count: int = 0,
					 **kwargs) -> AIClientResponse | Iterator[AIClientResponse]:
		"""
		Returns chat response for a given messages

		@param messages(str|list): messages or string
		@param model_name(str): the name of the model to use
		@return (tuple): a response
		"""

		if not model_name:
			model_name = self.model_name

		messages = messages if isinstance(messages, list) else [{'role': self.ROLE_USER, 'content': messages}]

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		if stream:
			return self.get_response_stream(messages=messages, model_name=model_name, options=options, format_=format_, **kwargs)

		data: dict[str, Any] = {

			'stream': False,
			'model': model_name,
			'messages': messages,
			**options
		}

		if tools:
			data['tools'] = [tool.to_dict(client=self) for tool in tools]

		headers: dict[str, Any] = {'Content-Type': 'application/json'}

		try:
			response = self.session.post(f'{self.base_url}/v1/chat/completions', data=json.dumps(data),
										 headers=headers, timeout=self.TIMEOUT, stream=False)

		except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError) as exception:

			if _retry_count <= 3:

				if _retry_count > 1:
					logger.warning('request timed out. trying againâ€¦')

				sleep(_retry_count + 5)
				return self.get_response(messages=messages, tools=tools, model_name=model_name, temperature=temperature, stream=stream,
										 format_=format_, _retry_count=_retry_count + 1, **kwargs)

			else:

				logger.error('request timed out too many times')
				raise exception

		if not response.ok:
			logger.error(f'{response.text=}, {messages=}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()

		message: dict[str, Any] = response_json.get('choices')[0].get('message')
		return self.get_ai_client_response_for_message(message=message)

	def get_response_stream(self, messages: str|list[dict], model_name: str, options: dict[str, Any],
							format_: str = None, _retry_count: int = 0, **kwargs) -> Iterator[AIClientResponse]:
		"""
		[Internal]
		To use streaming, please use `AI.ask(â€¦, stream=True)` or `OllamaClient().get_response(â€¦, stream=True)`

		Calls OllamaClient's /api/chat endpoint and returns raw JSON response
		"""

		if _retry_count > 3:
			raise Exception('retried too many times')

		data: dict[str, Any] = {

			'stream': True,
			'model': model_name,
			'messages': messages,
			**options
		}

		headers: dict[str, Any] = {'Content-Type': 'application/json'}
		with self.session.post(f'{self.base_url}/v1/chat/completions', data=json.dumps(data),
							   headers=headers, timeout=self.TIMEOUT, stream=True) as response:

			for line in response.iter_lines():

				if not line:
					continue

				line = line.decode().replace('data: ', '', 1)

				if line == '[DONE]':
					break

				response_json: dict[str, Any] = None

				try:
					response_json = json.loads(line)

				except Exception as exception:
					raise Exception(line)

				delta: dict[str, Any] = response_json.get('choices')[0].get('delta')
				content: str = delta.get('content')

				if content in self.STOP_WORDS:
					break

				yield AIClientResponse(raw=delta, content=content, tool_calls=[])

	def get_completion(self, prompt: str, model_name: str = None,
					   temperature: float = None, **kwargs) -> AIClientResponse:
		"""
		Returns a completion for a given text/code

		@param prompt(str): a text prompt/code to complete
		@param model_name(str): the name of the model to use
		@param temperature(float): the temperature to use (creativeness)
		@return (AIClientResponse): a response (the completion will be in `.content`)
		"""

		if not model_name:
			model_name = self.model_name

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		data: dict[str, Any] = {

			'stream': False,
			'prompt': prompt,
			'model': model_name,
			**options
		}
		headers: dict[str, Any] = {'Content-Type': 'application/json'}

		response = self.session.post(f'{self.base_url}/v1/completions', data=json.dumps(data),
									 headers=headers, timeout=self.TIMEOUT, stream=False)

		if response.status_code == 404 and 'not found' in response.json().get('error'):

			logger.info(f'â¬‡ï¸ downloading {model_name}â€¦')
			self.pull_model(model_name)

			return self.get_completion(prompt=prompt, model_name=model_name, temperature=temperature, **kwargs)

		if not response.ok:
			logger.error(f'{response.text=}, {prompt=}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		content: str = response_json.get('response')

		return AIClientResponse(raw=response_json, content=content)

	def get_embedding(self, input: str, model_name: str = None, _retry_count: int = 0) -> list[float]:
		"""
		Returns embeddings for a given input string
		"""
		raise NotImplementedError('MLXClient does not support embeddings yet. Please use OllamaClient')

	def fine_tune(self, data: list[dict[str, Any]], validation_data: list[dict[str, Any]] = None,
				  model_name: str = None, options: dict[str, Any] = None) -> str:
		"""
		[Internal]
		Fine-tunes an on-device model.
		Use `AI.learn(â€¦, fine_tune=True)` instead.

		@param data(list): A list of messages (user question and response).
		@param validation_data(list): (Optional) A list of messages (user question and response) to use
									  for validating the fine-tuned model.
		@param model_name(str): (Optional) The name of the model to fine-tune. Defaults to the one from `MLXClient(model_name=â€¦)`
		@param options(dict): (Optional) Custom arguments to pass to MLX (such as `iters`, `grad_checkpoint`, `seed`, `validation_set_portion` etc.)
		@return (str): The name of the new fine-tuned model with the adapter path
		"""

		from time import time
		from mlx_lm import lora
		from collections import namedtuple
		from mlx_lm.tuner.datasets import Dataset

		model_name = model_name or self.model_name
		new_model_name: str = f'fine-tuned--{model_name.replace("/", "--", 1)}-{int(time())}'
		adapter_path: str = f'{self.MODELS_PATH}/{new_model_name}'

		Arguments = namedtuple('Arguments', list(lora.CONFIG_DEFAULTS.keys()) + ['grad_checkpoint'])

		class CustomArguments(Arguments):

			@property
			def __dict__(self):
				return self._asdict()

		class CustomChatDataset(Dataset):

			def __init__(self, data: list[dict], tokenizer: 'PreTrainedTokenizer'):

				self._data = data
				self._tokenizer = tokenizer

			def __getitem__(self, idx: int):

				messages: list[dict] = self._data[idx]['messages']
				return self._tokenizer.apply_chat_template(conversation=messages, tokenize=False, add_generation_prompt=True)

		options = options or {}

		# set default fine_tuning iterations
		# TODO: tune these hyperparameters in the future
		logger.info(f"ðŸ“¦ Size of dataset: {len(data)}")

		if 'iters' not in options:

			if len(data) < 200:
				options['iters'] = 120

			elif len(data) < 500:
				options['iters'] = 1000

			else:
				options['iters'] = 2500

		model, tokenizer = lora.load(path_or_hf_repo=model_name)

		arguments: CustomArguments = CustomArguments(**{**lora.CONFIG_DEFAULTS, **{
			'train': True, 'test': False, 'grad_checkpoint': True, 'steps_per_eval': 20,
			'adapter_path': adapter_path, **(self.options.get('fine_tune', {})),
			**options
		}})

		if 'seed' in options:
			random.seed(options['seed'])

		validation_set_portion: float = options.pop('validation_set_portion', 0.2)

		if not validation_data:
			# if custom validation data not provided,
			# split part of the train data as validation data
			indices: list[int] = list(range(len(data)))
			random.shuffle(indices)

			validation_set_size: int = max(1, int(len(data) * validation_set_portion))

			validation_data = [data[i] for i in indices[:validation_set_size]]
			data[:] = [data[i] for i in indices[validation_set_size:]]

		os.makedirs(adapter_path, exist_ok=True)
		with open(os.path.join(adapter_path, 'train_data.jsonl'), 'w', encoding='utf-8') as f:
			for item in data:
				json_line = json.dumps(item)
				f.write(json_line + '\n')

		with open(os.path.join(adapter_path, 'validation_data.jsonl'), 'w', encoding='utf-8') as f:
			for item in validation_data:
				json_line = json.dumps(item)
				f.write(json_line + '\n')

		logger.info(f"ðŸ“¥ Train and validation data saved to {adapter_path}")

		train_set: CustomChatDataset = CustomChatDataset(data=data, tokenizer=tokenizer)
		valid_set: CustomChatDataset = CustomChatDataset(data=validation_data, tokenizer=tokenizer)

		lora.train_model(args=arguments, model=model, tokenizer=tokenizer, train_set=train_set,
						 valid_set=valid_set, training_callback=None)

		# run MLX server with the adapter set
		self.run_mlx_server(arguments={'adapter_path': adapter_path})

		return new_model_name

	""" Utilities """

	def get_ai_client_response_for_message(self, message: dict[str, Any]) -> AIClientResponse:
		"""
		[Internal]
		Converts a raw response message to `AIClientResponse`

		@param message(dict): e.g. `{'role': 'user', 'content': 'Lorem ipsum'}`
		@return (AIClientResponse): A response
		"""
		content: str = message.get('content')
		tool_calls: list[ToolCall] = []

		if content:

			if 'name' in content and 'arguments' in content:

				matches: list[str] = re.findall(r'{.*?}', content, flags=re.MULTILINE | re.DOTALL)
				matches = [match for match in matches if 'name' in match and 'arguments' in match]

				for match in matches:

					tool_call_dict: dict[str, Any] = Tool.get_dictionary_from_string(string=match.replace('\'', ''))

					# only add if the tool_call is a dict
					if not isinstance(tool_call_dict, dict):
						raise Exception(f'{model_name} responded with an invalid tool call ({tool_call_dict=}, {content=})')

					tool_calls.append(ToolCall(id=None, name=tool_call_dict.get('name'), raw_arguments=json.dumps(tool_call_dict.get('arguments'))))

			# some responses may have `<tool_response>`
			content = content.replace('<tool_response>', '').replace('</tool_response>', '')

		return AIClientResponse(raw=message, content=content, tool_calls=tool_calls)

	def create_message(self, role: str, content: str, files: list[str|bytes] = None, **kwargs) -> dict[str, Any]:
		"""
		Creates a message that matches the format used by the client that inherits
		this class

		@param role(str): the value of any of the `ROLE_` properties
		@param content(str): the text content of the message
		@param files(list): a list of file paths or bytes
		"""

		message: dict[str, Any] = {'role': role, 'content': content}

		if files:
			message['images'] = [self.encode_file(path=file) if isinstance(file, str) else self.encode_bytes(bytes_=file) for file in files]

		return message

	""" MLX-specific functions """

	def run_mlx_server(self, arguments: dict[str, Any] = None) -> None:
		"""
		[Internal]
		Runs the MLX server if needed
		"""
		try:
			import mlx_lm
			import psutil

		except ImportError:

			Utilities.install_package('mlx-lm')
			Utilities.install_package('psutil')

			import psutil

		# run MLX		
		mlx_pid: int = None
		should_restart: bool = False
		mlx_process_name: str = 'mlx_lm.server'

		for process in psutil.process_iter(['pid', 'name', 'cmdline']):

			process_name: str = process.info['name']
			process_cmdline: str = process.info['cmdline']

			if not process_cmdline:
				continue

			if mlx_process_name not in str(process_cmdline):
				continue

			mlx_pid = process.pid
			process_cmdline = process_cmdline[next((argument_index for argument_index, argument_string in enumerate(process_cmdline) if argument_string.endswith(mlx_process_name)), len(process_cmdline)):]

			if process_cmdline and process_cmdline[0].endswith(mlx_process_name):
				process_cmdline = process_cmdline[1:]

			# match all of the required arguments
			if arguments:

				# all of the arguments we want are set
				if not all(argument_name in f'-{process_cmdline}' for argument_name in arguments.keys()):
					should_restart = True

			# close the process and rerun it without the arguments
			elif process_cmdline and 'adapter-path' in str(process_cmdline):
				should_restart = True

		should_restart = should_restart or not mlx_pid

		if not should_restart:
			return

		if mlx_pid:

			import signal
			os.killpg(os.getpgid(mlx_pid), signal.SIGTERM)

		with open(os.devnull, 'w') as devnull:

			import subprocess
			arguments: list[str] = sum([[f'--{argument_name}', argument_value] for argument_name, argument_value in arguments.items()], []) if arguments else []

			subprocess.Popen(['nohup', mlx_process_name] + arguments, stdout=devnull, stderr=devnull, preexec_fn=os.setpgrp)


TOOLS_SYSTEM_PROMPT: str =  \
'''
You are an assistant with access to the functions below. If the user's prompt requires using any of the tools below, respond with the exact JSON format below:
{"name": "function_name", "arguments": {}}

If you respond with the above, a function will be called and its response will be returned to you. Your task is to either:
- Repeat the response back to the user or
- Call the next function if necessary

---
'''


if __name__ == '__main__':

	# MLXClient(model_name='').run_mlx_server()
	print(MLXClient(model_name='').get_models())
	# print(MLXClient(model_name='microsoft/Phi-3-mini-128k-instruct').get_embedding(input='test'))
	# print(MLXClient(model_name='microsoft/Phi-3-mini-128k-instruct').get_response(messages='hey! how is it going? whats your name?').content)
