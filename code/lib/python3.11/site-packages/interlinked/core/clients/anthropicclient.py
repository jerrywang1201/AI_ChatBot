import json
import logging
import requests
from time import sleep
from typing import Any
from requests import HTTPError
from collections.abc import Iterator
from requests.adapters import HTTPAdapter, Retry

from interlinked.core.tool import Tool
from interlinked.core.config import Config
from interlinked.core.utilities import Utilities
from interlinked.core.clients.baseaiclient import BaseAIClient, AIModel, AIClientResponse, ToolCall, File

logger = logging.getLogger(__name__)


class AnthropicClient(BaseAIClient):
	"""
	Anthropic client to interact with Anthropic Claude APIs
	"""

	ROLE_USER: str = 'user'
	ROLE_SYSTEM: str = 'system'
	ROLE_ASSISTANT: str = 'assistant'

	# With `use_external`, this is `tool_result`. When not using external this is set to `OpenAIClient.ROLE_TOOL` during initialization.
	ROLE_TOOL: str = 'tool_result'

	ANTHROPIC_VERSION: str = '2023-06-01'
	BASE_URL: str = 'https://api.anthropic.com'

	MAX_RETRIES: int = 3

	api_key: str = None
	base_url: str = None
	model_name: str = None
	options: dict[str, Any] = None
	embedding_model_name: str = None

	# internal
	use_external: bool = False
	use_floodgate: bool = False

	def __init__(self, model_name: str = None, embedding_model_name: str = None,
				 api_key: str = None, base_url: str = None, options: dict[str, Any] = None,
				 use_external: bool = False, **kwargs):
		"""
		Initializes this AI client to use with `AI.ask(…, client=AnthropicClient(…))`

		@param model_name(str): (optional) the default model name to use to chat
		@param embedding_model_name(str): (optional) the default model name to use to embeddings
		@param api_key(str): most preferred option. Get an API Key from [here](https://interlinked.apple.com/playground)
		@param options(dict): (optional) any default options you'd like to set for the model. [Learn more](https://docs.anthropic.com/en/api/messages)
		@param use_external(bool): (optional) Whether to use the external version of Anthropic
		"""

		self.base_url = base_url
		self.use_external = use_external
		self.api_key = api_key or Config.current.ANTHROPIC_API_KEY
		self.embedding_model_name = embedding_model_name or 'voyage-3'
		self.model_name = model_name or ('claude-3-7-sonnet-latest' if self.use_external else 'claude-3-7-sonnet-20250219')

		if not self.api_key and not self.use_external:
			self.use_floodgate = True

		# determine the `base_url`
		if not self.base_url:

			if (self.api_key and self.api_key.startswith('in-')) or self.use_floodgate or not self.use_external:
				self.base_url = f'{Config.current.FLOODGATE_BASE_URL}/anthropic'

			else:
				self.base_url = 'https://api.anthropic.com'

		# check if the API key is valid
		if self.api_key and self.api_key.startswith('in-') and len(self.api_key) < 4:
			raise Exception('The api_key you passed is not valid. You can create one in interlinked.apple.com/playground.')

		if not self.use_external:

			if not self.model_name.startswith('anthropic'):
				self.model_name = f'anthropic.{self.model_name}-v1:0'

			# API Key is not needed for narrative
			if Config.current.is_development:

				if 'oauth_token' in kwargs:
					self.api_key = kwargs['oauth_token']

				else:
					self.api_key = Utilities.get_apple_connect_token(app_id=Config.current.FLOODGATE_CLIENT_ID, type='oauth')

		self.options = {**{'temperature': 0.6}, **(options or {})}
		self.configure_session(base_url=self.base_url, **kwargs)

	def configure_session(self, base_url: str, **kwargs) -> None:
		"""
		[Internal]
		Configures the session

		@param base_url(str): Base url to connect to
		"""

		self.session = requests.session()

		if not Config.current.is_development:

			if base_url and 'floodgate' in base_url and 'cert' in kwargs and 'dsid' in kwargs:

				self.session.headers.update({
					'X-Narrative-Dsid': kwargs['dsid']
				})
				self.session.cert = kwargs['cert']

			else:
				self.session.proxies = {'https': 'http://dps.iso.apple.com:443'}

		if 'headers' in kwargs and kwargs['headers']:
			self.session.headers.update(kwargs['headers'])

		# status forcelist depends on whether we're using Interlinked as the backend
		# since Anthropic returns intermittent 500 and 429 for retry, while Interlinked doesn't
		retry: Retry = Retry(total=8, backoff_factor=1,
							 status_forcelist=[429] if ((self.api_key and self.api_key.startswith('in-')) or self.use_floodgate)
							 else [500, 429])
		self.session.mount('https://', HTTPAdapter(max_retries=retry))

	def should_refresh_token(self, response: Any) -> bool:
		"""
		[Internal]
		Checks if a 401 response indicates expired FloodGate token and attempts refresh

		@param response: The HTTP response object
		@param retry_count: Current retry attempt count
		@return (bool): True if token was refreshed and request should be retried, False otherwise
		"""

		if not self.use_floodgate or not Config.current.is_development:
			return False

		if (response.status_code == 401):

			# Refresh token
			self.api_key = Utilities.get_apple_connect_token(app_id=Config.current.FLOODGATE_CLIENT_ID, type='oauth')
			return True

		return False

	def get_models(self, dsid: int = None, _raw: bool = False,
				   _retry_count: int = 0) -> list[AIModel | dict]:
		"""
		Returns a list of `AIModel` containing models available

		@param dsid(int): (Optional) DSID for authentication
		@param _raw(bool): (Optional) Whether to return raw JSON response
		@param _retry_count(int): (Optional) Current retry attempt count
		"""

		if not self.api_key and not self.use_floodgate:
			raise Exception('Missing api_key')

		response: Any = None

		if (self.api_key and self.api_key.startswith('in-')) or self.use_floodgate or not self.use_external:

			headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
			response = self.session.get(f'{self.base_url}/v1/models', headers=headers, timeout=20)

		else:
			headers: dict[str, Any] = {'x-api-key': f'{self.api_key}', 'anthropic-version': self.ANTHROPIC_VERSION}
			response = self.session.get(f'{self.base_url}/v1/models', headers=headers, timeout=20)

		if not response.ok:
			logger.error(response.text)

			# FloodGate token refresh
			if self.should_refresh_token(response):
				return self.get_models(dsid=dsid, _raw=_raw, _retry_count=_retry_count + 1)

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()

		if 'data' not in response_json:
			response_json['data'] = []

		# return raw JSON response, if the caller wants to
		# used with Interlinked API keys (e.g. `in-…`)
		if _raw:
			return response_json

		models: list[AIModel] = [AIModel(name=model.get('id'), description=None, raw=model) for model
								 in response_json.get('data', [])]

		ai_models: list[AIModel] = []

		for ai_model in models:

			ai_model.name = ai_model.name.removeprefix('aws:anthropic.').removeprefix('anthropic.').removesuffix('-v1:0')
			ai_models.append(ai_model)

		return sorted(ai_models, key=lambda ai_model: ai_model.name, reverse=True)

	@Utilities.dynamic
	def get_token_count(self, input: str = None, messages: list[dict] = None,
						tools: list[Tool] = None, model_name: str = None) -> int:
		"""
		Returns the number of tokens for a given input string

		@param input(str): (Optional) an input string
		@param messages(list): A list of messages or a prompt string (which gets converted to a message)
		@param tools(list): (Optional) A list of functions/tools to give AI access to
		@param model_name(str): (Optional) The name of the model to use. Defaults to the model you use in `AI.ask(…)`
		@return (int): the number of tokens
		"""

		model_name = model_name or self.model_name

		if input and not messages:
			messages = [self.create_message(role=self.ROLE_USER, content=input)]

		data: dict[str, Any] = {'model': self.model_name, 'messages': messages}

		if tools:
			data['tools'] =  [tool.to_dict(self) for tool in tools]

		response: Any = None

		if (self.api_key and self.api_key.startswith('in-')) or self.use_floodgate or not self.use_external:

			# not available yet
			raise NotImplementedError('Not available with the internal Anthropic')

		else:
			headers: dict[str, Any] = {'Content-Type': 'application/json', 'x-api-key': f'{self.api_key}', 'anthropic-version': self.ANTHROPIC_VERSION}
			response = self.session.post(f'{self.base_url}/v1/messages/count_tokens', headers=headers,
										 timeout=10, data=json.dumps(data))

		if not response.ok:
			logger.error(f'{response.text=}, {input=}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		return response_json.get('input_tokens')

	@Utilities.dynamic
	def get_embedding(self, input: str | list[str], model_name: str = None) -> list[float] | list[list[float]]:
		"""
		Returns embeddings for a given input string
		"""
		raise Exception('Anthropic does not currently have a dedicated embedding model from Claude')

	@Utilities.dynamic
	def get_response(self, messages: str | list[dict], tools: list[Tool] = None, model_name: str = None,
					 temperature: float = 0.2, stream: bool = False, format_: str = None, _retry_count: int = 0,
					 **kwargs) -> AIClientResponse | Iterator[AIClientResponse]:
		"""
		Returns chat response for a given messages

		@param messages(str|list): A list of messages or a prompt string (which gets converted to a message)
		@param tools(list): (Optional) A list of functions/tools to give AI access to
		@param model_name(str): (Optional) The name of the model to use. Defaults to the model you use in `AI.ask(…)`
		@param temperature(float): (Optional) The temperature (controls creativeness). Defaults to the value in `AI.ask(…)`,
								   which is typically 0.6
		@return (tuple): a response
		"""

		if stream:
			return self.get_response_stream(messages=messages, model_name=model_name, temperature=temperature, tools=tools, **kwargs)

		model_name = model_name or self.model_name
		messages = messages.copy() if isinstance(messages, list) else [self.create_message(role=self.ROLE_USER, content=messages)]

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		if 'thinking' in options and options.get('thinking', {}).get('type') == 'enabled':
			options['temperature'] = 1

		if 'max_tokens' not in options:
			options['max_tokens'] = 4096

		data: dict[str, Any] = {**options}

		if messages[0]['role'] == self.ROLE_SYSTEM:

			data['system'] = [{'text': messages[0].get('content'), 'type': 'text'}]

			del messages[0]

		data['messages'] = messages
		data['model'] = model_name

		if tools:
			data['tools'] = [tool.to_dict(self) for tool in tools]

		response: Any = None

		try:

			if (self.api_key and self.api_key.startswith('in-')) or self.use_floodgate or not self.use_external:
				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}

			else:
				headers: dict[str, Any] = {'Content-Type': 'application/json', 'x-api-key': f'{self.api_key}',
										   'anthropic-version': self.ANTHROPIC_VERSION}

			response = self.session.post(f'{self.base_url}/v1/messages', headers=headers,
											 timeout=500, data=json.dumps(data))

		except HTTPError as exception:

			if _retry_count <= self.MAX_RETRIES:

				logger.warning('request timed out. trying again…')

				sleep(_retry_count + 2)
				return self.get_response(messages=messages, tools=tools, model_name=model_name,
										 temperature=temperature, stream=stream, format_=format_,
										 _retry_count=_retry_count + 1, **kwargs)

			else:

				logger.error('request timed out too many times')
				raise exception

		if not response.ok:

			logger.error(f'{response.text=}, {json.dumps(messages)}')

			# FloodGate token refresh
			if self.should_refresh_token(response):

				return self.get_response(messages=messages, tools=tools, model_name=model_name,
										 temperature=temperature, stream=stream, format_=format_,
										 _retry_count=_retry_count + 1, **kwargs)

			if response.status_code == 400:
				raise Exception(f'400 Error: {response.text}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		return self.get_ai_client_response_for_message(message=response_json)

	@Utilities.dynamic
	def get_response_stream(self, messages: list[dict], model_name: str = None,
							temperature: float = 0.2, tools: list[Tool] = None, format_: str = None, _retry_count: int = 0,
							**kwargs) -> Iterator[AIClientResponse]:
		"""
		Returns streamed chat responses for given messages

		@param messages(list): A list of messages
		@param model_name(str): the name of the model to use
		@return (tuple): a response
		"""

		if _retry_count > self.MAX_RETRIES:
			raise Exception('retried too many times')

		model_name = model_name or self.model_name
		messages = messages.copy()

		# merge messages, if needed
		merged_messages: list[dict[str, Any]] = []
		thinking_buffer: str = ''
		signature: str = ''

		for message_index, message in enumerate(messages):

			if message.get('type') == 'thinking_delta':

				thinking_buffer += message.get('thinking', '')
				continue

			if message.get('type') == 'signature_delta':

				# When there is a thinking signature always present
				signature = message.get('signature')

			if message_index == 0:

				merged_messages.append(message)
				continue


			if message.get('role') == 'assistant' and thinking_buffer:

				existing = message.get('content', '')

				if isinstance(existing, list):
					has_thinking = any(item.get('type') == 'thinking' for item in existing)

					if not has_thinking:
						message['content'].insert(0, {'type': 'thinking', 'thinking': thinking_buffer, 'signature': signature})

				else:
					message['content'] = [{'type': 'thinking', 'thinking': thinking_buffer, 'signature': signature},
										  {'type': 'text', 'text': str(existing)}]
				thinking_buffer = ''

			# Check if current message should merge with the last merged message
			if merged_messages and self.get_should_merge(partial_message=merged_messages[-1], complete_message=message):
				merged_messages[-1] = self.get_merged_messages(partial_message=merged_messages[-1], complete_message=message)

			# Check if current message (text_delta) should merge with previous delta message
			elif message.get('type') == 'text_delta' and merged_messages and merged_messages[-1].get('type') == 'text_delta':
				merged_messages[-1] = self.get_merged_messages(partial_message=merged_messages[-1], complete_message=message)

			else:

				merged_messages.append(message)

		messages = merged_messages

		# merge messages, if needed
		merged_messages: list[dict[str, Any]] = []

		for message_index, message in enumerate(messages):

			if message_index == 0:

				merged_messages.append(message)
				continue

			previous_message: dict[str, Any] = merged_messages[-1]

			if self.get_should_merge(partial_message=previous_message, complete_message=message):
				merged_messages[-1] = self.get_merged_messages(partial_message=previous_message, complete_message=message)

			else:
				merged_messages.append(message)

		merged_messages = messages

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		if 'thinking' in options and options.get('thinking', {}).get('type') == 'enabled':
			options['temperature'] = 1

		if 'max_tokens' not in options:
			options['max_tokens'] = 32000

		data: dict[str, Any] = {**options}

		if messages[0]['role'] == self.ROLE_SYSTEM:

			data['system'] = [{'text': messages[0].get('content'), 'type': 'text'}]

			del messages[0]

		data['stream'] = True
		data['model'] = model_name
		data['messages'] = messages


		if tools:
			data['tools'] =  [tool.to_dict(self) for tool in tools]

		generator: Any = None

		try:

			if (self.api_key and self.api_key.startswith('in-')) or self.use_floodgate or not self.use_external:
				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}

			else:
				headers: dict[str, Any] = {'Content-Type': 'application/json', 'x-api-key': f'{self.api_key}',
										   'anthropic-version': self.ANTHROPIC_VERSION}

			generator = self.session.post(f'{self.base_url}/v1/messages', headers=headers,
										  timeout=500, data=json.dumps(data), stream=True)


			with generator as response:

				if not response.ok:

					logger.error(response.text)

					# FloodGate token refresh
					if self.should_refresh_token(response):
						return self.get_response_stream(messages=messages, model_name=model_name, temperature=temperature,
														tools=tools, format_=format_, _retry_count=_retry_count + 1, **kwargs)

					raise Exception(response.text)

				response.raise_for_status()

				tool_calls: list[ToolCall] = []
				current_tool_call: ToolCall = None
				accumulated_json = ""

				for line in response.iter_lines():

					line = line.decode().strip()

					# if the response is finished, return so we don't yield an empty response
					if line == 'event: message_stop':
						break

					line_data = line.split('\n', 1)[0]
					if not line_data.startswith('data: '):
						continue

					content_block: dict = json.loads(line_data.removeprefix('data: '))

					# Handle content block start (new tool call)
					if 'content_block' in content_block:

						block = content_block.get('content_block', {})

						if block.get('type') == 'tool_use':

							current_tool_call = ToolCall(id=block.get('id'),name=block.get('name'),raw_arguments='')
							accumulated_json = ''

					# Handle content block delta (streaming content)
					elif 'delta' in content_block:

						delta = content_block.get('delta', {})

						if delta.get('type') == 'text_delta':
							yield AIClientResponse(raw=delta, content=delta.get('text'), tool_calls=tool_calls)

						elif 'thinking' in delta.keys():
							yield AIClientResponse(raw=delta, content=delta.get('thinking'), metadata={'thinking': delta.get('thinking')}, tool_calls=tool_calls)

						elif 'signature' in delta.keys():
							yield AIClientResponse(raw=delta, content='', metadata={'signature': delta.get('signature')}, tool_calls=tool_calls)

						# Handle tool call JSON arguments
						elif delta.get('type') == 'input_json_delta' and current_tool_call:

							partial_json = delta.get('partial_json', '')
							accumulated_json += partial_json
							current_tool_call.raw_arguments = accumulated_json

					elif content_block.get('type') == 'content_block_stop':

						if current_tool_call:

							if current_tool_call not in tool_calls:
								tool_calls.append(current_tool_call)

							raw = {'role': self.ROLE_ASSISTANT, 'content': []}

							for tool_call in tool_calls:

								raw['content'].append({
									'type': 'tool_use',
									'id': tool_call.id,
									'name': tool_call.name,
									'input': tool_call.arguments
								})

							# Yield the completed tool call
							yield AIClientResponse(raw=raw, content='', tool_calls=tool_calls)

							current_tool_call = None
			return

		except HTTPError as error:

			logger.warning('request timed out. trying again…')

			return self.get_response_stream(messages=messages, model_name=model_name, temperature=temperature,
											format_=format_, _retry_count=_retry_count + 1, **kwargs)

	""" Utilities """

	def create_message(self, role: str, content: str, files: list[str | bytes | File] = None, **kwargs) -> dict[str, Any]:
		"""
		Creates a message that matches the format used by Anthropic Claude

		@param role(str): the value of any of the `ROLE_` properties
		@param content(str): the text content of the message
		@param files(list): a list of file paths or bytes
		"""

		message: dict[str, Any] = {}

		if files:

			content_list: list[dict] = []

			for file in files:

				if isinstance(file, File):

					file_data: dict[str, Any] = {
						'type': 'image', 'source': {
							'type': 'base64',
							'data': file.content,
							'media_type': file.mime_type,
						}
					}

				elif ((isinstance(file, str) and file.lower().endswith('.pdf')) or
					  (isinstance(file, bytes) and file.startswith(b'%PDF-'))):

					file_base64: str = self.encode_file(path=file) if isinstance(file, str) else self.encode_bytes(bytes_=file)
					file_data: dict[str, Any] = {
						'type': 'document', 'source': {
							'type': 'base64',
							'media_type': 'application/pdf',
							'data': file_base64
						}
					}

				else:

					file_base64: str = self.encode_file(path=file) if isinstance(file, str) else self.encode_bytes(bytes_=file)
					file_data: dict[str, Any] = {
						'type': 'image', 'source': {
							'type': 'base64',
							'data': file_base64,
							'media_type': 'image/jpeg',
						}
					}

				content_list.append(file_data)

			if content:
				content_list.append({'type': 'text', 'text': content})

			message['content'] = content_list

		else:
			message['content'] = content

		# if this is a message from a tool/function, add the tool arguments
		if role == self.ROLE_TOOL:

			message['role'] = self.ROLE_USER

			message['content'] = [{

				'content': content,
				'type': 'tool_result',
				'tool_use_id': kwargs['tool_call_id'],
			}]

		else:
			message['role'] = role

		return message

	def get_ai_client_response_for_message(self, message: list[dict[str, Any]]) -> AIClientResponse:
		"""
		Converts a raw response message to `AIClientResponse`

		@param message(dict): e.g. `{'role': 'user', 'content': 'Lorem ipsum'}`
		@return (AIClientResponse): a response
		"""

		content: str = ''
		tool_calls: list[ToolCall] = []
		role: str = self.ROLE_ASSISTANT
		metadata: dict = {}
		message = message.get('content', [])

		if parts := message:

			for part in parts:

				if isinstance(message, str):
					break

				type: str = part['type']

				if type == 'text':
					content = part.get('text')

				elif type == 'thinking':
					metadata['thinking'] = part.get('thinking')

				elif type == 'tool_use':

					tool_call_dict: str = part

					if tool_call_dict:
						tool_calls.append(ToolCall(id=part.get('id'), name=tool_call_dict.get('name'),
												   raw_arguments=json.dumps(tool_call_dict['input'])))

				elif type == 'tool_result':

					content = part.get('content')
					role = self.ROLE_USER

		return AIClientResponse(raw={'role': role, 'content': message}, content=content, tool_calls=tool_calls, metadata=metadata)

	""" Anthropic-specific """

	def get_should_merge(self, partial_message: dict, complete_message: dict) -> bool:
		"""
        Checks if a partial message (from streaming) should be merged into a complete message
        """

		if not partial_message:
			return False

		if partial_message.get('type') == 'text_delta' and complete_message.get('role') == 'assistant':
			return True

		if partial_message.get('type') == 'text_delta' and complete_message.get('type') == 'text_delta':
			return True

		if partial_message.get('role') != complete_message.get('role'):
			return False

		role = partial_message.get('role') or complete_message.get('role')

		if role == self.ROLE_TOOL:
			return False

		return True

	def get_merged_messages(self, partial_message: dict, complete_message: dict) -> dict[str, Any]:
		"""
			Merges a partial message (from streaming) into a complete message
			"""

		if not partial_message:
			return complete_message

		if partial_message.get('type') == 'text_delta' and complete_message.get('role') == 'assistant':

			merged = complete_message.copy()
			delta_text = partial_message.get('text', '')

			if isinstance(merged.get('content'), list):

				content_list = merged['content'].copy()

				for i, item in enumerate(content_list):

					if isinstance(item, dict) and item.get('type') == 'text':

						content_list[i] = {'type': 'text', 'text': delta_text + item.get('text', '')}
						break

				merged['content'] = content_list

			else:
				merged['content'] = delta_text + str(merged.get('content', ''))

			return merged

		if partial_message.get('type') == 'text_delta' and complete_message.get('type') == 'text_delta':

			merged = {'type': 'text_delta', 'text': partial_message.get('text', '') + complete_message.get('text', '')}
			return merged

		if complete_message.get('text') and partial_message.get('text'):
			complete_message['text'] = f'{partial_message["text"]}{complete_message["text"]}'

		return complete_message


if __name__ == '__main__':

	from interlinked import AI
	client = AnthropicClient()
	print(AI.ask('Hello Claude AI!').response)

	for observation in AI.ask(prompt='Generate a long lorem ipsum of 100 words', client=client, stream=True):
		print(observation.response.raw, end='', flush=True)