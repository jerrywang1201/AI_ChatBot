import os
import re
import time
import json
import base64
import inspect
import logging
import requests
import mimetypes
import websockets
from time import sleep
from requests import HTTPError
from pydantic import BaseModel
from typing import Any, Optional
from collections.abc import Iterator
from requests.adapters import HTTPAdapter, Retry

from interlinked.core.tool import Tool
from interlinked.core.config import Config
from interlinked.core.utilities import Utilities
from interlinked.core.clients.baseaiclient import BaseAIClient, AIModel, AIClientResponse, ToolCall, Call, Event, File

logger = logging.getLogger(__name__)


class GoogleAIClient(BaseAIClient):
	"""
	Google AI client to interact with Google's Gemini APIs
	"""

	ROLE_USER: str = 'user'
	ROLE_SYSTEM: str = 'system'
	ROLE_TOOL: str = 'function'
	ROLE_ASSISTANT: str = 'model'

	# realtime events
	EVENT_SETUP: str = 'setup'
	EVENT_SETUP_DONE: str = 'setupComplete'

	EVENT_TEXT_DELTA: str = 'serverContent'
	EVENT_TEXT_DONE: str = 'internal::text_done'

	EVENT_ITEM_CREATE: str = 'internal::item_create'

	EVENT_AUDIO_APPEND: str = 'audio_append'

	EVENT_TOOL_CALL: str = 'toolCall'
	EVENT_TOOL_RESPONSE: str = 'tool_response'
	EVENT_FUNCTION_CALLS: str = 'functionCalls'

	# events to ignore when handling
	SKIPPED_EVENTS: list[str] = set()

	# the input rate Google AI uses for audio playback
	INPUT_RATE: int = 16000

	BASE_URL: str = 'https://generativelanguage.googleapis.com'

	MAX_RETRIES: int = 3
	TIMEOUT: int = 500

	api_key: str = None
	base_url: str = None
	location: str = None
	model_name: str = None
	project_id: str = None
	options: dict[str, Any] = None
	embedding_model_name: str = None

	# [internal]
	labels: dict[str, Any] = None

	# [internal]
	use_floodgate: bool = False
	use_appleconnect: bool = False

	# automatically set if using Vertex AI (the hosted version of Gemini)
	credentials: Any = None

	def __init__(self, model_name: str = None, embedding_model_name: str = None,
				 project_id: str = None, location: str = None, use_floodgate: bool = False,
				 api_key: str = None, base_url: str = None, options: dict[str, Any] = None, **kwargs):
		"""
		Initializes this AI client to use with `AI.ask(…, client=GoogleAIClient(…))`

		@param model_name(str): (Optional) the default model name to use to chat
		@param embedding_model_name(str): (Optional) the default model name to use to embeddings
		@param project_id(str): (Optional) Only needed if you have a GCP Project (e.g. `interlinked-283`)
		@param use_floodgate(bool): (Internal) Only needed if you'd like to use Floodgate for authentication
		@param api_key(str): (Optional) Most preferred option. Get an API Key from [here](https://interlinked.apple.com/playground)
		@param options(dict): (Optional) any default options you'd like to set for the model. [Learn more](https://ai.google.dev/tutorials/rest_quickstart#stream_generate_content)
		"""

		self.model_name = model_name or 'gemini-2.0-flash'
		self.embedding_model_name = embedding_model_name or 'text-embedding-005'

		project_id = project_id or Config.current.GOOGLEAI_PROJECT_ID
		self.use_floodgate = use_floodgate

		if use_floodgate:

			self.base_url = base_url or Config.current.FLOODGATE_BASE_URL
			self.base_url = f'{self.base_url}/gemini/v1'

			# API Key is not needed for narrative
			if Config.current.is_development:
				self.api_key = Utilities.get_apple_connect_token(app_id=Config.current.FLOODGATE_CLIENT_ID, type='oauth')

		elif project_id and not api_key:

			self.project_id = project_id
			self.location = location or Config.current.GOOGLEAI_LOCATION or 'us-central1'
			self.base_url = f'https://{self.location}-aiplatform.googleapis.com/v1beta1/projects/{self.project_id}/locations/{self.location}'

			if Config.current.is_development:

				try:
					import google.auth

				except (ImportError, ModuleNotFoundError) as error:

					# for convenience, auto-install google-auth
					if Config.current.is_development:

						logger.section('installing google-auth…')
						Utilities.install_package(name='google-auth')

					import google.auth

				try:
					self.credentials, _ = google.auth.default()

				except google.auth.exceptions.DefaultCredentialsError:

					logger.section('authenticating with Google Account… Please log-in to your Apple Google acccount')
					os.system(f'gcloud auth application-default login')
					self.credentials, _ = google.auth.default()
					os.system(f'gcloud auth application-default set-quota-project {self.project_id}')

				self.credentials._quota_project_id = project_id

				if not self.credentials.valid:

					from google.auth.transport.requests import Request

					try:
						self.credentials.refresh(Request())

					except google.auth.exceptions.RefreshError:

						logger.section('authenticating with Google Account… Please log-in to your Apple Google acccount')
						os.system(f'gcloud auth application-default login')
						self.credentials, _ = google.auth.default()
						os.system(f'gcloud auth application-default set-quota-project {self.project_id}')
						self.credentials.refresh(Request())

			else:

				try:
					import google.auth
					from pygcpappleconnect import Authenticator

				except (ImportError, ModuleNotFoundError) as error:

					# for convenience, auto-install google-auth
					if Config.current.is_development:

						logger.section('installing google-auth…')
						Utilities.install_package(name='pygcpappleconnect google-auth')

					import google.auth
					from pygcpappleconnect import Authenticator

				# authenticate with Vertex API
				authenticator: Authenticator = Authenticator(
					username=Config.current.APPLECONNECT_USERNAME or Utilities.get_apple_connect_username(),
					password=Config.current.APPLECONNECT_PASSWORD, totp_secret=Config.current.APPLECONNECT_TOTP_SECRET,
					device_id=Config.current.APPLECONNECT_DEVICE_ID, profile='production')
				self.credentials = authenticator.credentials()
				self.credentials._quota_project_id = project_id

		else:

			self.api_key = api_key or Config.current.GOOGLEAI_API_KEY

			if not self.api_key:
				self.use_appleconnect = True

			self.base_url = base_url or ('https://interlinked.apple.com/api/v1' if (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect
																				else 'https://generativelanguage.googleapis.com' )

		# check if the API key is valid
		if self.api_key and self.api_key.startswith('in-') and (len(self.api_key) < 4) or self.api_key == 'in-…':
			raise Exception('The api_key you passed is not valid. You can create one in interlinked.apple.com/playground.')

		self.options = {**{'temperature': 0.6}, **(options or {})}
		self.configure_session(base_url=self.base_url, **kwargs)

	def configure_session(self, base_url: str, **kwargs) -> None:
		"""
		[Internal]
		Configures the session
		"""

		self.session = requests.session()

		if not Config.current.is_development:

			# floodgate
			if base_url and 'floodgate' in base_url and 'cert' in kwargs and 'dsid' in kwargs:

				self.session.headers.update({'X-Narrative-Dsid': kwargs['dsid']})
				self.session.cert = kwargs['cert']

			else:
				self.session.proxies = {'https': Config.current.PROXY_URL}

		if 'headers' in kwargs and kwargs['headers']:
			self.session.headers.update(kwargs['headers'])

		# status forcelist depends on whether we're using Interlinked as the backend
		# since only Gemini returns intermittent 500s, while Interlinked doesn't
		retry: Retry = Retry(total=8, backoff_factor=1, status_forcelist=[502, 503, 504, 429] if ((self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect)
																							  else [500, 502, 503, 504, 429])
		http_adapter: HTTPAdapter = HTTPAdapter(max_retries=retry)
		self.session.mount('http://', http_adapter)
		self.session.mount('https://', http_adapter)

	def should_refresh_token(self, response: Any) -> bool:
		"""
		[Internal]
		Checks if a 401 response indicates expired FloodGate token and attempts refresh

		@param response: The HTTP response object
		@return (bool): True if token was refreshed and request should be retried, False otherwise
		"""

		if not self.use_floodgate or not Config.current.is_development:
			return False

		if (response.status_code == 401):

			# Refresh token
			self.api_key = Utilities.get_apple_connect_token(app_id=Config.current.FLOODGATE_CLIENT_ID, type='oauth')
			return True

		return False

	def get_models(self, dsid: int = None, _raw: bool = False,
				   _retry_count: int = 0) -> list[AIModel|dict]:
		"""
		Returns a list of `AIModel` containing models available

		@param dsid(int): (Optional) DSID for authentication
		@param _raw(bool): (Optional) Whether to return raw JSON response
		@param _retry_count(int): (Optional) Current retry attempt count
		"""

		if not self.api_key and not self.credentials and not self.use_appleconnect:
			raise Exception('missing api_key or project_id')

		response: Any = None

		if self.credentials or self.use_floodgate:

			headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
			response = self.session.get(f'{self.base_url}/models', timeout=10, headers=headers)

		elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:

			if not self.api_key:
				self.api_key = self.get_api_key()

			headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
			response = self.session.get(f'{self.base_url}/v1beta/models', headers=headers, timeout=20)

		else:
			response = self.session.get(f'{self.base_url}/v1beta/models', params={'key': self.api_key}, timeout=20)

		if not response.ok:
			logger.error(response.text)

			# FloodGate token refresh
			if self.should_refresh_token(response):
				return self.get_models(dsid=dsid, _raw=_raw, _retry_count=_retry_count + 1)

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()

		# add default models, if not returned by the API
		default_model_names: list[str] = ['gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-2.0-flash', 'imagen-3.0-fast-generate-001', 'veo-2.0-generate-001', 'veo-3.0-generate-preview']

		if 'models' not in response_json:
			response_json['models'] = []

		for model_name in default_model_names:

			if not any(model for model in response_json.get('models', []) if model['name'] == model_name):
				response_json['models'].append({'name': model_name})

		# return raw JSON response, if the caller wants to
		# used with Interlinked API keys (e.g. `in-…`)
		if _raw:
			return response_json

		models: list[AIModel] = [AIModel(name=model.get('name').removeprefix('models/'), description=model.get('description'), raw=model) for model
								 in response_json.get('models', [])]

		return models

	def get_token_count(self, input: str = None, messages: list[dict] = None,
						tools: list[Tool] = None, model_name: str = None) -> int:
		"""
		Returns the number of tokens for a given input string

		@param input(str): (Optional) an input string
		@param messages(list): A list of messages or a prompt string (which gets converted to a message)
		@param tools(list): (Optional) A list of functions/tools to give AI access to
		@param model_name(str): (Optional) The name of the model to use. Defaults to the model you use in `AI.ask(…)`
		@return (int): the number of tokens
		"""

		model_name = model_name or self.model_name

		if input and not messages:
			messages = [self.create_message(role=self.ROLE_USER, content=input)]

		data: dict[str, Any] = {'contents': messages}

		if messages[0]['role'] == self.ROLE_SYSTEM:

			if 'content' in messages[0]:
				data['system_instruction'] = self.create_message(role=self.ROLE_SYSTEM, content=messages[0]['content'])

			elif 'parts' in messages[0]:
				data['system_instruction'] = self.create_message(role=self.ROLE_SYSTEM, content=messages[0]['parts'][0]['text'])

			else:
				data['system_instruction'] = messages[0]

			del messages[0]

		if tools:
			data['tools'] = {'function_declarations': [tool.to_dict(self) for tool in tools]}

		response: Any = None

		if self.credentials or self.use_floodgate:

			headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
			response = self.session.post(f'{self.base_url}/publishers/google/models/{model_name}:countTokens', timeout=10, data=json.dumps(data), headers=headers)

		elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:

			if not self.api_key:
				self.api_key = self.get_api_key()

			headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
			response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:countTokens', headers=headers, timeout=20,
				data=json.dumps(data))

		else:
			response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:countTokens', params={'key': self.api_key},
										 timeout=10, data=json.dumps(data))

		if not response.ok:
			logger.error(f'{response.text=}, {input=}')

			# FloodGate token refresh
			if self.should_refresh_token(response):
				return self.get_token_count(input=input, messages=messages, tools=tools, model_name=model_name)

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		return response_json.get('totalTokens')

	def get_embedding(self, input: str|list[str], model_name: str = None) -> list[float]|list[list[float]]:
		"""
		Returns embeddings for a given input string
		"""

		model_name = model_name or self.embedding_model_name
		data: dict[str, Any] = {'instances': {'content': input}}

		response: Any = None

		if self.credentials or self.use_floodgate:
			headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
			response = self.session.post(f'{self.base_url}/publishers/google/models/{model_name}:predict', timeout=10, data=json.dumps(data), headers=headers)

		elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:
			if not self.api_key:
				self.api_key = self.get_api_key()

			headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
			response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:predict', headers=headers, timeout=20,
				data=json.dumps(data))
		else:
			response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:predict', params={'key': self.api_key},
										 timeout=10, data=json.dumps(data))

		if not response.ok:
			logger.error(f'{response.text=}, {input=}')

			# FloodGate token refresh
			if self.should_refresh_token(response):
				return self.get_embedding(input=input, model_name=model_name)

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		return response_json.get('predictions', [])[0].get('embeddings', {}).get('values')

	def get_response(self, messages: str|list[dict], tools: list[Tool] = None, model_name: str = None,
					 temperature: float = 0.2, stream: bool = False, format_: str | dict | BaseModel = None,
					 _retry_count: int = 0, **kwargs) -> AIClientResponse | Iterator[AIClientResponse]:
		"""
		Returns chat response for a given messages

		@param messages(str|list): A list of messages or a prompt string (which gets converted to a message)
		@param tools(list): (Optional) A list of functions/tools to give AI access to
		@param model_name(str): (Optional) The name of the model to use. Defaults to the model you use in `AI.ask(…)`
		@param format_(str|dict|BaseModel): (Optional) if set, the response will be in JSON
		@param temperature(float): (Optional) The temperature (controls creativeness). Defaults to the value in `AI.ask(…)`,
								   which is typically 0.6
		@return (tuple): a response
		"""

		if stream:
			return self.get_response_stream(messages=messages, model_name=model_name, tools=tools, temperature=temperature, **kwargs)

		model_name = model_name or self.model_name

		# Image Generation
		if 'imagen' in model_name:
			return self.get_response_media(messages=messages, model_name=model_name, **kwargs)

		# Video Generation
		if 'veo' in model_name:
			return self.get_response_video(messages=messages, model_name=model_name, **kwargs)

		messages = messages.copy() if isinstance(messages, list) else [self.create_message(role=self.ROLE_USER, content=messages)]

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		if format_:

			if isinstance(format_, dict):
				options['responseSchema' if self.credentials or self.use_floodgate else 'response_schema'] = format_

			elif inspect.isclass(format_) and issubclass(format_, BaseModel):
				options['responseSchema' if self.credentials or self.use_floodgate else 'response_schema'] = self.get_dictionary_for_base_model(base_model=format_)

			options['responseMimeType' if self.credentials or self.use_floodgate else 'response_mime_type'] = 'application/json'

		data: dict[str, Any] = {

			'safety_settings': [
				{'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},
				{'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},
				{'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'},
				{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},
			],
			'generationConfig' if self.credentials or self.use_floodgate else 'generation_config': options,
		}

		if messages[0]['role'] == self.ROLE_SYSTEM:

			if 'content' in messages[0]:
				data['system_instruction'] = self.create_message(role=self.ROLE_SYSTEM, content=messages[0]['content'])

			elif 'parts' in messages[0]:
				data['system_instruction'] = self.create_message(role=self.ROLE_SYSTEM, content=messages[0]['parts'][0]['text'])

			else:
				data['system_instruction'] = messages[0]

			del messages[0]

		data['contents'] = messages

		if tools:
			data['tools'] = self.create_message_for_tools(tools)

		# [internal]
		if self.labels:
			data['labels'] = self.labels

		response: Any = None
		endpoint: str = None

		# custom models use the `:predict` endpoint
		if '/custom' in model_name:
			endpoint = 'predict'

		else:
			endpoint = 'generateContent'

		try:
			if self.credentials or self.use_floodgate:

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
				response = self.session.post(f'{self.base_url}/publishers/google/models/{model_name}:{endpoint}',
											 timeout=self.TIMEOUT, data=json.dumps(data), headers=headers)

			elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:

				if not self.api_key:
					self.api_key = self.get_api_key()

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
				response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:{endpoint}', headers=headers,
											 timeout=self.TIMEOUT, data=json.dumps(data))

			else:
				response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:{endpoint}', params={'key': self.api_key},
											 timeout=self.TIMEOUT, data=json.dumps(data))

		except HTTPError as exception:

			if _retry_count <= self.MAX_RETRIES:

				logger.warning('request timed out. trying again…')

				sleep(_retry_count + 2)
				return self.get_response(messages=messages, tools=tools, model_name=model_name,
									   temperature=temperature, stream=stream, format_=format_,
										 _retry_count=_retry_count + 1, **kwargs)

			else:

				logger.error('request timed out too many times')
				raise exception

		if not response.ok:

			if response.status_code == 429:
				raise Exception(f'⚠️ Rate-limit reached {response.json()}')

			elif response.status_code == 400:
				raise Exception(f'400 Error: {response.text}')

			elif response.text.strip().lower() == 'none':
				raise Exception(
                    f'Gemini returned an invalid response: {response.text}. '
                    'This typically means your output token limit was exceeded. '
                    'Consider requesting a higher limit.'
                )
			else:
				logger.error(f'{response.text=}, {json.dumps(messages)}')

				# FloodGate token refresh
				if self.should_refresh_token(response):

						return self.get_response(messages=messages, tools=tools, model_name=model_name,
												temperature=temperature, stream=stream, format_=format_,
												_retry_count=_retry_count + 1, **kwargs)

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		candidates: list[dict] = response_json.get('candidates', [])
		return self.get_ai_client_response_for_message(message=candidates[0])

	def get_response_stream(self, messages: str|list[dict], model_name: str = None,
							tools: list[Tool] = None, temperature: float = 0.2, format_: str = None, _retry_count: int = 0,
							**kwargs) -> Iterator[AIClientResponse]:
		"""
		Returns streamed chat responses for given messages

		@param messages(str|list): a messages or string
		@param model_name(str): the name of the model to use
		@return (tuple): a response
		"""
		model_name = model_name or self.model_name
		messages = messages.copy() if isinstance(messages, list) else [self.create_message(role=self.ROLE_USER, content=messages)]

		options: dict[str, Any] = self.options.copy()

		if temperature is not None:
			options['temperature'] = temperature

		if format_ == 'json':
			options['response_mime_type'] = 'application/json'

		data: dict[str, Any] = {

			'safety_settings': [
				{'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},
				{'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},
				{'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'},
				{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},
			],
			'generationConfig' if self.credentials else 'generation_config': options,
		}

		if messages[0]['role'] == self.ROLE_SYSTEM:

			if 'content' in messages[0]:
				data['system_instruction'] = self.create_message(role=self.ROLE_SYSTEM, content=messages[0]['content'])

			elif 'parts' in messages[0]:
				data['system_instruction'] = self.create_message(role=self.ROLE_SYSTEM, content=messages[0]['parts'][0]['text'])

			else:
				data['system_instruction'] = messages[0]

			del messages[0]

		data['contents'] = messages

		if tools:
			data['tools'] = self.create_message_for_tools(tools)

		# [internal]
		if self.labels:
			data['labels'] = self.labels

		generator: Any = None

		try:

			if self.credentials or self.use_floodgate:

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
				generator = self.session.post(f'{self.base_url}/publishers/google/models/{model_name}:streamGenerateContent', timeout=self.TIMEOUT,
											 params={'alt': 'sse'}, data=json.dumps(data), headers=headers, stream=True)

			elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:

				if not self.api_key:
					self.api_key = self.get_api_key()

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
				generator = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:streamGenerateContent', headers=headers,
											  params={'alt': 'sse'}, timeout=self.TIMEOUT, data=json.dumps(data), stream=True)

			else:
				generator = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:streamGenerateContent',
											  params={'key': self.api_key, 'alt': 'sse'}, timeout=self.TIMEOUT, data=json.dumps(data), stream=True)

			with generator as response:

				if not response.ok:

					# retry on 429s
					if response.status_code == 429 and 'exp' not in model_name:

						logger.warning('request timed out. trying again…')

						yield from self.get_response_stream(messages=messages, model_name=model_name, temperature=temperature,
														format_=format_, _retry_count=_retry_count + 1, **kwargs)

					# FloodGate token refresh
					if self.should_refresh_token(response):

						yield from self.get_response_stream(messages=messages, model_name=model_name, temperature=temperature,
														   tools=tools, format_=format_, _retry_count=_retry_count + 1, **kwargs)
						return

					logger.error(f'could not stream ({response.text}) ({json.dumps(data)})')
					response.raise_for_status()
					return

				for line in response.iter_lines():

					line = line.decode().strip()

					if line.startswith('data: '):

						line_data: dict[str, Any] = json.loads(line.removeprefix('data: '))
						candidates: list[dict] = line_data.get('candidates', {})

						if not candidates:

							prompt_feedback: dict[str, Any] = line_data.get('promptFeedback')

							if 'blockReason' in prompt_feedback:
								raise Exception(f'The prompt was blocked by Google due to its content. Block reason: {prompt_feedback["blockReason"]}')

						yield self.get_ai_client_response_for_message(message=candidates[0])

					# this flow is never used unless `alt: sse` is not set above
					else:

						# the response is a json split across many lines
						# so we parse the line that has `"text": "…"` and yield the result
						if matches := re.findall(r'"text":', line, flags=re.MULTILINE):

							match: str = matches[0]
							message: dict[str, str] = json.loads(f'{{{line}}}')

							yield AIClientResponse(raw=message, content=message.get('text'), tool_calls=[])

		except Exception as exception:

			logger.exception(f'could not stream')
			raise

	def get_response_media(self, messages: str|list[dict], model_name: str = None,
							_retry_count: int = 0, **kwargs) -> AIClientResponse:
		"""
		Returns media generated by model for given message like image generation

		@param messages(str|list): a messages or string
		@param model_name(str): the name of the media model to use
		@return (tuple): a response
		"""

		if _retry_count > self.MAX_RETRIES:
			raise Exception('retried too many times')

		options: dict[str, Any] = self.options.copy()
		message = messages[-1]['parts'][0]['text'] if isinstance(messages, list) else messages

		data: dict[str, Any] = {

			'parameters': options,
			'instances': [{'prompt': message}]
		}

		response: Any = None
		endpoint: str = 'predict'

		try:
			if self.credentials or self.use_floodgate:

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
				response = self.session.post(f'{self.base_url}/publishers/google/models/{model_name}:{endpoint}',
											 timeout=self.TIMEOUT, data=json.dumps(data), headers=headers)

			elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:

				if not self.api_key:
					self.api_key = self.get_api_key()
				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
				response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:{endpoint}', headers=headers,
											 timeout=self.TIMEOUT, data=json.dumps(data))

			else:
				response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:{endpoint}', params={'key': self.api_key},
											 timeout=self.TIMEOUT, data=json.dumps(data))

		except HTTPError as exception:

			if _retry_count <= self.MAX_RETRIES:

				logger.warning('request timed out. trying again…')
				sleep(_retry_count + 2)
				return self.get_response_media(messages=messages, model_name=model_name,
										 _retry_count=_retry_count + 1, **kwargs)

			else:

				logger.error('request timed out too many times')
				raise exception

		if not response.ok:

			logger.error(f'{response.text=}, {json.dumps(messages)}')

			# FloodGate token refresh
			if self.should_refresh_token(response):
				return self.get_response_media(messages=messages, model_name=model_name,
											  _retry_count=_retry_count + 1, **kwargs)

			if response.status_code == 400:
				raise Exception(f'400 Error: {response.text}')

		response.raise_for_status()

		response_json: list[dict[str, Any]] = response.json()
		return self.get_ai_client_response_for_media(message=response_json)

	def get_response_video(self, messages: str|list[dict], model_name: str = None,
							_retry_count: int = 0, **kwargs) -> AIClientResponse:
		"""
		Returns video generated by Veo model for given message

		@param messages(str|list): a messages or string
		@param model_name(str): the name of the video model to use (veo-2.0-generate-001 or veo-3.0-generate-preview)
		@return (AIClientResponse): a response with video files
		"""

		if _retry_count > self.MAX_RETRIES:
			raise Exception('retried too many times')

		# Video-specific parameters
		video_params = ['durationSeconds', 'aspectRatio', 'sampleCount', 'negativePrompt', 'personGeneration', 'seed', 'storageUri', 'enhancePrompt', 'generateAudio']
		options: dict[str, Any] = self.options.copy()

		video_options = {k: v for k, v in options.items() if k in video_params}

		message = messages[-1]['parts'][0]['text'] if isinstance(messages, list) else messages

		image_data = None

		if isinstance(messages, list):

			for msg in messages:
				if 'parts' in msg:
					for part in msg['parts']:
						if 'inline_data' in part or 'file_data' in part:

							if 'inline_data' in part:

								image_data = {
									'bytesBase64Encoded': part['inline_data']['data'],
									'mimeType': part['inline_data']['mime_type']
								}

							elif 'file_data' in part:

								image_data = {
									'gcsUri': part['file_data']['file_uri'],
									'mimeType': part['file_data']['mime_type']
								}

							break

				if image_data:
					break

		instance_data = {'prompt': message}

		if image_data:
			instance_data['image'] = image_data

		data: dict[str, Any] = {
			'instances': [instance_data],
			'parameters': {
				'durationSeconds': video_options.get('durationSeconds', 8 if 'veo-3.0' in model_name else 8),
				'aspectRatio': video_options.get('aspectRatio', '16:9'),
				'sampleCount': video_options.get('sampleCount', 1),
				**{k: v for k, v in video_options.items() if k in ['negativePrompt', 'personGeneration', 'seed', 'storageUri', 'enhancePrompt']}
			}
		}

		# Add generateAudio for veo-3.0-generate-preview
		if 'veo-3.0' in model_name:
			data['parameters']['generateAudio'] = video_options.get('generateAudio', True)

		response: Any = None
		endpoint: str = 'predictLongRunning'

		try:

			if self.credentials or self.use_floodgate:

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
				response = self.session.post(f'{self.base_url}/publishers/google/models/{model_name}:{endpoint}',
											 timeout=500, data=json.dumps(data), headers=headers)

			elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:

				if not self.api_key:
					self.api_key = self.get_api_key()

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
				response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:{endpoint}', headers=headers,
											 timeout=500, data=json.dumps(data))

			else:
				response = self.session.post(f'{self.base_url}/v1beta/models/{model_name}:{endpoint}', params={'key': self.api_key},
											 timeout=500, data=json.dumps(data))

		except HTTPError as exception:

			if _retry_count <= self.MAX_RETRIES:

				logger.warning('request timed out. trying again…')
				sleep(_retry_count + 2)
				return self.get_response_video(messages=messages, model_name=model_name,
										 _retry_count=_retry_count + 1, **kwargs)
			else:

				logger.error('request timed out too many times')
				raise exception

		if not response.ok:

			logger.error(f'{response.text=}, {json.dumps(messages)}')

			# floodGate token refresh
			if self.should_refresh_token(response):
				return self.get_response_video(messages=messages, model_name=model_name, _retry_count=_retry_count + 1, **kwargs)

			if response.status_code == 400:
				raise Exception(f'400 Error: {response.text}')

			# Handle rate limiting for video generation with slow retry
			elif response.status_code == 429:

				if _retry_count < self.MAX_RETRIES:

					# exponential backoff with longer delays for video generation
					wait_time = min(60 * (2 ** _retry_count), 300)
					logger.warning(f'Video generation rate limited. Retrying in {wait_time} seconds… (attempt {_retry_count + 1}/{self.MAX_RETRIES})')
					sleep(wait_time)

					return self.get_response_video(messages=messages, model_name=model_name, _retry_count=_retry_count + 1, **kwargs)

				else:
					raise Exception(f'Video generation rate limit exceeded: {response.text}')

		response.raise_for_status()

		response_json: dict[str, Any] = response.json()
		operation_name = response_json.get('name')

		if not operation_name:
			raise Exception('No operation name returned from video generation request')

		return self._poll_video_operation(operation_name, model_name)

	def _poll_video_operation(self, operation_name: str, model_name: str, max_wait_time: int = 300) -> AIClientResponse:
		"""
		Polls a long-running video generation operation until completion using fetchPredictOperation

		@param operation_name(str): The operation name returned from the initial request
		@param model_name(str): The model name used for the operation
		@param max_wait_time(int): Maximum time to wait in seconds (default: 5 minutes)
		@return (AIClientResponse): a response with video files
		"""

		start_time = time.time()

		while time.time() - start_time < max_wait_time:

			if (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:
				fetch_url = f'{self.base_url}/v1beta/models/{model_name}:fetchPredictOperation'

			else:
				fetch_url = f'https://{self.location}-aiplatform.googleapis.com/v1/projects/{self.project_id}/locations/{self.location}/publishers/google/models/{model_name}:fetchPredictOperation'

			request_data = {'operationName': operation_name}

			if self.credentials or self.use_floodgate:

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}
				response = self.session.post(fetch_url, headers=headers, json=request_data, timeout=30)

			elif (self.api_key and self.api_key.startswith('in-')) or self.use_appleconnect:

				if not self.api_key:
					self.api_key = self.get_api_key()

				headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}
				response = self.session.post(fetch_url, headers=headers, json=request_data, timeout=30)

			else:
				response = self.session.post(fetch_url, params={'key': self.api_key}, json=request_data, timeout=30)

			if not response.ok:

				logger.error(f'Error polling operation: {response.text}')
				response.raise_for_status()

			operation_data = response.json()

			if operation_data.get('done', False):
				if 'error' in operation_data:
					raise Exception(f'Video generation failed: {operation_data["error"]}')

				result = operation_data.get('response', {})
				return self.get_ai_client_response_for_video(result)

			sleep(5)

		raise Exception(f'Video generation timed out after {max_wait_time} seconds')

	def get_ai_client_response_for_video(self, message: dict[str, Any]) -> AIClientResponse:
		"""
		Converts a raw video response from fetchPredictOperation to `AIClientResponse`

		@param message(dict): The response from fetchPredictOperation endpoint
		@return (AIClientResponse): a response with video files
		"""

		files: list[File] = []

		for video in message.get('videos', []):

			if 'bytesBase64Encoded' in video:

				content_type: str = video.get('mimeType', 'video/mp4')
				raw: str = video['bytesBase64Encoded']
				content: bytes = base64.b64decode(video['bytesBase64Encoded'])
				files.append(File(raw=raw, content=content, content_type=content_type))

			elif 'gcsUri' in video:

				content_type: str = video.get('mimeType', 'video/mp4')
				files.append(File(content_url=video['gcsUri'], content_type=content_type))

		return AIClientResponse(raw=message, files=files, content='Video generated successfully')

	""" Realtime """

	async def start_call(self, model_name: str = None,
						 tools: list[Tool] = None, options: dict = None) -> Call:
		"""
		Starts a Realtime session/call (a call with AI)

		@param model_name(str): The name of the model to use (e.g. gemini-2.0-flash-exp)
		@param options(dict): Session configuration
		"""

		options = options or self.options
		model_name = model_name or self.model_name

		headers: dict[str, Any] = {}
		url: str = f'wss://{self.location}-aiplatform.googleapis.com/ws/google.cloud.aiplatform.v1beta1.LlmBidiService/BidiGenerateContent'

		if self.credentials or self.use_floodgate:
			headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key if self.use_floodgate else self.credentials.token}'}

		elif self.api_key and self.api_key.startswith('in-'):
			{'Content-Type': 'application/json', 'Authorization': f'Bearer {self.api_key}'}

		call: Call = None

		if not Config.current.is_development:
			call = Call(websocket=await websockets.connect(url, additional_headers=headers, proxy=Config.current.PROXY_URL), client=self)

		else:
			call = Call(websocket=await websockets.connect(url, additional_headers=headers), client=self)

		instructions: str|dict = options.pop('systemInstruction', None) or options.pop('system_instruction', None)

		if instructions and isinstance(instructions, str):
			instructions = self.create_message(role=self.ROLE_SYSTEM, content=instructions)

		# add tools/Function Calling (if any)
		event: dict[str, Any] = {'model': model_name if self.api_key and self.api_key.startswith('in-') else
										  f'projects/{self.project_id}/locations/{self.location}/publishers/google/models/{model_name}',
								 'tools': {'function_declarations': [tool.to_dict(self) for tool in tools]},
								 'generationConfig': options, 'system_instruction': instructions}
		await call.send(event={self.EVENT_SETUP: event})
		await call.websocket.recv()

		return call

	""" Utilities """

	def get_ai_client_event_for_event(self, raw: dict|str) -> Event:
		"""
		Creates an Event given a raw event

		@param raw(dict|str): A raw event from the OpenAI API (via `.start_call()`)
		"""

		if isinstance(raw, bytes):
			raw = raw.decode()

		if isinstance(raw, str):
			raw = json.loads(raw)

		type: str = list(raw.keys())[0]

		content: str = ''
		tool_calls: list[ToolCall] = []
		audio_bytes: Optional[bytes] = None
		server_content: dict[str, Any] = raw.get('serverContent', {})

		if parts := server_content.get('modelTurn', {}).get('parts', []):

			for part in parts:

				if not content:
					content = part.get('text')

				inline_data: dict[str, str] = part.get('inlineData', {})

				if inline_data.get('mimeType') == 'audio/pcm' and (data := inline_data.get('data')):
					audio_bytes = base64.b64decode(data)

		if type == self.EVENT_FUNCTION_CALLS:

			for tool_call_raw in raw.get(type):
				tool_calls.append(ToolCall(id=tool_call_raw.get('id'), name=tool_call_raw.get('name'), raw_arguments=tool_call_raw.get('args')))

		elif type == self.EVENT_TOOL_CALL:

			tool_call_raw: dict[str, Any] = raw.get(type)

			for tool_call_raw in tool_call_raw.get('functionCalls'):
				tool_calls.append(ToolCall(id=tool_call_raw.get('id'), name=tool_call_raw.get('name'), raw_arguments=tool_call_raw.get('args')))

		if server_content.get('turnComplete'):
			type = self.EVENT_TEXT_DONE

		return Event(raw=raw, type=type, text=content, tool_calls=tool_calls,
					 audio_bytes=audio_bytes, is_speech_started=server_content.get('interrupted', False))

	def create_message(self, role: str, content: str, files: list[str|bytes|File] = None, **kwargs) -> dict[str, Any]:
		"""
		Creates a message that matches the format used by Google's Gemini

		@param role(str): the value of any of the `ROLE_` properties
		@param content(str): the text content of the message
		@param files(list): a list of file paths or bytes
		"""

		message: dict[str, Any] = {'role': role}
		parts: list[dict[str, Any]] = None

		# if this is a message from a tool/function, add the tool arguments
		if role == self.ROLE_TOOL:

			parts = [{

				'functionResponse': {
					'name': kwargs['name'],

					'response': {
						'name': kwargs['name'],
						'content': content
					}
				}
			}]

		else:
			parts = [{'text': content}]

		if files:

			for file in files:

				if isinstance(file, str):

					mime_type, _ = mimetypes.guess_type(file)

					# direct public urls are supported by Gemini
					if file.startswith(('http://', 'https://')):

						part_data: dict = {'file_data': {'file_uri': file, 'mime_type': mime_type}}

						if 'youtube' in file:
							part_data['file_data']['mime_type'] = "video/mp4"

						parts.append(part_data)
						continue

					num_bytes: float = os.path.getsize(file)
					mb_size: float = num_bytes / (1024 * 1024)

					# if unable to find `mime_type`, treat it like a image
					mime_type = mime_type or 'image/jpeg'

					if mb_size >= 20:
						logger.warning('Please pass file url instead of file name for better performance. Like File(content_type="...", content_url="...")')

					file_base64: str = self.encode_file(path=file)
					parts.append({'inline_data': {'mime_type': mime_type, 'data': file_base64}})

				elif isinstance(file, File):

					if file.content_url:
						parts.append({'file_data': {'mime_type': file.content_type, 'file_uri': file.content_url}})

					else:
						parts.append({'inline_data': {'mime_type': file.content_type, 'data': self.encode_bytes(bytes_=file.content)}})

				else:
					file_base64: str = self.encode_bytes(bytes_=file)
					parts.append({'inline_data': {'mime_type': 'image/jpeg', 'data': file_base64}})

		message['parts'] = parts
		return message

	def create_events(self, type: str, is_partial: bool = False, **kwargs) -> list[dict[str, Any]]:
		"""
		Creates events that matches the format used by OpenAI

		@param type(str): The type of message
		"""

		events: list[Event] = []

		# tool/function response
		if type == self.EVENT_TOOL_RESPONSE:

			tool_call: ToolCall = kwargs['tool_call']
			event_data = {
				'tool_response': {
					'function_responses': [{
						'name': tool_call.name,
						'response': {'result': {'string_value': kwargs['content']}}
					}]
				}
			}

			if tool_call.id is not None:
				event_data['tool_response']['function_responses'][0]['id'] = tool_call.id

			events.append(Event(raw=event_data, type=type))


		# reply to AI via text
		elif type == self.EVENT_ITEM_CREATE:

			events.append(Event(raw={
				'client_content': {
					'turn_complete': not is_partial,
					'turns': [{'role': kwargs.get('role'), 'parts': [{'text': kwargs.get('content')}]}],
				}
			}, type=type))

		# stream audio to AI
		elif type == self.EVENT_AUDIO_APPEND:
			audio_bytes: bytes | str = kwargs.get('audio_bytes')

			if isinstance(audio_bytes, bytes):
				audio_bytes = base64.b64encode(audio_bytes).decode()

			events.append(Event(raw={
				'realtime_input': {
					'media_chunks': [{'data': audio_bytes, 'mime_type': 'audio/pcm'}]
				}
			}, type=type))

		else:
			raise NotImplementedError(f'Unsupported event type ({type=}, {kwargs=})')

		return events


	def get_ai_client_response_for_message(self, message: dict[str, Any]) -> AIClientResponse:
		"""
		Converts a raw response message to `AIClientResponse`

		@param message(dict): e.g. `{'role': 'user', 'content': 'Lorem ipsum'}`
		@return (AIClientResponse): a response
		"""

		content: str = ''
		files: list[File] = []
		metadata: Any = message
		tool_calls: list[ToolCall] = []

		# the `.get()` fallback is for any caller (including Amoeba), which
		# passes the message with `content` unwrapped
		if parts := message.get('content', message).get('parts', []):

			for part in parts:

				if not content and 'text' in part:
					content = part.get('text')

				elif 'text' in part:

					# use the last text block for the content since
					# multiple are generated when using code execution in Gemini
					content += part.get('text')

				if 'inlineData' in part:

					inline_data = part.get('inlineData')
					content_type: str = inline_data.get('mimeType')
					raw: bytes = inline_data.get('data')
					files.append(File(raw=raw, content=base64.b64decode(raw), content_type=content_type))

				tool_call_dict: str = part.get('functionCall')

				if tool_call_dict:
					tool_calls.append(ToolCall(id=None, name=tool_call_dict.get('name'), raw_arguments=json.dumps(tool_call_dict.get('args'))))

		return AIClientResponse(raw=message.get('content'), content=content, files=files, tool_calls=tool_calls, metadata=metadata)

	def get_ai_client_response_for_media(self, message: list[dict[str, Any]]) -> AIClientResponse:
		"""
		Converts a raw response message to `AIClientResponse`

		@param message(dict): e.g. `[{'bytesBase64Encoded': 'Base 64 encoded file', 'mimeType': 'image/png'}]`
		@return (AIClientResponse): a response
		"""

		files: list[File] = []
		for prediction in message.get('predictions', []):

			if 'bytesBase64Encoded' in prediction:

				content_type: str = prediction.get('mimeType')
				raw: str = prediction['bytesBase64Encoded']
				content: bytes = base64.b64decode(prediction['bytesBase64Encoded'])
				files.append(File(raw=raw, content=content, content_type=content_type))

		return AIClientResponse(raw=message, files=files)

	def get_dictionary_for_base_model(self, base_model: 'BaseModel') -> dict:
		"""
		Converts a given Pydantic model to a dictionary

		@param base_model(BaseModel): a Pydantic model
		"""

		from pydantic.fields import PydanticUndefined
		from typing import Union, get_origin, get_args

		def get_python_type_info(annotation: Any) -> dict[str, Any]:
			"""
			Extract type information from Python type annotations

			@param annotation(Any): The type annotation to extract information from
			@return (dict[str, Any]): Dictionary containing type information
			"""

			arguments: list = get_args(annotation)
			origin: Any = get_origin(annotation)

			# union
			if origin is Union:

				# check for optional (`Union` with `None`)
				non_none_types = [argument for argument in arguments if argument is not type(None)]

				if len(non_none_types) == 1 and type(None) in arguments:

					result = get_python_type_info(non_none_types[0])
					result['nullable'] = True
					return result

				# union
				else:
					return {'anyOf': [get_python_type_info(argument) for argument in arguments if argument is not type(None)]}

			# list/Array types
			if origin is list or annotation is list:

				result = {'type': 'ARRAY'}

				if arguments:
					result['items'] = get_python_type_info(arguments[0])

				return result

			# dict/object types
			if origin is dict or annotation is dict:
				return {'type': 'OBJECT'}

			# basic types
			if annotation is str:
				return {'type': 'STRING'}

			elif annotation is int:
				return {'type': 'INTEGER', 'format': 'int64'}

			elif annotation is float:
				return {'type': 'NUMBER', 'format': 'double'}

			elif annotation is bool:
				return {'type': 'BOOLEAN'}

			# nested `BaseModel`
			if isinstance(annotation, type) and inspect.isclass(annotation) and issubclass(annotation, BaseModel):
				return convert_model_to_schema(annotation)

			return {'type': 'STRING'}

		def convert_model_to_schema(model_class: type[BaseModel]) -> dict[str, Any]:
			"""
			Converts a Pydantic model class to schema format

			@param model_class(type[BaseModel]): The Pydantic model class to convert
			@return (dict[str, Any]): The schema representation of the model
			"""

			schema: dict[str, Any] = {

				'required': [],
				'type': 'OBJECT',
				'properties': {},
			}

			for field_name, field_info in model_class.model_fields.items():

				field_schema = get_python_type_info(field_info.annotation)

				if field_info.title:
					field_schema['title'] = field_info.title

				if field_info.description:
					field_schema['description'] = field_info.description

				# default values
				if field_info.default is not PydanticUndefined:
					field_schema['default'] = field_info.default

				for item in field_info.metadata:

					constraint_type = type(item).__name__

					if constraint_type == 'Ge':
						field_schema['minimum'] = item.ge

					elif constraint_type == 'Le':
						field_schema['maximum'] = item.le

					elif constraint_type == 'Gt':
						field_schema['minimum'] = item.gt

					elif constraint_type == 'Lt':
						field_schema['maximum'] = item.lt

					elif constraint_type == 'MinLen':
						field_schema['minLength'] = str(item.min_length)

					elif constraint_type == 'MaxLen':
						field_schema['maxLength'] = str(item.max_length)

				schema['properties'][field_name] = field_schema

				if field_info.is_required():
					schema['required'].append(field_name)

			return schema

		return convert_model_to_schema(base_model)

	""" Google AI-specific """

	def append_message(self, message: dict[str, Any], messages: list[dict]) -> None:
		"""
		[Internal]
		Merges subsequent Function Call responses into one message to
		match Google AI's (current) API schema

		Once the schema is stable, this function will likely be replaced
		with a more permanent option
		"""

		if message.get('role') == self.ROLE_TOOL and messages and messages[-1].get('role') == self.ROLE_TOOL:

			messages[-1]['parts'] += message['parts']
			return

		messages.append(message)

	def create_message_for_tools(self, tools: list[Tool]) -> list[dict]:
		"""
		[Internal]
		Merges google defined tools and user defined tools
		"""
		tools_message = [tool.to_dict(self) for tool in tools if not tool.has_code]
		user_tools = [tool.to_dict(self) for tool in tools if tool.has_code]

		if user_tools:
			tools_message.append({'function_declarations': user_tools})

		return tools_message

	def get_api_key(self) -> str:
		"""
		[Internal]
		Uses AppleConnect to return the user's active API Key

		@return (str): The user's API Key (e.g. `in-…`)
		"""

		daw_token: str = Utilities.get_apple_connect_token(app_id=Config.current.IDMS_APP_ID)

		cookies: dict[str, Any] = {'acack': daw_token}
		headers: dict[str, Any] = {'Content-Type': 'application/json'}

		response = self.session.post(f'{self.base_url}/keys/appleconnect', headers=headers,
									 cookies=cookies, json={'client_name': 'googleaiclient'}, timeout=20)

		response_json: dict[str, Any] = response.json()
		value: str = response_json['value']

		if not value:
			raise Exception('🔑 You do not have an API Key created yet. Create one in interlinked.apple.com/playground › API Keys, then run again.')

		return value

	def get_should_skip_event(self, event: Event) -> bool:
		"""
		Returns whether `AI.ask(…)` should skip a given `Event`

		Some events are irrelevant for the user to listen to

		@param event(Event): the event from the AI model
		@return (bool): Whether the user should skip this event (not an important event)
		"""
		return False

if __name__ == '__main__':
	print(GoogleAIClient(use_floodgate=True).get_response(messages='Hi! What\'s your name?'))
