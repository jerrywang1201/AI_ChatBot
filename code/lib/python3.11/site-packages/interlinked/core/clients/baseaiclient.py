import json
import base64
import logging
import asyncio
import mimetypes
import websockets
from typing import Any, Optional
from websockets import WebSocketClientProtocol
from websockets.asyncio.client import ClientConnection
from pydantic import BaseModel, ConfigDict, field_serializer, Field

logger = logging.getLogger(__name__)


class File(BaseModel):
	"""
	[Internal]
	"""

	model_config = ConfigDict(extra='allow')

	# the ID of the file (some AI models include it)
	id: str | None = None

	# the raw file object from the client
	raw: Any | None = None

	# e.g. `image/png`
	content_type: str | None = None

	content: str | bytes | None = None

	# e.g. Url for the file in remote storage
	content_url: str | None = None

	@property
	def base64_representation(self) -> str:
		"""
		Converts the contents of this file to Base64
		"""

		if not self.content:
			return

		return base64.b64encode(self.content).decode()

	@property
	def mime_type(self) -> str:
		"""
		Find mime type of the file contents
		"""

		if (not self.content_type) and self.content_url:

			mime_type, _ = mimetypes.guess_type(self.content_url)
			return mime_type

		return self.content_type


class ToolCall(BaseModel):
	"""
	This class is created by an AI Client when calling `AI.ask(…, tools=[…])` and AI responds with
	a tool/function call.
	"""

	# some AI clients/models generate this field
	# and is only used for their internal uses
	id: str | None = None

	# not currently used
	type: str = 'function'

	# the name of the `Tool` (e.g. `lookup_radar`)
	name: str | None

	# the raw arguments returned in the response
	# typically, a string containing a dictionary
	raw_arguments: str | dict | None

	@property
	def arguments(self) -> dict[str, Any]:
		"""
		The arguments AI is passing to this tool/function
		"""

		if isinstance(self.raw_arguments, dict):
			return self.raw_arguments

		from interlinked.core.tool import Tool
		return Tool.get_dictionary_from_string(string=self.raw_arguments)

	@arguments.setter
	def arguments(self, value: dict[str, Any]) -> None:
		"""
        Setter for arguments property - updates the raw_arguments field
        """
		self.raw_arguments = value

	@property
	def display_arguments(self) -> str:
		"""
		[Internal]
		Returns a printable/loggable version of the arguments

		e.g. radar_id='123456'

		Used in `AI.ask(…)` to log what AI is passing to each function
		"""

		display_arguments: list[str] = []

		for key, value in self.arguments.items():

			display_value: str = None

			# hide the value if the argument is in the commonly-named list for secrets
			if key in {'password', 'secret', 'api_key', 'credentials', 'token'}:
				display_value = '*****'

			else:

				# escape some characters, then truncate
				# e.g. `What's your name?` › `What\'s your name?`
				# 	   `What\'s your name?` › `What\'s your…`
				display_value = str(value)
				display_value = display_value.replace('\'', '\\\'').replace('\n', '')
				display_value = f'\'{display_value[:20]}…\'' if len(display_value) > 20 else f'\'{display_value}\''

			display_arguments.append(f'{key}={display_value}')

		return ', '.join(display_arguments)


class ToolResponse(BaseModel):
	"""
	[Internal/Not used yet]
	This class can be used by functions/tools to return images and other
	content in addition to text
	"""

	content: Optional[Any] = None
	files: Optional[File] = None


class AIClientResponse(BaseModel):
	"""
	This class is returned by the `.get_response()` function in each AI client.
	The response is used internally by `AI.ask(…)`, then converted into an `Observation` class.
	"""

	model_config = ConfigDict(extra='allow')

	# the raw response object from the client
	raw: Any

	# the text content of AI's response
	content: str | None = None

	# files returned in AI's response
	files: list[File] | None = None

	# a list of the functions/tools AI would like to call
	tool_calls: list[ToolCall] | None = []

	# generic metadata for grounding, code execution, etc
	metadata: Any = None


class AIModel(BaseModel):
	"""
	This class is returned when calling an AI client's `.get_models()` function.
	It contains information about each model, such as name, description, and capabilities.

	Examples:
	```
	from interlinked.core.clients.ollamaclient import OllamaClient

	print(OllamaClient.get_models())
	# [AIModel(…), AIModel(…)]
	```
	"""

	# the name or ID of the model
	name: str

	# the description of the model (could contain relevant metadata, such as parameters)
	description: str | None = None

	# whether this model is fine-tuned
	# e.g. fine-tuned on your team's data
	is_fine_tuned: bool = False

	# whether this model supports faster responses
	# (word/chunk by chunk). You can then use it in `for observation in AI.ask(…, stream=True):`
	supports_stream: bool = True

	# raw response for the AI model (excluded from `__repr__`)
	raw: Any = Field(default=None, repr=False)


class BaseAIClient:
	"""
	This is the base class that all AI clients inherit
	"""

	ROLE_USER: str = 'user'
	ROLE_SYSTEM: str = 'system'
	ROLE_TOOL: str = 'function'
	ROLE_ASSISTANT: str = 'assistant'

	# [Internal] Interlinked's own events
	# which are translated by each client to the actual event types
	EVENT_TOOL_CALL: str = 'tool_call'
	EVENT_TEXT_DELTA: str = 'text_delta'
	EVENT_AUDIO_DELTA: str = 'audio_delta'
	EVENT_TOOL_RESPONSE: str = 'tool_response'
	EVENT_AUDIO_TRANSCRIPT_DELTA: str = 'audio_transcript_delta'

	MAX_RETRIES: int = 4

	def get_embedding(self, input: str, model_name: str = None) -> list[float]:
		raise NotImplementedError()

	def get_response(self, messages: str|list[dict], files: list[bytes] = None, tools: list[dict[str, Any]] = None,
					 model_name: str = None, temperature: float = 0.2, format_: str = None, **kwargs) -> AIClientResponse:
		raise NotImplementedError()

	def get_completion(self, prompt: str|list[dict], model_name: str = None,
					   temperature: float = 0.2, **kwargs) -> AIClientResponse:
		raise NotImplementedError()

	def get_models(self, dsid: int = None) -> list[AIModel]:
		raise NotImplementedError()

	def get_token_count(self, input: str, model_name: str = None, _retry_count: int = 0) -> int:
		raise NotImplementedError()

	def fine_tune(self, data_path: str, model_name: str = None, **kwargs) -> str:
		raise NotImplementedError()

	""" Utilities """

	def get_ai_client_response_for_message(self, message: dict[str, Any]) -> AIClientResponse:
		raise NotImplementedError()

	def create_message(self, role: str, content: str, files: list[str|bytes] = None, **kwargs) -> dict[str, Any]:
		raise NotImplementedError()

	def append_message(self, message: dict[str, Any], messages: list[dict]) -> None:
		"""
		Only needed since some AI clients (e.g. GoogleAIClient)
		combine parallel Function Call responses into one message
		"""
		messages.append(message)

	def encode_file(self, path: str) -> str:
		"""
		[Internal]
		Encodes a given file to string

		If the client inheriting this class uses a different encoding method,
		override `encode_bytes` instead

		@param path(str): a path to a file to encode
		@return (str): an encoded representation of the file
		"""

		with open(path, 'rb') as file:
			return self.encode_bytes(bytes_=file.read())

	def encode_bytes(self, bytes_: bytes) -> str:
		"""
		[Internal]
		Encodes a given file in base64

		If the client inheriting this class uses a different encoding method,
		you can override this function with a custom return type

		@param path(str): a path to a file to encode
		@return (str): a base64 representation of the file
		"""
		return base64.b64encode(bytes_).decode()

	def get_role_for_message(self, message: dict[str, Any]) -> int|str:
		"""
		[Internal]
		Returns the role of a given message.

		Since each client may have its own definition of a role, clients
		can override this function to return the role

		@param message(dict): a message (from AI, user, tool)
		@return (str): a role (e.g. `assistant`)
		"""
		return message['role']

	def model_dump(self) -> dict[str, Any]:
		"""
		[Internal]
		Used by `@AI.track`

		Only includes relevant fields
		"""
		return {'options': self.options, 'model_name': self.model_name,
				'embedding_model_name': self.embedding_model_name}


class Event(BaseModel):
	"""
	Events are sent and recieved during a Call
	"""

	# if there is an ID associated with this event
	id: Optional[str|int] = None

	# the raw type of this event, returned by the AI client
	# e.g. `response.audio_transcript.delta`
	type: str | int

	# if AI responded over text
	text: Optional[str] = None

	# if AI responded with an image
	image_bytes: Optional[bytes] = None

	# if AI responded with audio
	audio_bytes: Optional[bytes] = None

	# if AI responded with a video
	video_bytes: Optional[bytes] = None

	# a list of the functions/tools AI would like to call
	tool_calls: list[ToolCall] | None = []

	# when the user starts speaking
	is_speech_started: Optional[bool] = False

	# when the user stops speaking
	is_speech_stopped: Optional[bool] = False

	# the raw version of this `Event`, returned by the AI client
	raw: dict

	# if this event is an error, this would contain the error itself
	error: Optional[dict|str] = None

	@field_serializer('audio_bytes')
	def serialize_audio_bytes(self, audio_bytes: bytes, _info) -> str:
		"""
		[Internal]
		Converts `audio_bytes` to base64 when calling `event.model_dump()`
		"""
		return base64.b64encode(audio_bytes).decode() if audio_bytes else None


class Call(BaseModel):
	"""
	This class is automatically created when you use `AI.ask(realtime=True)`

	It represents a realtime phone Call with AI. During the Call, any time
	you (the user) or AI talk, an `Event` is created.

	Examples of an `Event`:
	- Speech started (you started talking)
	- Speech stopped (you stopped talking)

	You can access the events by calling `observation = AI.ask(realtime=True)`, then:
	```python
	async for event_observation in observation:
		print(event_observation.event)
	```
	"""

	model_config = ConfigDict(arbitrary_types_allowed=True)

	client: BaseAIClient
	websocket: WebSocketClientProtocol | ClientConnection = None

	def __aiter__(self):
		return self

	async def __anext__(self) -> Event:
		"""
		Allows iterating over events using:
		```python

		async for event in call:
			print(event)
		```
		"""

		if self.websocket is None:
			raise StopAsyncIteration

		try:

			raw_event: dict = await self.websocket.recv()
			return self.client.get_ai_client_event_for_event(raw=raw_event)

		except websockets.ConnectionClosed:

			await self.end()
			raise StopAsyncIteration

		except asyncio.CancelledError:

			await self.end()
			raise asyncio.CancelledError

		except Exception as exception:

			logger.error(f'error during a call with AI: {exception}', exc_info=True)
			raise exception

	async def send(self, event: dict[str, Any]) -> None:
		"""
		Sends an event to AI using Realtime
		"""

		if not self.websocket:
			raise Exception('Please call `.start_call(…)` first')

		await self.websocket.send(json.dumps(event))

	async def end(self):
		"""
		Ends the call with AI
		"""

		if self.websocket:
			await self.websocket.close()
