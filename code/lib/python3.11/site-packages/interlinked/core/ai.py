import os
import re
import sys
import json
import base64
import random
import logging
import asyncio
import inspect
from textwrap import dedent
from itertools import takewhile
from collections.abc import Iterator
from qdrant_client.models import Filter
from types import FrameType, TracebackType
from typing import Any, Callable, Optional, Pattern, Self
from interlinked.core.clients.qdrantclient import QdrantClient
from pydantic import BaseModel, ConfigDict, Field as PydanticField, computed_field, \
					 field_validator

from interlinked.core.applet import Applet
from interlinked.core.config import Config
from interlinked.core.utilities import Utilities
from interlinked.core.clients.mlxclient import MLXClient
from interlinked.core.clients.ajaxclient import AJAXClient
from interlinked.core.clients.endorclient import EndorClient
from interlinked.core.clients.openaiclient import OpenAIClient
from interlinked.core.clients.ollamaclient import OllamaClient
from interlinked.core.clients.googleaiclient import GoogleAIClient
from interlinked.core.clients.anthropicclient import AnthropicClient
from interlinked.core.clients.basevectorstoreclient import BaseVectorStoreClient
from interlinked.core.tool import Tool, MissingArguments, RunToolAgain, PendingToolResponse
from interlinked.core.clients.baseaiclient import BaseAIClient, AIClientResponse, ToolCall, ToolResponse, File, Call, Event

logger = logging.getLogger(__name__)


class Field:
	"""
	Represents a pre-defined field in a `Template`
	"""

	TYPE_INT: str = 'int'
	TYPE_ENUM: str = 'enum'
	TYPE_BOOL: str = 'bool'
	TYPE_JSON: str = 'json'
	TYPE_CODE: str = 'code'
	TYPE_FLOAT: str = 'float'
	TYPE_STRING: str = 'string'

	TYPES: set[str] = {TYPE_INT, TYPE_ENUM, TYPE_BOOL, TYPE_JSON, TYPE_CODE, TYPE_FLOAT, TYPE_STRING}

	field_dict: dict[str, Any] = None

	# e.g. `technical_component`
	name: str = None

	# e.g. `Technical Component`
	name_raw: str = None

	# helpful when the field name is too long
	# e.g. `Is the comment asking to use a can?` -> `is_can`
	alternative_name: str = None

	# e.g. `name of vulnerable product, service, project, or function mentioned in the report`
	description: str = None

	type: str = None
	required: bool = False

	# minimum/maximum value (for int/float)
	min_: int = None
	max_: int = None

	# the minimum/maximum length of the field value
	min_length: int = None
	max_length: int = None

	# the maximum number of words the field value is expected to have
	# default is 5
	max_segments: int = None

	enum_options: list[str] = []
	invalid_enum_options: list[str] = []

	def __init__(self, name: str, description: str, field_dict: dict[str, Any]):

		name = name.strip()
		description = description.strip()

		self.name_raw = name
		self.description = description
		self.name = name.lower().replace(' ', '_')

		self.field_dict = field_dict

		self.required = self.field_dict.get('required') == 'true'
		self.type = self.field_dict.get('type', Field.TYPE_STRING)
		self.alternative_name = self.field_dict.get('alternative_name')
		self.min_length = int(self.field_dict.get('min_length')) if 'min_length' in self.field_dict else None
		self.max_length = int(self.field_dict.get('max_length')) if 'max_length' in self.field_dict else None
		self.max_segments = int(self.field_dict.get('max_segments')) if 'max_segments' in self.field_dict else 5

		if self.type == Field.TYPE_ENUM:

			self.enum_options = self.field_dict.get('enum_options')
			self.invalid_enum_options = self.field_dict.get('invalid_enum_options')

			# if the field is an `enum` and `max_segments` is smaller than one of the enum options
			# e.g. `max_segments = 1`, but `enum_options = [enhancement_request]` (2 segments), raise an exception
			if self.max_segments and self.enum_options:

				if any(enum_option for enum_option in self.enum_options if len(enum_option.replace('_', ' ').split('|')[0].split()) > self.max_segments):
						raise UserError(f'The field "{self.name_raw}" has `max_segments` set to {self.max_segments}, but one of the enum options is longer')

		elif self.type in {Field.TYPE_INT, Field.TYPE_FLOAT}:

			# min/max values
			if 'min' in self.field_dict:

				min_: float|int = self.field_dict.get('min')
				self.min_ = float(min_) if '.' in min_ else int(min_)

			if 'max' in self.field_dict:

				max_: float|int = self.field_dict.get('max')
				self.max_ = float(max_) if '.' in max_ else int(max_)

		elif self.type not in self.TYPES:
			raise UserError(f'Unknown field type ({self.type}). valid options: {", ".join(self.TYPES)}')

		# must have a description
		if not self.description:
			raise UserError(f'The field "{self.name_raw}" is missing a description')

	def __str__(self):
		return f'<Field name={self.name}, field_dict={self.field_dict}>'

	__repr__ = __str__

	def get_is_valid(self, value: Any) -> tuple[bool, str]:
		"""
		Validates a given value for this field

		@param value(Any): a value for this field
		@return (tuple): (bool): whether the value is valid, (str): reason if not valid
		"""

		if self.required and value is None:
			return False, 'Rewrite your response in the correct format'

		# enum fields
		# check if the field value is an invalid enum option (one that the template says is invalid)
		if self.type == Field.TYPE_ENUM:

			if self.invalid_enum_options:

				is_enum_option_valid: bool = True

				# e.g. `value = 'macOS'` and `invalid_enum_options` has `macOS`
				if isinstance(value, str):

					# e.g. `macos ventura`
					field_raw_value_lower: str = value.raw.lower()

					# e.g. `['macOS', 'iOS *']`
					for invalid_enum_option in self.invalid_enum_options:

						# e.g. `macos`
						invalid_enum_option_lower: str = invalid_enum_option.lower()

						# support options that start or end with `*`, and exact values
						if invalid_enum_option.startswith('*'):

							if field_raw_value_lower.endswith(invalid_enum_option_lower.removeprefix('*')):
								is_enum_option_valid = False

						elif invalid_enum_option.endswith('*'):

							if field_raw_value_lower.startswith(invalid_enum_option_lower.removesuffix('*')):
								is_enum_option_valid = False

						elif invalid_enum_option_lower == field_raw_value_lower:
							is_enum_option_valid = False

				elif isinstance(value, int):

					if int(invalid_enum_option) == value:
						is_enum_option_valid = False

				if not is_enum_option_valid:
					return False, f'The "{self.name_raw}" is something else. Think step by step then respond in the correct format'

			# if there's a value and the value does not match any of the available options
			if value and self.enum_options and not any(str(value).lower() in enum_option.replace(' ', '_') for enum_option in self.enum_options):
				return False, f'The "{self.name_raw}" must be any of [{", ".join(self.enum_options)}]'

			# if the field value is too short
			if self.min_length and value and len(value.raw) < self.min_length:
				return False, f'Your response for "{self.name_raw}" should be at least {self.min_length} characters.'

			# if the field value is too long
			if self.max_length and value and len(value.raw) > self.max_length:
				return False, f'Your response for "{self.name_raw}" is too long. It should be less than {self.max_length} characters.'

			# if the value is a list (e.g. `safari or webkit`)
			if value and (' or ' in value.raw or ' and ' in value.raw or ', ' in value.raw):
				return False, f'"{self.name_raw}" should be one value, not multiple. Respond in the correct format'

			# if the field value has too many words
			if self.max_segments is not None and value and (value_split := value.raw.split()) and len(value_split) > self.max_segments:

				if len(value_split) > self.max_segments * 2:
					return False, f'Your response for "{self.name_raw}" is not valid. It should be less than {self.max_segments} words'

				else:
					return False, f'"{value.raw}" is not valid. The expected "{self.name_raw}" should be less than {self.max_segments} words'

		# bool fields
		elif self.type == Field.TYPE_BOOL and value is not None and not isinstance(value, bool):
			return False, f'Your response for "{self.name_raw}" should be just an bool (Yes/No).'

		# number fields
		elif self.type in {Field.TYPE_INT, Field.TYPE_FLOAT}:

			# if the value was not converted to an int/float earlier,
			# it means it's not a number
			if not isinstance(value, (int, float)):

				if self.type == Field.TYPE_INT:
					return False, f'Your response for "{self.name_raw}" should be just an integer.'

				else:
					return False, f'Your response for "{self.name_raw}" should be just a float.'

			elif self.type == Field.TYPE_INT and isinstance(value, float):
				return False, f'Your response for "{self.name_raw}" must be an integer.'

			# if the value is too low or too high
			if self.min_ and value < self.min_:
				return False, f'Your response for "{self.name_raw}" is too low. It should higher than {self.min_ - 1}'

			elif self.max_ and value > self.max_:
				return False, f'Your response for "{self.name_raw}" is too high. It should lower than {self.max_ + 1}'

		# JSON fields
		elif self.type == Field.TYPE_JSON and not isinstance(value, (dict, list)):
			return False, f'Your response for "{self.name_raw}" should be a correct JSON format.'

		# make sure the code field is valid markdown
		elif self.type == Field.TYPE_CODE:

			# value type is always `Code`
			if isinstance(value, Code) and not (value.raw_literal.startswith('`')):
				return False, f'The "{self.name_raw}" must be valid code wrapped with backticks'

		# if the field value is too short
		if self.min_length and value is not None and len(str(value)) < self.min_length:
			return False, f'Your response for "{self.name_raw}" should be at least {self.min_length} characters.'

		# if the field value is too long
		if self.max_length and value is not None and len(str(value)) > self.max_length:
			return False, f'Your response for "{self.name_raw}" is too long. It should be less than {self.max_length} characters.'

		# if the value is equivalent to the field description
		if self.required and str(value).lower().strip() == self.description.lower():
			return False, f'Your response for "{self.name_raw}" is missing/invalid.'

		return True, None


class Template:
	"""
	The string that is used in `AI.ask(…, template=…)`.
	It contains a list of fields and tools, and can be populated with values.

	e.g.
	```
	You are a software engineer who is excellent at Python. Your task is to review the code below
	written by another AI and determine if it high quality and uses type-hinting.

	Let's respond with the exact format:
	comment<!type:string,required:true!>: Your brief comments on the code on how to improve the code
	high_quality<!type:bool,required:true!>: Yes/No. If the code is high quality

	---
	Code:
	<#response#>
	```

	This class and the `AI.ask(…, template=…)` argument are optional. If you do not use them and only
	use `AI.ask(prompt=…)`, Interlinked automatically converts your prompt into `Template` behind-the-scenes.

	"""

	# the instructions/text of the template
	text: str = None

	# available options:
	# - None: (default) free-form text or if you have fields requested in your prompt (e.g. `high_quality<!type:bool!>: …`)
	# - json: ensures the entire response is in JSON (only compatible with OpenAIClient, GeminiClient, and OllamaClient)
	format_: str = None

	# if format is `None` (default) and you have fields
	# set in the text (e.g. `high_quality<!type:bool!>: …`), this will contain
	# a dictionary of each field defined
	fields: dict[str, dict] = {}

	def __init__(self, text: str = None, format_: str|BaseModel = 'auto', require_macros: bool = False):
		"""
		@param text(str): the text content of the template
		@param format_(str|BaseModel): (Optional) The desired format of the output (e.g. "json". see available options above)
		@param require_macros(bool): (Internal) if False, lines that seem to be fields will be treates as fields
		"""

		self.fields = {}
		self.format_ = format_

		if self.format_ and not (inspect.isclass(self.format_) and issubclass(self.format_, BaseModel)) and (not isinstance(self.format_, str) or self.format_ not in {'json', 'auto'}):
			raise UserError('Only Template(…, format_=\'json\') is currently supported')

		if text:

			self.text = text
			self.fields = Template.get_fields_for_text(text=self.text, require_macros=require_macros)

		elif self.__doc__ and self.__doc__ != Template.__doc__:
			self.text = dedent(self.__doc__).strip().replace('\n', ' ')

		else:
			raise UserError('The Template has no text set (e.g. `Template(text=…)`)')

		# if there's a format set, make sure the template mentions it
		# this also matches OpenAI's built-in checks, but here, we do it
		# for all models
		if self.format_ and isinstance(self.format_, str) and self.format_ != 'auto' and self.format_.lower() not in text.lower():
			raise UserError(f'Please edit your prompt or template to explicitly ask AI to respond in {format_} (e.g. "Respond with {format_}")')

	@property
	def display_format(self) -> str:
		"""
		[Internal]
		Returns the resolved format to use in `AIClient`
		"""

		display_format: str = self.format_

		if self.format_ == 'auto' and not self.fields:

			if re.findall(r'(?:in|with|exact)(?:\W+|\W+a\W+)JSON', self.text, flags=re.IGNORECASE | re.MULTILINE):
				display_format = 'json'

			else:
				display_format = None

		return display_format

	@classmethod
	def get_fields_for_text(cls, text: str, require_macros: bool) -> dict[str, Field]:
		"""
		[Internal]
		Sets `fields` based on the template

		@param require_macros(bool): if False, lines that seem to be fields will be treates as fields
		"""

		fields: dict[str, dict] = {}

		# extract enums
		# e.g. `Importance: available options: [critical, somewhat_important, not_security]`
		for line in text.splitlines():

			if not (':' in line or '?' in line):
				continue

			line = line.strip()

			field_dict: dict[str, Any] = {}

			# get macros
			macros: dict[str, str] = {}
			macros_raw: str = re.findall(r'\<\!(.*?)\!\>', line)

			if macros_raw:

				if len(macros_raw) > 1:
					raise Exception(f'must only have one macros group per line ({line=})')

				macros_raw = macros_raw[0]

				# remove the macro group from the line (so AI doesn't process it)
				line = line.replace(f'<!{macros_raw}!>', '')

				for macro_raw in macros_raw.split(','):

					# remove any surrounding spaces, then read each macro
					# e.g. `<!type:enum, required:true !>` -> `[('type', 'enum'), ('required', 'true')]`
					macro_raw_split: list[str] = macro_raw.strip().split(':')

					# skip if this isn't an actual macro
					if len(macro_raw_split) < 2:
						continue

					macro_key, macro_value = macro_raw_split

					# e.g. `invalid_enum_options:macOS|iOS*|…`
					if macro_key == 'invalid_enum_options':
						macro_value = macro_value.split('|')

					macros[macro_key] = macro_value

			# if macros are required and we don't have any, we won't assume that this is a field
			if require_macros and not macros:
				continue

			# check again if this is still a valid field definition
			# after removing the macros
			# e.g. `<!type:enum, required:true !> text` -> `text` (invalid)
			#      `Field<!type:enum, required:true !>: text` -> `Field: text` (valid)
			if not (':' in line or '?' in line):
				continue

			# if the line starts with `:` or `?` and we don't have any macros
			# skip it, as it is not a field
			if line.startswith((':', '?')) and not macros:
				continue

			# merge macros with the field dict
			field_dict |= macros
			field_name, field_description = line.split(':' if ':' in line else '?', 1)

			# if the field name is prefixed with `-` or `*`
			# e.g. `- Name` -> `Name`
			field_name = field_name.removeprefix('-').removeprefix('*').strip()

			# Skip lines like "* : input [1,2,3]" where the field name is just a bullet or other non-alphanumeric character
			if not field_name.replace(' ', '').replace('_', '').isalpha() and not macros:
				continue

			# reserved field names
			if field_name.lower() in {'options', 'response', 'template', 'is_valid', 'is valid'}:

				# don't raise an error unless this is an actual field
				# the user defined
				if not field_dict:
					continue

				raise UserError(f'"{field_name}" is a field name used internally in Interlinked. Please use another name')

			# find a list (e.g. `[Critical, somewhat important, not security]`)
			enum_option_matches: list[str] = re.findall(r'\[.*\]', field_description)

			# the template should only have one list. if we have more, we should raise an exception
			# so the user updates it
			if len(enum_option_matches) > 1:
				raise UserError(f'The template should not have multiple lists: {line}')

			# skip lines that don't have enum values, macros, or is likely not a field
			# (by looking at the name/empty description, with no macros)
			if not enum_option_matches and not macros and  \
				((not field_name.replace(' ', '').replace('_', '').isalpha() or len(field_name.split()) > 2) or not field_description):
				continue

			if enum_option_matches and macros:

				# e.g. `"[Critical, somewhat important, …]"` -> `['Critical', 'somewhat important', …]`
				enum_options: list[str] = [enum_option.lower() for enum_option in enum_option_matches[0].removeprefix('[').removesuffix(']').split(', ')]
				field_dict['enum_options'] = enum_options
				field_dict['type'] = Field.TYPE_ENUM

				# if this is not an actual field (no macros defined)
				# we'll set `max_segments` to max length to avoid raising validation errors
				if not macros:
					field_dict['max_segments'] = len(field_description)

			if not field_name:
				raise UserError(f'The template is missing a field name: "{line}"')

			field: Field = Field(name=field_name, description=field_description, field_dict=field_dict)

			# check if the field already exists (template accidentally contains it multiple times)
			# only check if this is a defined field (vs an auto-detected field with no macros (e.g. `Lorem: ipsum`))
			# if the existing field has no macros, but this one does, skip this check and overwrite the other field
			# e.g.
			# Example responses:
			# Is Junk: Yes
			# ---
			# Let's respond with the exact format:
			#
			if field.name in fields and macros and fields[field.name].field_dict:
				raise UserError(f'The template contains multiple fields called "{field.name_raw}"')

			fields[field.name] = field

		return fields

	def get(self, field_name: str) -> Field:
		"""
		Returns a field for a given name

		@param field_name(str): a field name in the template text (e.g. `radar_component`)
		@return (Field): a field
		"""
		field: Field = self.fields.get(field_name)

		if field:
			return field

		return next((field for field in self.fields.values() if field.alternative_name == field_name), None)

	def get_populated_text(self, prompt: dict[str, Any]):
		"""
		Populates the template with values from a dictionary prompt

		e.g.
		prompt:
		```
		{'question': 'How many radars are Sec Type: Exposure?'}
		```

		text:
		```
		Help the user with the following question:
		<#question#>
		```

		returns:
		```
		Help the user with the following question:
		How many radars are Sec Type: Exposure?
		```

		@param prompt(dict): a dictionary with keys in the `template`, and their values
		@return (str): populated template
		"""

		if not isinstance(prompt, dict):
			raise Exception('prompt must be a dictionary')

		populated_text: str = self.text

		# hide <!…!> macros
		populated_text = re.sub(r'\<\!(.*?)\!\>', '', populated_text)

		for line in populated_text.split('\n'):

			# hide sub options in enums (e.g. `[xnu|*kernel, safari, other|unknown]`)
			matches: list[str] = re.findall(r'\|.*?(?=(?:,|]))', line)
			matches += re.findall(r'\*(?=(?:,|\]))', line)
			matches += re.findall(r'(?=(?:,|\[))\*', line)

			for match in matches:
				populated_text = populated_text.replace(match, '')

		for key, value in prompt.items():

			# if the value is a list of `Knowledge`, automatically convert it to a string format
			if value and isinstance(value, list) and issubclass(value[0].__class__, Knowledge):
				value = '\n-----\n'.join([knowledge.content for knowledge in value])

			elif issubclass(value.__class__, Knowledge):
				value = value.content

			elif issubclass(value.__class__, Section):
				value = value.full_content

			populated_text = populated_text.replace(f'<#{key}#>', str(value).strip())

		# make sure all the placeholders are replaced
		# by checking if there are any placeholders in the template text that still exist
		# in the populated text. exclude placeholders that we've already replaced, because they might be
		# text in the prompt
		# e.g. `template = "<#knowledges#>"`
		#	   `prompt = {"knowledges": [Knowledge(content='add <#knowledges#> to your prompt')]}
		if (placeholder := next((placeholder for placeholder in re.findall(r'<#[A-Za-z0-9_-]+#>', self.text, flags=re.MULTILINE) if
								 placeholder in populated_text and placeholder.replace('<#', '').replace('#>', '') not in prompt.keys()), None)):
			raise UserError(f'In AI.ask(…), your `template` mentions "{placeholder}" but no `prompt={{\'{placeholder.replace("<#", "").replace("#>", "")}\': \'…\'}}` argument was passed')

		return populated_text

	@classmethod
	def get_has_placeholders(cls, text: str) -> bool:
		"""
		[Internal]
		Returns whether a given template text has placeholders

		e.g. `text = 'lorem ipsum <#display_classification#>'` returns `True`
		"""
		return '<#' in text

	def __str__(self):
		return f'Format: {self.display_format or "auto"}\nText:\n{self.text}'

	__repr__ = __str__


class Enum(str):
	"""
	The class is the value of the enum fields you can ask for in a prompt.

	For example:
	```
	from interlinked import AI

	observation = AI.ask(prompt=
	'''Your task is to help me categorize emails. Read the email below, then respond with the exact format:
	Category<!type:enum,required:true!>: any of [feature_request, bug, security_vulnerability]

	---
	When I scroll horizontally, the UI glitches out''')
	print(observation.category, observation.category.is_bug)
	# bug True
	```

	The type of the `.category` field above is an `Enum`.
	"""

	# e.g. `Security vulnerability`
	# while `self` returns `security_vulnerability`
	raw: str = None

	def __new__(cls, string: str):

		# e.g. `TCC (Transparency, Consent, and Control)` -> `TCC`
		raw = Utilities.remove_acronyms(text=string)

		# even with raw strings, we don't want or expect characters like "`"
		# e.g. "```nvram```" (which should just be "nvram")
		# e.g. `"contentEditable" feature` -> `contentEditable feature`
		# e.g. `Virtualization (Virtual Machine)` -> `Virtualization`
		raw = string.replace('"', '').replace('`', '').removesuffix('.').split('(', 1)[0].strip()
		instance = super().__new__(cls, cls.get_clean_string(string=raw).lower())

		instance.raw = raw
		instance.raw_literal = string
		return instance

	def __getattr__(self, attr: str) -> Any:

		# e.g. response: `Category: Security vulnerability`
		#      usage: `observation.category.is_security_vulnerability`
		if attr.startswith('is_'):
			return attr.split('is_')[-1] == self

		if attr == 'raw':
			return self.raw

		try:
			return object.__getattribute__(self, attr)

		except AttributeError as error:
			return None

	@classmethod
	def get_clean_string(cls, string: str) -> str:
		"""
		[Internal]
		Converts a given raw string to a format that is suitable
		to be used as an enum

		@param string(str): a raw string value
		@return (str): a string value that can be used as an `Enum(…)`
		"""

		# e.g. `CoPresence-Core Lorem…` -> `copresence_core_lorem`
		# also make sure we don't have multiple `__` if there's a space after the dash in the name
		# e.g. `CoPresence- Core` -> `copresence_core`
		string = string.replace('/', '_').replace(' ', '_').  \
						replace('-', '_').replace('\\_', '_').replace('__', '_')

		# remove leading or trailing _
		# e.g. `__category` -> `category`
		return string.strip('_')


class Code(str):
	"""
	The class is the value of the code fields you can ask for in a prompt.

	For example:
	```
	from interlinked import AI

	observation = AI.ask(prompt=
	'''Your task is to help me generate new and unique JavaScript test for JavaScriptCore project without using any specific test API.

	Let's respond with the exact format:
	Test Code<!type:code,required:true!>: JavaScript test code
	Summary<!type:string,required:true!>: A summary of the code you wrote''')
	print(observation.test_code)
	# describe('Array', () => {…
	```

	The type of the `.test_code` field above is an `Code`.
	"""

	# [internal] used to match languages in code block
	# we intentionally use a set of languages instead of patterns to avoid mismatching in case
	# the markdown response has code on the same line as the backticks
	# e.g. ````const obj…`
	LANGUAGE_PATTERN: str = r'^`+(swift|c|objective-c|python|javascript|jsx|java|json|r|sql|go|typescript|yaml|xml|scss|ruby|css|html|rust)(?: |\n|$)'

	# e.g. ````swift\nlorem_ipsum()\n…````
	# while `self` returns `lorem_ipsum()\n…`
	raw_literal: str = None

	# e.g. `swift\nlorem_ipsum()\n…`
	# while `raw_literal` returns all of the backticks
	raw: str = None

	# the language the code was written in
	# if the code markdown is prefixed with the language
	# e.g. ```swift\n…```
	# note: not guaranteed to have a value
	language: Enum = None

	def __new__(cls, string: str):

		# the absolute literal string
		raw_literal: str = string

		# even with raw strings, we don't want or expect characters like "`"
		# e.g. "```test()```" (which should just be `test()`)
		raw: str = re.sub(r'^`+|`+$', '', raw_literal.strip(), flags=re.MULTILINE).strip()

		string = string.strip()
		language: Enum = None

		# extract the language, if this is a code block (that has multiple lines)
		# e.g. ````swift\ntest()````
		if string.count('\n') > 0 and (language_matches := re.findall(cls.LANGUAGE_PATTERN, string, flags=re.IGNORECASE)):

			# e.g. `swift`
			language = Enum(language_matches[0])

			# e.g. ````swift\ntest()```` -> `test()````
			string = re.sub(fr'^`+({language.raw})', '', string).strip()

			# e.g. `test()```` -> `test()`
			string = re.sub(r'`+$', '', string)

		else:
			string = raw

		instance = super().__new__(cls, string)

		instance.raw = raw
		instance.language = language
		instance.raw_literal = raw_literal
		return instance

	@property
	def line_count(self) -> int:
		"""
		Returns the number of lines in this code
		"""
		return len(self.split('\n'))


class Response(str):

	# e.g. `Sure, here is the updated text: Lorem Ipsum`
	# while `self` returns `Lorem Ipsum`
	raw: str = None

	files: list[File] = []

	# Generic metadata for grounding, code execution, etc.
	metadata: Any = None

	def __new__(cls, string: str, files: list[File] = None, template: Template = None, raw: str = None, metadata: Any = None):

		display_string: str = None

		# if the response starts with `Sure, here's the updated text: …`
		if string:

			display_string = re.sub(r'^Sure(?:,|\!) (?:h|H)ere.*?\:', '', string).strip()
			display_string = re.sub(r'^Here .*?\:', '', display_string).strip()
			display_string = re.sub(r'^I apologize.*?\:', '', display_string).strip()
			display_string = re.sub(r'^My apologies.*?\:', '', display_string).strip()

			# if the response is wrapped with quotes
			if display_string.startswith('"') and display_string.endswith('"'):
				display_string = display_string.removeprefix('"').removesuffix('"')

			# fix the response if it is not structured
			display_string = cls.get_structured_string(string=display_string, template=template)

		instance = super().__new__(cls, display_string)
		instance.raw = raw or string
		instance.files = files or []
		instance.metadata = metadata
		return instance

	@classmethod
	def get_structured_string(cls, string: str, template: Template = None) -> str:
		"""
		Converts a partially unstructured response into a structured one

		'''
		### Summary:
		awesome
		- Category:
		security_vulnerability
		#### Fits component: ####
		**no**
		'''
		⬇️ converted to ⬇️
		'''
		# Summary: awesome
		- Category: security_vulnerability
		#### Fits component: #### **no**
		"""

		string = re.sub(r'^([A-Z][A-Za-z]+:)\n', r'\1 ', string, flags=re.MULTILINE).strip()

		string_lines: list[str] = []

		# tracks the field value (if the value is on multiple lines)
		current_field_line: str = None

		# if the string has code blocks and the language is outside of the codeblock, move
		# the language to inside of the codeblock
		# e.g. swift```code``` -> ```swift\ncode```
		# the logic ensures that we don't make unnecessary changes
		# e.g. `swift```test()\nlet value```` will not convert it to an invalid codeblock ````swift\ntest()\nlet ```value`
		is_in_codeblock: bool = False

		for line_index, line in enumerate(string.split('\n')):

			field_name_matches: list[str] = re.findall(fr'(?:(?<=^-)|(?<=^#)|(?<=^##)|(?<=^####)|(?<=^###)|(?<=^\*\*)|(?<=^\*)|(?<=^))[/A-Z ()_-]+(?=:|\?)', line, flags=re.IGNORECASE)

			if field_name_matches:

				field_name: str = field_name_matches[0].strip()
				field_value: str = None

				# if the field name is empty, add the line as-is
				# e.g. `  : lorem ipsum`
				if field_name:
					field_value = re.sub(r'[#*:?-]+', '', line.split(field_name)[-1]).strip()

				if current_field_line:
					string_lines.append(current_field_line)

				if field_value and not field_value.startswith(' ') and len(field_value) > 0:

					string_lines.append(line)
					current_field_line = None
					continue

				current_field_line = f'{line}'
				continue

			if is_in_codeblock:

				# if this is line ends the codeblock
				if re.findall(r'`+\b', line):
					is_in_codeblock = False

			else:

				# if we match, ensure that we don't try to make more changes
				# while we're in the current codeblock
				is_in_codeblock = len(re.findall(r'`{2,5}', line, flags=re.IGNORECASE)) > 0

				if is_in_codeblock:
					line = re.sub(r'([A-Z-]+)(`+)\b', r'\2\1\n', line, flags=re.IGNORECASE)

			# if we're tracking a field, append this line to the same line as that field
			if current_field_line:

				# if we're in a code block, preserve new lines
				if is_in_codeblock:
					current_field_line = f'{current_field_line}\n{line}'

				else:
					current_field_line = f'{current_field_line} {line}'

			else:
				string_lines.append(line)

		if current_field_line:
			string_lines.append(current_field_line)

		if string_lines:
			string = '\n'.join(string_lines)

		if template and template.fields:

			# if the response has multiple fields on the same line, move the field to the next line so we can parse it later
			# e.g. `response = 'Rainfall: 1.2 Temperature: 80'` -> `response = 'Rainfall: 1.2\nTemperature: 80'`

			# collect all the fields in the template and account for all of the ways they could be in the response (raw name or name)
			# e.g. `Rainfall` or `rainfall`
			field_names: set[str] = set(sum([[field.name, field.name_raw] for field in template.fields.values()], []))

			# to avoid mismatching fields that have similar names and inadvertently moving them to the next line,
			# we remove any field names that are contained by another field
			# e.g. `field_names = {'Total Rainfall', 'Rainfall'}` -> `field_names = {'Total Rainfall'}`
			field_names = [field_name for field_name in field_names if not any(field_name != _field_name and field_name in _field_name for _field_name in field_names)]

			field_names_pattern: str = r'|'.join({re.escape(field_name) for field_name in field_names})
			string = re.sub(fr'(?!^)((?:\b{field_names_pattern}\b):)', r'\n\1', string, flags=re.IGNORECASE | re.MULTILINE)

		return string

	def reduce_verbosity(self, min_similarity: float = 65.0) -> 'Response':
		"""
		Reduces duplicate/verbosity in responses.
		Currently designed to trim the last line in the response, if it's too similar to previous lines.

		Helpful for less-capable models
		"""

		response_lines: list[str] = self.split('\n')

		if len(response_lines) <= 2:
			return self

		# tokenize and normalize words from the last line and the rest of the text
		previous_lines_chunks: set[str] = {re.sub(r'\W+', '', word.lower()) for word in ' '.join(response_lines[:-1]).split()}
		last_line_chunks: set[str] = {re.sub(r'\W+', '', word.lower()) for word in response_lines[-1].split()}

		intersected_chunks: set[str] = last_line_chunks.intersection(previous_lines_chunks)

		if not intersected_chunks:
			return self

		similarity: float = (len(intersected_chunks) / len(last_line_chunks)) * 100

		# if it's too similar to the overall string
		if similarity > min_similarity:
			return Response(string='\n'.join(response_lines[:-1]).strip(), files=self.files, raw=self.raw)

		return self


""" Knowledge """

class Section(BaseModel):
	"""
	Represents a document or a section within a document

	For example:
	```
	# My document
	Lorem ipsum

	## Installation
	Dolor sit amet. Consectetur adipiscing elit

	### Deploying to production
	In pellentesque scelerisque
	```

	⬇️ Using a parser, it becomes a structured `Section`:
	```
	Section(
		title='My document',
		content=['Lorem Ipsum'],

		subsections=[
			Section(
				title='Installation',
				content=['Dolor sit amet', 'Consectetur adipiscing elit'],

				subsections=[
					Section(
						title='Deploying to production',
						content=['In pellentesque scelerisque']
					)
				]
			)
		]
	)
	```
	"""

	# the title of the document or section within the document
	title: Optional[str] = None

	# a list of paragraphs or sentences within this section
	# e.g. if you have a header in a document with text underneath it,
	# this property will contain the text
	# if this is an file/image, content will be a list of bytes
	content: Optional[list[str|bytes]] = []

	# if this section is a document, `subsections` would be
	# all headers and text within this document
	# if this section is a subsection within a document, `subsections`
	# would be the smaller headers and text within this section
	subsections: list['Section'] = []

	# if this is a subsection, this property will point to the section this
	# subsection is in.
	parent: Optional['Section'] = PydanticField(default=None, exclude=True)

	# allows referencing the exact location of the subsection
	# supported by some sources
	external_id: Optional[str] = None

	# internal/private property. please use `.embedding` instead
	embedding_: Optional[list[float]] = PydanticField(default=None, exclude=True)
	is_example: Optional[bool] = PydanticField(default=False, exclude=True)

	@classmethod
	def from_markdown(cls, content: str) -> 'Section':
		"""
		Takes Markdown text (`.md`) and converts it into a structured `Section`.
		You can then pass this to `AI.learn`

		@param content(str): the contents of a Markdown file (not file path)
		@return (Section): the Markdown file, but in a structured format
		"""

		from interlinked.core.parsers.markdownparser import MarkdownParser
		return MarkdownParser.get_section_for_markdown(content=content)

	@classmethod
	def from_restructured(cls, content: str) -> 'Section':
		"""
		Takes ReStructured text (`.rst`) and converts it into a structured `Section`.
		You can then pass this to `AI.learn`

		@param content(str): the contents of a ReStructured file (not file path)
		@return (Section): the ReStructured file, but in a structured format
		"""

		from interlinked.core.parsers.restructuredparser import RestructuredParser
		return RestructuredParser.get_section_for_restructured(content=content)

	def append_subsection(self, subsection: 'Section'):
		"""
		Appends a sub-section to this section

		e.g. if this section represents an `h1`. The sub-section would be `h2` or lower

		@param subsection(Section): A sub-section to append
		"""

		subsection.parent = self
		self.subsections.append(subsection)

	@property
	def is_content_empty(self) -> bool:
		"""
		Returns whether this section has any content, and if the content
		has actual data or is empty

		e.g. if content is `[]` or `['- ']`, it will be treated as empty
		"""

		if not self.content:
			return True

		# if this is a file (e.g. image), check if it has a value
		if isinstance(self.content[0], bytes):

			if not self.content[0]:
				return True

			return False

		content: set[str] = set()

		for _content in self.content:

			_content = _content.strip()

			if not _content:
				continue

			# skip if _content is just one non-alphanumeric character (e.g. `-`)
			# but keep it if it's something like `No` (e.g. if the title is a question and the content is an answer)
			if not _content.isalnum() and len(_content) <= 1:
				continue

			content.add(_content)

		content: str = ''.join(content)

		if not content or (content.isalnum() and len(content) <= 1):
			return True

		return False

	def get_all(self, include_subsections: bool = True, split_by: str = 'sentence', **kwargs) -> list['Section']:
		"""
		Returns a flattened list of this section (+ all nested subsections)

		@param include_subsections(bool): (optional) Whether to include all nested subsections
		@param split_by(str): (optional) Currently, only `sentence` or `None` are supported

		Additional arguments if `split_by='sentence'`:
		@param max_characters(int): Maximum number of characters each section should have
		@param overlap_size(int): The number of sentences to include
		@return (Section): A flattened list of this section and all subsections split
		"""

		sections: list[Section] = []

		if split_by == 'sentence':

			max_characters: str = kwargs.get('max_characters', 2000)
			overlap_size: int = kwargs.get('overlap_size', 1)

			for section in ([self] + self.all_subsections if include_subsections else [self]):
				sections += section.split_by_sentence(max_characters=max_characters, overlap_size=overlap_size)

		elif split_by == 'character':

			max_characters: str = kwargs.get('max_characters', 2000)

			for section in ([self] + self.all_subsections if include_subsections else [self]):
				sections += section.split_by_character(max_characters=max_characters)

		elif split_by is None:
			return ([self] + self.all_subsections if include_subsections else [self])

		else:
			raise UserError('split_by currently only supports "sentence"')

		return sections

	@property
	def full_content(self) -> str|bytes:
		"""
		Returns the full content of this section with:
		- title (+ title of all parents)
		- content

		Does not contain subsections
		"""

		# if the content is bytes (e.g. file/image), return the content directly
		if self.content and isinstance(self.content[0], bytes):
			return self.content[0]

		parents: list[Section] = []
		current_section: Section = self

		# find all parents of this section
		# to use for including context
		while current_section.parent:

			parents.append(current_section.parent)
			current_section = current_section.parent

		# reverese order, so it's most immediate parent to root
		parents.reverse()
		parent_titles: list[str] = [parent.title for parent in parents if parent.title]

		full_content: list[str] = []

		if self.title:

			if parent_titles:
				full_content.append(f'{self.title} ({", ".join(parent_titles)})')

			else:

				# TODO (feature): if this is root-level section and the title lacks context, use `AI.ask` on
				# the title + chunk of the content
				# for Quip, use folder name
				# for Markdown, use file name
				full_content.append(self.title)

		elif parent_titles:
			full_content.append(f'({", ".join(parent_titles)})')

		if self.content:
			full_content.append('\n'.join([content.strip() for content in self.content]))

		return '\n'.join(full_content)

	@property
	def embedding(self) -> list[float]:
		"""
		Returns the embedding of `.full_content`

		Same as `.get_embedding(…)` but uses the default AI Client
		"""

		if self.embedding_ is None:
			self.embedding_ = AI.get_embedding(input=self.full_content)

		return self.embedding_

	def get_embedding(self, client: BaseAIClient = None) -> list[float]:
		"""
		Returns the embedding of `full_content`.

		Use this instead of `.embedding` if you want to use a custom AI client (e.g. `OpenAIClient`)

		@param client(BaseAIClient): (optional) Any AI client (e.g. `OpenAIClient`, `OllamaClient`, or `AJAXClient`)
		"""

		self.embedding_ = AI.get_embedding(input=self.full_content, client=client)
		return self.embedding_

	""" Fine-tuning """

	def get_examples(self, prompt: str = None, include_subsections: bool = True,
					 min: int = 10, max: int = None, client: BaseAIClient = None, **kwargs) -> list['Section']:
		"""
		Converts this Section into a list of Sections with questions and answers.
		Primarily designed to use in fine-tuning models on your own data.

		e.g.
		```
		Section:
			title: Interlinked, A Powerful AI Library and Platform
			description: Interlinked is a powerful (and lightweight) plug-and-play…
		```

		Return:
		```
		[
			Section:
				title: What is Interlinked?
				description: Interlinked is a powerful plug-n-play…
			Section:
				title: How to install Interlinked?
				content: To install Interlinked, run `pip3…`
		]
		```

		@param prompt(str): (optional, but recommended) The prompt that asks AI to respond with example Q&A
		@param include_subsections(bool): (optional) Whether examples should be created for all subsections
		@param min(int): (optional) Minimum number of example questions and answers per section, default value is 10
		@param max(int): (optional) Maximum number of example questions and answers per section, should be greater than min
		@return (list): A list of new Sections that are questions and answers
		"""

		if max and min > max:
			raise UserError(f"The min value {min} is greater than the max value {max}")

		example_sections: list[Section] = []
		prompt: str = prompt or f'Given the knowledge snippet below, respond with a JSON list of at least {int(min * 0.7)} diverse example questions and answers.\n\n'  \
								'Respond with this exact format: [{"question": …, "answer": …}, …]'

		if not ('question' in prompt and 'answer' in prompt):
			raise UserError('The prompt should ask AI to respond with this format: `[{"question": …, "answer": …}, …]`')

		# auto-add `<#section#>` (the knowledge AI will use to create questions and answers from)
		if '<#section#>' not in prompt:
			prompt = f'{prompt}\n---\nKnowledge:\n<#section#>'

		def validator(observation: Observation) -> str:
			"""
			Validates the JSON schema
			"""
			response_as_dictionary: list[dict] = observation.response_as_dictionary

			if not response_as_dictionary:
				return

			if not isinstance(response_as_dictionary, list) or not isinstance(response_as_dictionary[0], dict):
				return 'Respond with the exact format: [{"question": …, "answer": …}, …]'

		for section in self.get_all(include_subsections=include_subsections):

			if section.is_content_empty:

				logger.warning(f'skipping "{section.title}" because it is empty…')
				continue

			response_as_dictionary: list[dict] = []

			while len(response_as_dictionary) < min:

				observation: Observation = AI.ask(prompt={'section': section}, template=prompt,
												  client=client, validator=validator, **kwargs)

				if isinstance(observation.response_as_dictionary, list):
					response_as_dictionary += observation.response_as_dictionary

			_example_sections: list[Section] = []

			# TODO: remove duplicates (`set` does not work on `dict`)
			for question_answer in response_as_dictionary:

				if not isinstance(question_answer, dict):

					logger.warning(f'could not get examples: {question_answer}')
					continue

				if max and len(_example_sections) > max:
					break

				example_section: Section = Section(title=question_answer.get('question'),
												   content=[question_answer.get('answer')],
												   external_id=section.external_id, is_example=True)
				_example_sections.append(example_section)

			example_sections += _example_sections

		return example_sections


	""" Utilities """

	def split_by_sentence(self, max_characters: int|Callable, overlap_size: int = 1) -> list['Section']:
		"""
		Splits this section into multiple sections if the content is longer than `max_characters`.
		This function keeps `self.subsections` but adds it to the last `Section` returned in the list

		@param max_characters(int|Callable): Can be either a hardcoded list of maximum characters or a dynamic function that calculates
											 based on content (e.g. a tokenzier)
		@param overlap_size(int): The number of sentences/characters to include from surrounding sections to help maintain context
		"""

		# content must be strings
		if not all(isinstance(paragraph, str) for paragraph in self.content):
			raise UserError('The content in `Section` must be strings to use split_by_sentence')

		# count the number of characters in each paragraph
		# if the amount of characters in the entire section is less than `max_characters`,
		# return the section as-is early
		if sum(len(paragraph) for paragraph in self.content) <= max_characters:
			return [self]

		# number of characters in
		current_character_count: int = 0
		sections: list[Section] = []

		current_section_sentences: list[str] = []

		for paragraph in self.content:

			# split each paragraph into sentences
			sentences: list[str] = re.split(r'(?<=[.。։!?])\s+|[\r\n]+', paragraph)

			# if no sentences, split by punctuation
			if len(sentences) == 1 and sentences[0] == paragraph:
				sentences = re.split(r'[.。։!?]', paragraph)

			# if no sentences, split by space
			if len(sentences) == 1 and sentences[0] == paragraph:
				sentences = re.split(r'\s+', paragraph)

			for sentence in sentences:

				# skip any empty content
				if not sentence:
					continue

				# check if adding this sentence would exceed the max character limit
				if current_character_count + len(sentence) > max_characters and current_section_sentences:

					sections.append(Section(title=self.title, content=current_section_sentences,
											external_id=self.external_id))

					# start the new section with an overlap of sentences (if specified)
					# TODO (feature): include the first sentence from the first split section (e.g. Registries in `flows-and-cli.rst`)
					current_section_sentences = current_section_sentences[-overlap_size:] + [sentence.strip()]
					current_character_count = sum(len(sentence) for sentence in current_section_sentences)

				else:
					current_section_sentences.append(sentence.strip())
					current_character_count += len(sentence.strip())

		# add the last section if it's not empty
		if current_section_sentences:

			sections.append(Section(title=self.title,
									content=current_section_sentences, external_id=self.external_id))

		# if we (self) have subsections, since `sections` is a list of brand-new sections,
		# we want to make sure at least one of the new sections has our subsections, so we pick
		# the last one (but can be any one)
		if self.subsections:
			sections[-1].subsections.extend(self.subsections)

		return sections

	def split_by_character(self, max_characters: int|Callable) -> list['Section']:
		"""
		Splits this section into multiple sections by character if the content is longer than `max_characters`.
		This function keeps `self.subsections` but adds it to the last `Section` returned in the list

		TODO: add the ability to set `overlap_size`

		@param max_characters(int|Callable): can be either a hardcoded list of maximum characters or a dynamic function that calculates
											 based on content (e.g. a tokenzier)
		"""

		# count the number of characters in each paragraph
		# if the amount of characters in the entire section is less than `max_characters`,
		# return the section as-is early
		if (len(self.title or '') + sum(len(paragraph) for paragraph in self.content)) <= max_characters:
			return [self]

		sections: list[Section] = []

		# pre-calculate room for content by subtracting title length (and space)
		max_content_length: int = max_characters - len(self.title or '') - 1
		max_content_length = 1 if max_characters < 1 else max_characters
		full_content: str = ' '.join(self.content)

		# start splitting full_content considering max_content_length
		content_chunks = [full_content[i:i + max_content_length + 1] for i in range(0, len(full_content), max_content_length + 1)]
		return [Section(title=(self.title or '')[:max_characters - len(chunk)], content=chunk.split()) for chunk in content_chunks]


	""" Internal """

	@field_validator('title')
	def strip_title(cls, value: str) -> str:
		"""
		[Internal]
		Setter for `title`. Strips spaces
		"""

		if isinstance(value, str):
			return value.strip()

		return value

	@property
	def all_subsections(self) -> list['Section']:
		"""
		[Internal/private property]
		Returns all a flattened list of nested subsections (excluding this Section)
		"""

		flattened_subsections: list[Section] = []

		for subsection in self.subsections:
			flattened_subsections += [subsection] + subsection.all_subsections

		return flattened_subsections

	def to_html(self, level: int = 1) -> str:
		"""
		Converts this Section and all subsections to HTML while maintaining structure

		@param level(int): the current level (used to determine the `<h>` level)
		@return (str): the HTML representation of the Section
		"""

		html: str = ''

		if self.title:
			html = f'<h{level}>{self.title}</h{level}>'

		html = f'{html}<p>{" ".join(self.content)}</p>'

		for section in self.subsections:
			html += section.to_html(level=level + 1)

		return html

	def to_markdown(self, level: int = 1) -> str:
		"""
		Converts this Section and all subsections to Markdown while maintaining structure.

		@param level(int): The starting heading level (default: 1 for `h1`)
		@return (str): The Markdown representation of the Section
		"""

		markdown_lines: list[str] = []

		if self.title:
			markdown_lines.append(f"{'#' * level} {self.title}\n")

		if self.content:
			markdown_lines.append("\n".join(self.content) + "\n")

		for subsection in self.subsections:
			markdown_lines.extend(subsection.to_markdown(level + 1).splitlines(keepends=True))

		return ''.join(markdown_lines)

	def __str__(self, level: int = 0) -> str:

		indent: str = '\t' * level
		title: str = f'{indent}Title: {self.title}'

		if self.external_id:
			title = f'{title} ({self.external_id})'

		content: str = f'{indent}Content: {self.content}' if self.content else ''
		subsection_strs = '\n'.join(subsection.__str__(level + 1) for subsection in self.subsections)

		if not content and not subsection_strs:
			return title

		return f'{title}\n{content}\n{subsection_strs}'

	__repr__ = __str__


class Knowledge(BaseModel):
	"""
	Extend AI's knowledge what it was trained on.

	Some use-cases:
	- Chat with Quip, Markdown, HW spec documents
	- Find similar/duplicate content (e.g. radars, job posts)

	Use the `AI.learn` first on your data, then `Knowledge.search(…)`.
	"""

	__collection_name__: str = 'knowledge'
	model_config = ConfigDict(arbitrary_types_allowed=True)

	# [internal] the internal ID of the Knowledge
	id: Optional[str] = None

	# the raw string/bytes content used to generate the embeddings
	# string if it's text content
	# bytes if it's a file/image
	content: str | bytes | None

	# embeddings are numerical representation of given `Knowledge`
	embedding: Optional[list[float]] = None

	# an optional list of example prompts that a user could ask about this `Knowledge`
	# this is something you'd populate after calling `AI.learn`
	examples: Optional[list[str]] = None

	# allows referencing the exact location of the subsection
	# supported by some sources (e.g. an E918)
	# e.g. `E:FVS827ee2173f55409eaf5e5c9d0`
	external_id: Optional[str] = None

	# the source this Knowledge was made by
	knowledge_source_id: Optional[int|str] = None

	# this gets populated by `.search` and can be used to measure how similar this
	score: Optional[float] = PydanticField(default=None, exclude=True)

	# the vector store client used to store this Knowledge
	store_client: Optional[BaseVectorStoreClient] = None

	@computed_field
	@property
	def content_hash(self) -> str:
		"""
		Returns the unique hash for this Knowledge's content.
		Used as an ID to avoid recreating Knowledge
		"""
		return Knowledge.get_hash_for_content(content=self.content)

	@classmethod
	def search(cls, query: str|list[float], knowledge_source_ids: list[str|int] = None, limit: int = 3,
			   min_score: float = 0.3, max_gap: float = None, unique: bool = True, search_filter: Filter = None,
			   rerankers: list[str] = [], client: Any = None, store_client: BaseVectorStoreClient = None) -> list['Knowledge']:
		"""
		Searches for Knowledge that AI learned from that is similar to a given text (or embedding)

		@param query(str|float): The string (or embedding) to find similar Knowledge to (e.g. "How do I run __?")
		@param knowledge_source_ids(list): (Optional) Limit search to a list of Knowledge Sources IDs (e.g. certain files)
										   Knowledge Source IDs are unique IDs you may want to generate per file/knowledge source
										   when calling `AI.learn`. You can then use them to filter the knowledge you search for
		@param limit(int): (Optional) Limit the number of Knowledge(s) to return
		@param min_score(float): Exclude results below a certain score (0.0 › 1.0)
		@param max_gap(float): The maximum score gap between similar `Knowledge`
							   (e.g. if it's `0.2`, that means any Knowledge that scores less than `0.2` than the first/next Knowledge in the
								results will be included)
		@param search_filter(Filter): (Optional) Custom filters that can be applied (see qdrant.tech/documentation/concepts/filtering)
		@param unique(bool): (Optional) If set, duplicate Knowledge (same content) will not be included in the results
		@param rerankers(list): (Optional) [Internal] A list of methods to rerank the results (e.g. `['transformers:colbert-ir/colbertv2.0']`)
		@param client(Any): (Optional) Pass any of the supported clients (e.g. `OpenAIClient`, `GoogleAIClient`, `AnthropicClient` or `OllamaClient`) to use other models
		@param store_client(Any): (Optional) A custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		@return (list): A list of similar `Knowledge`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		# ensure min_score is within the correct range
		if min_score < 0 or min_score > 1:
			raise UserError('min_score can only be between 0.0 and 1.0')

		if type(store_client).__name__ == 'ElasticSearchClient':

			return cls._search_elastic(query=query, knowledge_source_ids=knowledge_source_ids, limit=limit, min_score=min_score,
									   max_gap=max_gap, unique=unique, search_filter=search_filter, rerankers=rerankers, client=client,
									   store_client=store_client)

		elif type(store_client).__name__ == 'MilvusClient':

			return cls._search_milvus(query=query, knowledge_source_ids=knowledge_source_ids, limit=limit, min_score=min_score,
									  max_gap=max_gap, unique=unique, search_filter=search_filter, rerankers=rerankers, client=client,
									  store_client=store_client)

		return cls._search_qdrant(query=query, knowledge_source_ids=knowledge_source_ids, limit=limit, min_score=min_score,
								  max_gap=max_gap, unique=unique, search_filter=search_filter, rerankers=rerankers, client=client,
								  store_client=store_client)

	@classmethod
	def _search_qdrant(cls, query: str | list[float], knowledge_source_ids: list[str | int] = None, limit: int = 3,
					   min_score: float = 0.3, max_gap: float = None, unique: bool = True, search_filter: Filter = None,
					   rerankers: list[str] = [], client: BaseAIClient = None, store_client: BaseVectorStoreClient = None) -> list['Knowledge']:

		# ⛽ If Store Client is Qdrant
		from qdrant_client.models import FieldCondition, MatchValue, MatchExcept, \
			MatchAny, ScoredPoint, HasIdCondition, ValuesCount, \
			PayloadField

		# some rerankers require having `query` to be the prompt (string) itself instead of the embedding
		if any(reranker.startswith('transformers:') for reranker in rerankers) and isinstance(query, list):
			raise UserError('To use rerankers, `query` must be the string prompt itself (instead of embedding)')

		query_embedding: list[float] = query if isinstance(query, list) else AI.get_embedding(input=query, client=client)
		search_filter = search_filter or Filter(must=[])

		if search_filter.must is None:
			search_filter.must = []

		# the `None` vs length check is important
		# to ensure that we don't accidentally return all Knowledge
		# if the caller passes an empty list
		if knowledge_source_ids is not None:
			search_filter.must.append(FieldCondition(key='knowledge_source_id',
													 match=MatchAny(any=knowledge_source_ids)))

		# check if we have a collection already created. if not, return an empty list
		if not store_client.get_has_collection(name=cls.__collection_name__):

			store_client.create_collection(name=cls.__collection_name__, size=len(query_embedding))
			return []

		# if we have rerankers, increase the limit to capture more results, then only
		# return the original limit the caller requested
		_limit: int = limit + 3 if rerankers else limit

		try:
			query_result = store_client.query_points(collection_name=cls.__collection_name__,
													 query=query_embedding, query_filter=search_filter, limit=_limit,
													 score_threshold=min_score)
			scored_points: list[ScoredPoint] = query_result.points
		except ValueError as error:

			# the collection was created with an embedding model that is different from the `AI.client`
			if str(error).startswith('shapes') or 'dimension error' in str(error):
				raise UserError('The database was populated with embeddings from a different model. '
								'To fix this, you need to delete the database via `interlinked chat -d` then create the data again.')

			raise error

		# remove duplicate points based on content and Knowledge Source ID
		if unique:
			scored_points = list({scored_point.payload.get('content'): scored_point for scored_point in scored_points}.values())

		# exclude any knowledge that is too distant from the first or its previous neighboring knowledge
		if max_gap is not None and scored_points:

			first_scored_point: float = scored_points[0]
			previous_score: float = first_scored_point.score

			# iterate and remove distant knowledge
			for scored_point_index, scored_point in enumerate(scored_points):

				# if this point is too distant from the previous point
				if scored_point.score - previous_score > max_gap:

					scored_point[:knowledge_index]
					break

				previous_score = scored_point.score

		# convert scored points to Knowledge
		knowledges: list[Knowledge] = []

		for scored_point in scored_points:

			knowledge: Knowledge = cls(store_client=store_client, id=scored_point.id, content=None,
									   embedding=scored_point.vector, score=scored_point.score)

			# set each key in the payload to the corresponding field in `Knowledge`
			# since `Knowledge` class can be inherited with custom fields, we'll set their values
			for key, value in scored_point.payload.items():

				if key in knowledge.model_fields:
					setattr(knowledge, key, value)

			knowledges.append(knowledge)

		# rerank Knowledge
		for reranker in rerankers:

			# reranker models via Transformers (e.g. ColBERT)
			if reranker.startswith('transformers:'):
				knowledges = cls.rerank_with_transformers(knowledges=knowledges, prompt=query,
														  model_name=reranker.removeprefix('transformers:'))

			# maximal marginal relevance
			elif reranker.startswith('mmr'):
				knowledges = cls.rerank_with_mmr(knowledges=knowledges, prompt=query_embedding)

			# prompt
			elif reranker.startswith('prompt'):
				knowledges = cls.rerank_with_prompt(knowledges=knowledges, prompt=query)

		# if we used rerankers, unset `.score` since each reranker has its own scoring method
		if rerankers:

			for knowledge in knowledges:
				knowledge.score = None

		return knowledges[:_limit]

	@classmethod
	def _search_elastic(cls, query: str | list[float], knowledge_source_ids: list[str | int] = None, limit: int = 3,
						min_score: float = 0.3, max_gap: float = None, unique: bool = True, search_filter: Filter = None,
						rerankers: list[str] = [], client: Any = None, store_client: BaseVectorStoreClient = None) -> list['Knowledge']:

		from interlinked.core.clients.elasticsearchclient import ElasticSearchClient

		if rerankers:
			raise NotImplementedError(
				'Reranker not supported in Elastic search. Not available yet.'
			)

		# if query is string then generate embedding from client
		query_embedding: list[float] = query if isinstance(query, list) else AI.get_embedding(input=query, client=client)

		# check if we have a collection already created. if not, return an empty list
		if not store_client.get_has_collection(name=cls.__collection_name__):
			return []

		search_filter = search_filter or []

		if knowledge_source_ids is not None:
			search_filter.append({'terms': {'payload.knowledge_source_id': knowledge_source_ids}})

		scored_points: list[dict] = []

		try:
			query_result = store_client.query_points(collection_name=cls.__collection_name__, query=query_embedding,
													 limit=limit, score_threshold=min_score, query_filter=search_filter)
			scored_points = query_result.points

		except Exception as exception:

			# the collection was created with an embedding model that is different from the `AI.client`
			if "mapper_parsing_exception" in str(exception):
				raise UserError(
					"The database was populated with embeddings from a different model. "
					"To fix this, you need to delete the database then create the data again."
				)

		# remove duplicate points based on content and Knowledge Source ID
		if unique:

			scored_points = {scored_point['_source']['payload'].get('content'): scored_point for scored_point in scored_points}.values()
			scored_points = list(scored_points)

		# convert scored points to Knowledge
		knowledges: list[Knowledge] = []

		for scored_point in scored_points:

			knowledge: Knowledge = cls(store_client=store_client, id=scored_point.get("_id"), content=None,
									   embedding=scored_point['_source']['vector'], score=scored_point.get('_score'))

			# set each key in the payload to the corresponding field in `Knowledge`
			# since `Knowledge` class can be inherited with custom fields, we'll set their values
			for key, value in scored_point['_source'].get('payload').items():

				if key in knowledge.model_fields:
					setattr(knowledge, key, value)

			knowledges.append(knowledge)

		return knowledges[:limit]

	@classmethod
	def _search_milvus(cls, query: str | list[float], knowledge_source_ids: list[str | int] = None, limit: int = 3,
					   min_score: float = 0.3, max_gap: float = None, unique: bool = True, search_filter: Filter = None,
					   rerankers: list[str] = [], client: Any = None, store_client: BaseVectorStoreClient = None) -> list['Knowledge']:

		from interlinked.core.clients.milvusclient import MilvusClient

		if not isinstance(store_client, MilvusClient):
			raise ValueError('Incorrect store client passed')

		if rerankers:
			raise NotImplementedError('Reranker not yet implemented for MilvusClient')

		# check if we have a collection already created. if not, return an empty list
		if not store_client.get_has_collection(name=cls.__collection_name__):
			return []

		# 1. build the query/search parameters
		# If query is string then generate embedding from client
		query_embedding: list[float] = query if isinstance(query, list) else AI.get_embedding(input=query, client=client)

		_limit: int = limit + 3

		# TODO: need to update to be compatible with `Filter` objects
		if search_filter is None:
			search_filter = []

		if knowledge_source_ids is not None:
			search_filter.append(f'payload[\'knowledge_source_id\'] in {knowledge_source_ids}')

		scored_points: list[dict] = []
		filter_string = 'and '.join(search_filter)

		# 2. run the search
		# TODO: need to update to use min_score and max_gap
		try:
			scored_points = store_client.search(collection_name=cls.__collection_name__, data=[query_embedding],
												limit=_limit, filter=filter_string, output_fields=['vector', 'payload'],
												search_params={'params': {'radius': min_score}})
		except Exception as exception:

			# the collection was created with an embedding model that is different from the `AI.client`
			if exception.message.startswith('vector dimension mismatch'):
				raise UserError(
					'The database was populated with embeddings from a different model. '
					'To fix this, you need to delete the database then create the data again.'
				)

		# 3. Drop duplicates
		# remove duplicate points based on content and Knowledge Source ID
		if unique:
			scored_points = {
				scored_point['entity']['payload'].get('content'): scored_point
				for scored_point in scored_points[0]
			}.values()
			scored_points = list(scored_points)

		# 4. Create the knowledges
		# convert scored points to Knowledge
		knowledges: list[Knowledge] = []

		for scored_point in scored_points:

			knowledge: Knowledge = cls(store_client=store_client, id=str(scored_point.get('id')), content=None,
									   embedding=scored_point['entity']['vector'], score=scored_point.get('distance'))

			# set each key in the payload to the corresponding field in `Knowledge`
			# since `Knowledge` class can be inherited with custom fields, we'll set their values
			for key, value in scored_point['entity'].get('payload').items():

				if key in knowledge.model_fields:
					setattr(knowledge, key, value)

			knowledges.append(knowledge)

		return knowledges[:_limit]

	@classmethod
	def create(cls, store_client: BaseVectorStoreClient = None, **kwargs) -> 'Knowledge':
		"""
		[Internal]
		Creates an instance + row of this Knowledge

		@param store_client(Any): (optional) A custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		metadata: dict[str, Any] = kwargs.pop('metadata', {})
		knowledge: Knowledge = cls(store_client=store_client, **kwargs)

		if not store_client.get_has_collection(name=cls.__collection_name__):
			store_client.create_collection(name=cls.__collection_name__, size=len(knowledge.embedding))

		payload: dict[str, Any] = {

			'content': knowledge.content,
			'examples': knowledge.examples,
			'external_id': knowledge.external_id,
			'content_hash': knowledge.content_hash,
			'knowledge_source_id': knowledge.knowledge_source_id,
		}

		# if there's metadata, add each key to the payload
		if metadata:

			for key, value in metadata.items():

				# make sure none of the keys is internal
				if key in payload:
					raise UserError(f'`metadata` has a key `{key}` that is internal to Interlinked. Please use a different name')

				payload[key] = value

		try:

			if type(store_client).__name__ == 'MilvusClient':
				knowledge.id = store_client.insert(embedding=knowledge.embedding, payload=payload, collection_name=cls.__collection_name__)

			else:
				knowledge.id = store_client.upsert(embedding=knowledge.embedding, payload=payload, collection_name=cls.__collection_name__)

		except ValueError as error:

			# the collection was created with an embedding model that is different from the `AI.client`
			if 'shape' in str(error):
				raise UserError('The database was populated previously with embeddings from a different model. '
								'To fix this, you need to delete the database via `interlinked chat -d` then create the data again.')

		return knowledge

	@classmethod
	def get_or_none(cls, content: str, knowledge_source_id: str|int = None, store_client: BaseVectorStoreClient = None, **kwargs) -> 'Knowledge':
		"""
		[Internal]
		Returns an existing instance of `Knowledge`

		@param store_client(Any): (optional) a custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		if not store_client.get_has_collection(name=cls.__collection_name__):
			return None

		content_hash: str = cls.get_hash_for_content(content=content)

		# ElasticSearch
		if type(store_client).__name__ == 'ElasticSearchClient':

			return cls._get_or_none_elastic(content=content, content_hash=content_hash,
											knowledge_source_id=knowledge_source_id, store_client=store_client, **kwargs)

		# Milvus
		elif type(store_client).__name__ == 'MilvusClient':

			return cls._get_or_none_milvus(content=content, content_hash=content_hash,
										   knowledge_source_id=knowledge_source_id, store_client=store_client, **kwargs)

		# Qdrant
		return cls._get_or_none_qdrant(content=content, content_hash=content_hash,
									   knowledge_source_id=knowledge_source_id, store_client=store_client, **kwargs)

	@classmethod
	def _get_or_none_qdrant(cls, content: str, content_hash: str, knowledge_source_id: int = None,
							store_client: BaseVectorStoreClient = None, **kwargs) -> 'Knowledge':

		from qdrant_client.models import Filter, FieldCondition, MatchValue, ScoredPoint

		# find a Knowledge with the same `content_hash` and `KnowledgeSource`
		scroll_filter: Filter = Filter(must=[
			FieldCondition(key='content_hash', match=MatchValue(value=content_hash)),
		])

		# filter by `KnowledgeSource`, if ID is set
		if knowledge_source_id:
			scroll_filter.must.append(FieldCondition(key='knowledge_source_id', match=MatchValue(value=knowledge_source_id)))

		scored_points: list[ScoredPoint] = store_client.scroll(collection_name=cls.__collection_name__,
															   scroll_filter=scroll_filter, with_vectors=True, limit=1)[0]

		if not scored_points:
			return None

		# convert the scored point/payload to `Knowledge`
		scored_point: ScoredPoint = scored_points[0]
		knowledge = scored_point.payload
		return Knowledge(store_client=store_client, **{**knowledge, 'id': scored_point.id, 'embedding': scored_point.vector})

	@classmethod
	def _get_or_none_elastic(cls, content: str, content_hash: str, knowledge_source_id: int = None,
							 store_client: BaseVectorStoreClient = None, **kwargs) -> 'Knowledge':

		from interlinked.core.clients.elasticsearchclient import ElasticSearchClient

		# 🔎 find Knowledge with the same `content_hash` and `KnowledgeSource`
		scored_points: list[dict] = store_client._search_by_content_hash_or_knowledge_source(
			collection_name=cls.__collection_name__,
			content_hash=content_hash,
			knowledge_source_id=knowledge_source_id)

		if len(scored_points) == 0:
			return None

		# convert the scored point/payload to `Knowledge`
		scored_point: dict = scored_points[0]
		scored_point_source: dict[str, Any] = scored_point['_source']

		return Knowledge(
			id=scored_point['_id'],
			store_client=store_client,
			embedding=scored_point_source['vector'],
			content=scored_point_source.get('content', ''),
			external_id=scored_point_source.get('external_id'),
			content_hash=scored_point_source.get('content_hash'),
			knowledge_source_id=scored_point_source.get('knowledge_source_id')
		)

	@classmethod
	def _get_or_none_milvus(cls, content: str, content_hash: str, knowledge_source_id: int = None,
							store_client: BaseVectorStoreClient = None, **kwargs) -> 'Knowledge':

		from interlinked.core.clients.milvusclient import MilvusClient

		if not isinstance(store_client, MilvusClient):
			raise ValueError('Incorrect store client passed')

		filter_string = f"payload['content_hash'] == '{content_hash}'"

		if knowledge_source_id is not None:
			filter_string += f" and payload['knowledge_source_id'] == '{knowledge_source_id}'"

		found_matches = store_client.query(
			collection_name=cls.__collection_name__,
			filter=filter_string,
		)

		if found_matches is None or len(found_matches) == 0:
			return None

		match = found_matches[0]
		payload = match['payload']

		return Knowledge(
			id=str(match['id']),
			store_client=store_client,
			embedding=match['vector'],
			content=payload['content'],
			external_id=match.get('external_id', ''),
			content_hash=match.get('content_hash', ''),
			knowledge_source_id=match.get('knowledge_source_id', '')
		)

	@classmethod
	def get_or_create(cls, store_client: BaseVectorStoreClient = None, client: BaseAIClient = None, **kwargs) -> tuple[Any, bool]:
		"""
		[Internal]
		Returns an existing Knowledge or creates it

		@param store_client(Any): (optional) a custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		@param client(BaseAIClient): (optional) The AI client that was used. Defaults to the AI client returned by `AI.client`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		is_new: bool = False
		knowledge: Knowledge = cls.get_or_none(store_client=store_client, **kwargs)

		# if no matches, create `Knowledge`
		if not knowledge:

			is_new = True
			embedding: list[float] = kwargs.pop('embedding', AI.get_embedding(input=kwargs['content'], client=client))
			knowledge = cls.create(embedding=embedding, store_client=store_client, **kwargs)

		return knowledge, is_new

	@classmethod
	def get_examples(cls, knowledge_source_ids: list[int|str] = None, limit: int = 4,
					 store_client: BaseVectorStoreClient = None) -> list[str]:
		"""
		Returns a list of knowledge examples. This is something you can generate for each `Knowledge` after calling `AI.learn`

		@param knowledge_source_ids(list): (optional) Limit search to a list of Knowledge Sources IDs (e.g. certain files)
										   Knowledge Source IDs are unique IDs you may want to generate per file/knowledge source
										   when calling `AI.learn`. You can then use them to filter the knowledge you search for
		@param limit(int): the maximum number of examples to return
		@param store_client(Any): (optional) a custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		# check if we have a collection already created. if not, return an empty list
		if not store_client.get_has_collection(name=cls.__collection_name__):
			return []

		examples: list[str] = []

		# ⛽ if Store Client is ElasticSearch
		if type(store_client).__name__ == 'ElasticSearchClient':

			from interlinked.core.clients.elasticsearchclient import ElasticSearchClient

			# add the knowledge_source_id filter if provided
			if knowledge_source_ids:

				knowledge_source_ids_query = {'terms': {'payload.knowledge_source_id': knowledge_source_ids}}
				filter_: dict = {'query': {'nested': {'path': 'payload', 'query': {'bool': {'should': [knowledge_source_ids_query]}}}},
								 'size': limit}

			else:
				filter_: dict = {'query': {'match_all': {}}, 'size': limit}

			# execute the search
			scored_points = store_client.search_by_query(cls.__collection_name__, filter_).get('hits', [])

			for scored_point in scored_points:
				examples.extend(scored_point['_source']['payload'].get('examples', []))

		else:

			from qdrant_client.models import Filter, FieldCondition, MatchAny, ValuesCount, ScoredPoint

			scroll_filter: Filter = Filter(must=[
				FieldCondition(key='examples', values_count=ValuesCount(gt=0)),
			])

			if knowledge_source_ids:
				scroll_filter.must.append(FieldCondition(key='knowledge_source_id', match=MatchAny(any=knowledge_source_ids)))

			scored_points: list[ScoredPoint] = store_client.scroll(collection_name=cls.__collection_name__, scroll_filter=scroll_filter, limit=limit)[0]

			# convert the scored point/payload to `Knowledge`
			examples = sum([scored_point.payload.get('examples') for scored_point in scored_points], [])

		examples = list(set(examples))
		random.shuffle(examples)
		return examples

	@classmethod
	def get_count(cls, knowledge_source_ids: list[int|str] = None, store_client: BaseVectorStoreClient = None) -> int:
		"""
		Returns a count of Knowledge for a give list of `KnowledgeSource` IDs

		@param store_client(Any): (optional) a custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		# check if we have a collection already created. if not, return an empty list
		if not store_client.get_has_collection(name=cls.__collection_name__):
			return 0

		# elasticsearch
		if type(store_client).__name__ == 'ElasticSearchClient':

			from interlinked.core.clients.elasticsearchclient import ElasticSearchClient

			# add the knowledge_source_id filter if provided
			if knowledge_source_ids:

				knowledge_source_ids_query = {'terms': {'payload.knowledge_source_id': knowledge_source_ids}}
				filter_: dict = {'query': {'nested': {'path': 'payload',
													  'query': {'bool': {'should': [knowledge_source_ids_query]}}}}}

			else:
				filter_: dict = {'query': {'match_all': {}}}

			# Execute the search
			response = store_client.search_by_query(cls.__collection_name__, filter_)

			# Return the count of matching documents
			return response['total']['value']

		# qdrant
		elif type(store_client).__name__ == 'QdrantClient':

			from qdrant_client.models import Filter, FieldCondition, MatchAny, ValuesCount

			filter_: Filter = Filter(must=[])

			if knowledge_source_ids:
				filter_.must.append(FieldCondition(key='knowledge_source_id', match=MatchAny(any=knowledge_source_ids)))

			return store_client.count(collection_name=cls.__collection_name__, count_filter=filter_, exact=True).count

		else:
			raise NotImplementedError('unexpected VectorStore client')

	@classmethod
	def get_all_knowledge_source_ids(cls, store_client: BaseVectorStoreClient = None, limit: int = 100) -> list[int|str]:
		"""
		Returns a unique list of all Knowledge Source IDs stored

		The method should ideally only be used for testing as it searches the entire vector store to list the IDs.
		Qdrant does not currently support unique search.

		@param store_client(Any): (optional) A custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		# if there is no collection created, return an empty list
		if not store_client.get_has_collection(name=cls.__collection_name__):
			return []

		# ⛽ if Store Client is ElasticSearch
		if type(store_client).__name__ == 'ElasticSearchClient':

			from interlinked.core.clients.elasticsearchclient import ElasticSearchClient

			query: dict[str, Any] = {'query': {'match_all': {}}}

			# execute the query
			scored_points = store_client.search_by_query(cls.__collection_name__, query).get('hits',[])

			# extract the knowledge_source_ids from scored points
			knowledge_source_ids = [scored_point['_source']['payload'].get('knowledge_source_id') for scored_point in scored_points]

			# remove `None` values and return unique ids
			return list(set(filter(None, knowledge_source_ids)))

		# ⛽ If Store Client is Qdrant
		from qdrant_client.models import Filter, PayloadField, ScoredPoint, IsNullCondition

		scroll_filter: Filter = Filter(must_not=[
			IsNullCondition(is_null=PayloadField(key='knowledge_source_id'))
		])

		scored_points: list[ScoredPoint] = store_client.scroll(collection_name=cls.__collection_name__, scroll_filter=scroll_filter,
															   with_vectors=False, limit=limit)[0]
		return list(set([scored_point.payload.get('knowledge_source_id') for scored_point in scored_points]))

	@classmethod
	def delete_all(cls, knowledge_source_ids: list[int|str] = None, keep: list['Knowledge'] = None, store_client: BaseVectorStoreClient = None) -> None:
		"""
		Deletes all Knowledge for a given KnowledgeSource

		@param knowledge_source_ids(list): (optional) The KnowledgeSource IDs to delete Knowledge from.
										   if none passed, all knowledge is deleted
		@param keep(list): (optional) Delete all Knowledge except some
		@param store_client(Any): (optional) A custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		"""

		if not store_client:
			store_client = QdrantClient.shared

		# if no Knowledge Source IDs are passed, delete
		# the entire collection
		if not knowledge_source_ids:

			store_client.delete_collection(name=cls.__collection_name__)
			return

		if type(store_client).__name__ == 'ElasticSearchClient' and knowledge_source_ids:
			raise Exception('Elastic does not support deleting by knowledge source ids.')

		from qdrant_client.models import HasIdCondition

		filters: list[FieldCondition] = []

		if keep:
			filters.append(HasIdCondition(has_id=[knowledge.id for knowledge in keep]))

		store_client.delete(id_key='knowledge_source_id', ids=knowledge_source_ids, collection_name=cls.__collection_name__,
							keep_filters=filters)

	@classmethod
	def get_hash_for_content(cls, content: str) -> str:
		"""
		[Internal]
		Creates and returns a unique hash for a given content.
		Used as an ID to avoid recreating Knowledge

		@param content(str): The string content to get the hash for
		"""
		return Utilities.get_sha256_hash(value=content)

	def save(self, *args, **kwargs):
		"""
		Saves changes to this Knowledge
		"""

		store_client: BaseVectorStoreClient = self.store_client

		if not store_client:
			store_client = QdrantClient.shared

		store_client.update(id=self.id, embedding=self.embedding, payload={

			'content': self.content,
			'examples': self.examples,
			'external_id': self.external_id,
			'content_hash': self.content_hash,
			'knowledge_source_id': self.knowledge_source_id,

		}, collection_name=self.__collection_name__)

	""" Internal: Rerankers """

	@classmethod
	def rerank_with_transformers(cls, knowledges: list['Knowledge'], prompt: str,
								 model_name: str = 'colbert-ir/colbertv2.0') -> list['Knowledge']:
		"""
		[Internal/Not available yet] Please do not use
		Reranks/sorts a list of Knowledge using MaxSim and a reranking model, and updates `Knowledge.score`
		"""

		# import torch
		try:
			import torch

		except ImportError as error:
			raise UserError('To use this reranker, you need to run `pip3 install torch`.')

		from torch import Tensor
		from pathlib import Path

		# import transformers
		try:
			from transformers import AutoTokenizer, AutoModel
			from transformers.tokenization_utils_base import BatchEncoding

		except ImportError as error:
			raise UserError('To use this reranker, you need to run `pip3 install transformers`.')

		device: str = 'cuda' if torch.cuda.is_available() else 'cpu'

		def maxsim(tensor_1: Tensor, tensor_2: Tensor) -> Tensor:
			"""
			Returns tensor containing averaged maximum similarity between two tensors (embeddings)

			@param tensor_1(Tensor): an encoded string (using `.encode`)
			@param tensor_2(Tensor): an encoded string (using `.encode`)
			"""
			sim_matrix: Tensor = torch.nn.functional.cosine_similarity(tensor_1.unsqueeze(1), tensor_2.unsqueeze(0), dim=-1)
			max_sim_scores, _ = torch.max(sim_matrix, dim=-1)
			average_max_sim: Tensor = torch.mean(max_sim_scores, dim=-1)
			return average_max_sim

		def encode(string: str) -> Tensor:
			"""
			Returns an encoded tensor from a given string (prompt or Knowledge content)

			@param string(str): the string prompt or Knowledge content
			"""

			inputs: BatchEncoding = tokenizer(string, return_tensors='pt', padding=True, truncation=True)
			inputs = inputs.to(device)

			with torch.no_grad():
				return model(**inputs).last_hidden_state

		# load the model
		tokenizer: Any = AutoTokenizer.from_pretrained(model_name, local_files_only=not Config.current.is_development)
		model: Any = AutoModel.from_pretrained(model_name, local_files_only=not Config.current.is_development).to(device)

		# encode the prompt
		prompt_tensor: Tensor = encode(string=prompt).mean(dim=1)

		# encode Knowledge and get maxsim of each
		for knowledge in knowledges:

			knowledge_tensor: Tensor = encode(string=knowledge.content)
			knowledge.score = maxsim(tensor_1=prompt_tensor, tensor_2=knowledge_tensor).item()

		knowledges = sorted(knowledges, key=lambda knowledge: knowledge.score, reverse=True)
		return knowledges

	@classmethod
	def rerank_with_mmr(cls, knowledges: list['Knowledge'], prompt: str|list[float]) -> list['Knowledge']:
		"""
		[Internal/Not available yet] Please do not use
		Reranks/sorts a list of Knowledge using MMR, and updates `Knowledge.score`
		"""

		prompt: list[float] = prompt if isinstance(prompt, list) else AI.get_embedding(input=prompt)
		return knowledges

	@classmethod
	def rerank_with_prompt(cls, knowledges: list['Knowledge'], prompt: str) -> list['Knowledge']:
		"""
		[Internal] Please do not use directly
		Reranks/sorts a list of Knowledge using references and a reranking model, and updates `Knowledge.score`
		"""

		words: list[str] = prompt.split()
		knowledges = sorted(knowledges, key=lambda knowledge: len(Utilities.get_words_in_text(words=words, text=knowledge.content)), reverse=True)
		return knowledges

	def __str__(self) -> str:

		header: str = f'{"=" * 5} {self.score} {"=" * 5}'
		return f'{header}\n{self.content}\n{"=" * len(header)}'

	__repr__ = __str__


""" Observation """

class Observation:
	"""
	The class is returned by `AI.ask(…)` and contains AI's
	response along with many important attributes.

	You would generally use `AI.ask(…)` and receive an `Observation` back:
	```python
	from interlinked import AI, Observation

	observation: Observation = AI.ask(prompt='Can you say hello world?')
	print(observation.response)
	```
	"""

	# the AI client that was used to generate the response (e.g. `OpenAI`, `AJAXClient`, etc)
	client: BaseAIClient = None

	# the template that was used to generate the response
	template: str = None

	# the prompt that was used for this Observation
	prompt: str|dict[str, str] = None

	# contains a list of messages between the user and AI.
	# You can save this to a JSON file, then resume later via `AI.ask(messages=…)`
	messages: list[dict[str, str]] = None
	events: list[Event] = None

	# a list of tools that were originally passed in `AI.ask(…, tools=[…])`
	tools: list[Tool] = None

	# whether we are calling AI (e.g. a phone call)
	realtime: bool = None

	# the raw response
	_response: Response = None
	audio_bytes: bytes = None
	event: Event = None

	# [Internal] used to queue `ToolResponse`s
	# to return them back to the caller
	_queued_events: list[Event] = None

	# used for caching `response_as_dictionary`
	_response_as_dictionary: dict[str, Any] = None

	def __init__(self, response: Response|str = None, template: Template|str = None,
				 prompt: str|list[dict] = None, messages: list[dict[str, str]] = None,
				 events: list[Event] = None, tools: list[Tool] = None,
				 client: BaseAIClient = None, call: Call = None, event: Event = None):
		"""
		Initializes `Observation`, which is the class returned by `AI.ask(…)`.

		@param response(Response|str): (optional) AI's response
		@param template(Template|str): (optional) The Template used in `AI.ask(…, template=…)`
		@param prompt(str|list): (optional) The prompt used to ask/chat with AI in `AI.ask(prompt=…)`
		@param messages(list): (optional) A list of messages with AI so far. Used to maintain context
		@param tools(list): (optional) A list of functions/tools. Used in `AI.ask(…, tools=…)`
		@param client(BaseAIClient): (optional) The AI client that was used. Defaults to the AI client returned by `AI.client`
		@param call(Call): (Internal) Whether to use Realtime call with AI, like a phone call
		@param event(Event): (Internal) The `Event` from the Realtime call that created this `Observation`
		"""

		# for convenience, if there's no template passed, but the prompt has fields,
		# treat the prompt as a template to take advantage of type-validation
		if not template and prompt and isinstance(prompt, str):

			_template: Template = Template(text=prompt, require_macros=True)

			if len(_template.fields) > 0:

				template = _template
				prompt = None

		elif template and isinstance(template, str):
			template = Template(text=template)

		self.call = call
		self.tools = tools
		self.event = event
		self.prompt = prompt
		self.template = template
		self.events = events or []
		self.messages = messages or []
		self.client = client or AI.client

		# set response after everything has been initialized
		self.response = response

	def ask(self, prompt: str|dict[str, Any] = None, template: Template|str = None, tools: list[Tool|Callable] = None,
			messages: list[dict[str, str]] = None, files: list[str|bytes|File] = None, validator: Callable = None, max_iterations: int = 3,
			temperature: float = 0.6, dsid: int = None, realtime: bool = False, events: list[Event|dict] = None,
			stream: bool = False, completion: bool = False, tool_call_validator: Callable = None) -> Self | Iterator[Self]:
		"""
		Asks AI a follow-up question.
		This creates a new Observation and does not update this one (the `.prompt` stays the same)

		For full documentation of this function, see `ask` in the `AI` class.

		@return (Observation): returns a new `Observation` or iterator of Observation
		"""

		# for convenience, if there's no template passed, but the prompt has fields,
		# treat the prompt as a template to take advantage of type-validation
		if not template and prompt and isinstance(prompt, str):

			_template: Template = Template(text=prompt, require_macros=True)

			if len(_template.fields) > 0:

				template = _template
				prompt = None

		# if the template is a string (which is true almost in all cases), cast to `Template`
		if template and not isinstance(template, Template):
			template = Template(text=template)

		# raise an exception if there's a template but the prompt is a string
		# this is because when there's a template, the expectation is that `prompt` is a dictionary
		# that is used to populate placeholder values in the template
		if prompt and isinstance(prompt, str) and template:
			raise UserError('Did you mean to pass a dictionary as prompt instead?')

		# raise an exception if there's a dict prompt but no template
		if isinstance(prompt, dict) and not template:
			raise UserError('Missing `template` argument.')

		if files and any(isinstance(file, File) for file in files) and self.client.__class__.__name__ not in {'GoogleAIClient', 'AnthropicAIClient'}:
			raise UserError('You can only use `files` with `File` type with `GoogleAIClient`')

		# if we have a prompt but no template,
		# convert the prompt into a template, which allows us to later apply
		# features, such as auto-detection of format
		if not template and prompt:

			template = Template(text=prompt)
			prompt = None

		# the template but populated with the prompt values
		populated_text: str = prompt

		# populate the template
		if template and isinstance(prompt, dict):
			populated_text = template.get_populated_text(prompt=populated_text)

		elif template and not prompt:
			populated_text = template.get_populated_text(prompt={})

		# if the user is asking for a completion, return early
		if completion:

			# tools + completions are not supported
			if tools:
				raise UserError('completion and tools arguments can\'t be used at the same time')

			ai_client_response: AIClientResponse | Iterator[AIClientResponse] =  \
				self.client.get_completion(prompt=populated_text, temperature=temperature, stream=stream)

			# if we're streaming, convert each response to `Observation`
			if stream:

				def _get_observation_for_generator(generator: Iterator) -> Observation:
					"""
					Internal function that converts `AIClientResponse` to `Observation`
					"""

					for ai_client_response in generator:

						yield Observation(response=Response(string=ai_client_response.content, files=ai_client_response.files, template=template, metadata=ai_client_response.metadata),
										  template=template, prompt=prompt, client=self.client)

				return _get_observation_for_generator(generator=ai_client_response)

			return Observation(response=Response(string=ai_client_response.content, files=ai_client_response.files, template=template),
							   template=template, prompt=prompt, client=self.client)

		# prepare messages to be passed to the AI client
		if not messages and populated_text:
			messages = [self.client.create_message(role=self.client.ROLE_USER, content=populated_text, files=files)]

		if not (realtime or self.call):

			if self.messages and messages:
				messages = self.messages + messages

			# `prompt`, `template`, and `messages` may be empty when resuming an observation
			# but `self.messages` will be set
			elif self.messages:
				messages = self.messages

		# if we don't have tools passed to `ask`, check if we had ones previously
		# set (e.g. `AI.ask(…, tools=[…])`, then user follows up via `observation.ask(…)`)
		if not tools:
			tools = self.tools or []

		if tools and stream:

			if self.client.__class__.__name__ in {'OpenAIClient', 'GoogleAIClient', 'AnthropicClient'}:

				if len(messages) < 2:
					logger.warning('ℹ️ streaming with Function Calling is currently in beta. results may be unexpected')

			else:
				raise UserError('only `OpenAIClient`, `GoogleAIClient`, and `AnthropicClient` supports stream and tools at the same time')

		if max_iterations <= 0:
			raise UserError('max_iterations must be at least 1')

		# ensure that all the tools are type `Tool`
		# convert bundle IDs into tools
		_tools: list = []

		for tool in tools:

			if isinstance(tool, str) and tool.startswith('com.'):
				_tools += Applet.get(bundle_id=tool).get_tools(client=self.client)

			# the user accidentally passed a nested list of `Tool`s
			elif isinstance(tool, list):
				raise UserError('You passed a nested list of tools to `AI.ask(…, tools=[[…]])`')

			else:
				_tools.append(tool if isinstance(tool, Tool) else Tool(function=tool))

		tools = _tools

		# if we're starting a Realtime call or we're already on a call
		if realtime or self.call:

			# if we're already on a call,
			# this means the user wants to say/add something to AI
			# so, we append each message/event or prompt to the call
			if self.call:

				async def _ask():

					if events:

						for event in events:
							await self.call.send(event=event.raw if isinstance(event, Event) else event)

					else:

						# reply to AI by creating a reply event
						for event in self.client.create_events(type=self.client.EVENT_ITEM_CREATE, role=self.client.ROLE_USER,
															   content=populated_text):
							await self.call.send(event=event.raw)

				return _ask()

			# we're not on a call yet. start a new call
			# and append any messages/events or prompt the user passed to `AI.ask(…)`
			async def _start_call():

				call: Call = await self.client.start_call(tools=tools)

				if events:

					for event in events:
						await self.call.send(event=event.raw if isinstance(event, Event) else event)

				elif populated_text:

					# ask AI by creating a reply event
					for event in self.client.create_events(type=self.client.EVENT_ITEM_CREATE, role=self.client.ROLE_USER,
														   content=populated_text):
						await call.send(event=event.raw)

				return Observation(call=call, tools=tools, client=self.client)

			return _start_call()

		iteration: int = 0
		observation: Observation = None
		previous_tool_calls: list[ToolCall] = []
		previous_response_statuses: list[str] = []

		# resuming is when `Observation` is initialized and `.ask` is called with a list of previous messages
		# e.g. tool approval flow, where user approves running a tool
		is_resuming: bool = False

		# the criteria for resuming is that there's no prompt or template passed, just the previous messages
		# and the last message is a tool call
		if not prompt and not template and messages and  \
		   (ai_client_response := self.client.get_ai_client_response_for_message(message=messages[-1])):

			if ai_client_response.tool_calls and len(ai_client_response.tool_calls) > 0:
				is_resuming = True

		while iteration < max_iterations:

			iteration += 1

			# 1️⃣ ask AI
			ai_client_response: AIClientResponse = None

			if is_resuming:

				ai_client_response = self.client.get_ai_client_response_for_message(message=messages[-1])

				# streaming requires `ai_client_response` to be a generator
				if stream:
					ai_client_response = iter([ai_client_response])

			else:
				ai_client_response = self.client.get_response(messages=messages, temperature=temperature, tools=tools, stream=stream,
															  format_=template.display_format if template else None)

			# 2️⃣ handle response
			def _handle_tool_calls(tool_calls: list[ToolCall], ai_client_response: AIClientResponse) -> str:
				"""
				Internal function for handling `ToolCall` for streamed and non_streamed responses

				@param tool_calls(list): A list of `ToolCall`
				@return (str): contains 'break', 'continue', or `None`
				"""

				nonlocal iteration
				nonlocal is_resuming

				# used to stop iterating `iteration` if an issue occurs (e.g. stuck in a loop)
				should_stop: bool = False

				# when we're resuming, there's no prompt or template, just previous messages
				# this means that we don't want to re-append the same message that we wanted to resume
				# but do append in all other cases (e.g. when using `AI.ask(…, tools=…)`)
				if not is_resuming:
					messages.append(ai_client_response.raw)

				response_status: str = None

				for tool_call in tool_calls:

					response_status = None

					tool: Tool = next((tool for tool in tools if tool_call.name in {tool.name, tool.alternative_name}), None)
					tool_response: Any|ToolResponse = None

					# if the `tool_call` calls an invalid tool name (should never occur)
					if not tool:
						raise Exception(f'The model you are using hallucinated and called a tool that does not exist: {tool_call.name}')

					logger.section(f'calling {tool.alternative_name or tool.name}({tool_call.display_arguments})')


					if tool_call_validator and not is_resuming:
						tool_response = tool_call_validator(tool_call=tool_call, messages=messages)

					if not tool_response:

						try:
							tool_response = tool.run(arguments=tool_call.arguments)

						except MissingArguments as error:

							# if the call is missing an argument (or multiple), push-back with the argument names
							logger.warning(f'response is missing "{error.argument_names}" arguments. trying again…')
							response_status = f'Error: Try again with "{", ".join(error.argument_names)}"'

					if isinstance(tool_response, str) and tool_response.startswith('Error:'):
						logger.warning(f'{tool.alternative_name or tool.name} returned an error ({tool_response})')

					if not isinstance(tool_response, ToolResponse):
						tool_response = ToolResponse(content=tool_response if tool_response is not None else 'Success')

					# check if we're stuck calling the same tool + same arguments
					# `repeated_identical_count`: number or times the tool was called (repeatedly) with the same arguments
					# `repeated_count`: number or times the tool was called (repeatedly)
					repeated_identical_count: int = sum(1 for tool_call in takewhile(lambda _tool_call: _tool_call.name == tool_call.name and _tool_call.raw_arguments == tool_call.raw_arguments, reversed(previous_tool_calls)))
					repeated_count: int = sum(1 for tool_call in takewhile(lambda _tool_call: _tool_call.name == tool_call.name, reversed(previous_tool_calls)))

					if repeated_identical_count >= max(max_iterations, 10) - 2:

						logger.error(f'stuck calling {tool.alternative_name or tool.name} in a loop with identical arguments ({repeated_identical_count} times). stopping…')
						should_stop = True
						break

					elif repeated_count >= max(max_iterations, 100):

						logger.error(f'stuck calling {tool.alternative_name or tool.name} in a loop ({repeated_count} times). stopping…')
						should_stop = True
						break

					else:

						content: Any = None

						if response_status:
							content = response_status

						else:

							content = tool_response.content

							# convert to dict, if the function returned a class
							if isinstance(content, BaseModel):
								content = content.model_dump()

							# convert to JSON, if the function returned a dict
							# this allows parsing the function/tool's response later from `.messages`
							if isinstance(content, (dict, list, )):
								content = json.dumps(content, default=lambda obj: str(obj))

							else:
								content = str(content)

						# some clients (e.g. GoogleAIClient) has its own way of appending messages
						# where all parallel Function Call Responses are appended to one message
						message: dict[str, Any] = self.client.create_message(role=self.client.ROLE_TOOL, content=content,
																	 files=tool_response.files if tool_response else None,
																	 tool_call_id=tool_call.id, name=tool.name)
						self.client.append_message(message=message, messages=messages)

					previous_tool_calls.append(tool_call)

				if is_resuming:
					is_resuming = False

				# stop iterating if an issue happens (e.g. stuck in a loop)
				if should_stop:
					return 'break'

				# reset iterations, since we're still processing calls
				# so we don't accidentally reach max iterations
				if not response_status:
					iteration = 0

				return 'continue'

			if stream:

				def _get_observation_for_generator(generator: Iterator) -> Observation:
					"""
					Internal function that converts `AIClientResponse` to `Observation`
					"""

					for ai_client_response in generator:

						# handle tool calls
						tool_calls_result: str = None

						if tool_calls := ai_client_response.tool_calls:
							tool_calls_result = _handle_tool_calls(tool_calls=tool_calls, ai_client_response=ai_client_response)

						observation: Observation = Observation(response=Response(string=ai_client_response.content, files=ai_client_response.files, template=template, metadata=ai_client_response.metadata),
															   prompt=prompt, template=template, messages=messages, tools=tools, client=self.client)

						if tool_calls_result:

							if tool_calls_result == 'continue':

								# TODO: this resets max_iterations
								for _observation in observation.ask(stream=True):
									yield _observation

							# TODO
							else:
								raise NotImplementedError('unhandled result in streaming')

						if observation.response.raw or (observation.response.files and len(observation.response.files) > 0):
							yield observation

				return _get_observation_for_generator(generator=ai_client_response)

			# handle tool calls
			if tool_calls := ai_client_response.tool_calls:

				tool_calls_result: str = _handle_tool_calls(tool_calls=tool_calls, ai_client_response=ai_client_response)

				if tool_calls_result == 'break':
					break

				elif tool_calls_result == 'continue':
					continue

			observation = Observation(response=Response(string=ai_client_response.content, files=ai_client_response.files, template=template, metadata=ai_client_response.metadata),
									  template=template, prompt=prompt, messages=messages, tools=tools, client=self.client)

			# return early for image-specific AI models, with no content but only files
			if not ai_client_response.content and ai_client_response.files:
				break

			messages.append(ai_client_response.raw)

			response_status: str = observation.response_status

			if not response_status and validator:
				response_status = validator(observation=observation)

			# if the response does not match the template criteria, ask the AI to try again
			if response_status:

				# if this is the same response_status as the one we've seen before,
				# combine the previous user message (previous response_status) with this one
				# to encourage AI to think of another/better response
				# e.g. AI: the answer is iPhone
				#      User (Interlinked core): iPhone is incorrect
				#      AI: the answer is iOS
				#      User (Interlinked core): iOS is incorrect
				#      AI: the answer is iPhone
				#      User (Interlinked core): think again. iPhone is incorrect and iOS is incorrect
				if response_status in previous_response_statuses:

					del messages[-1]
					previous_user_message: dict[str, Any] = messages[-1]
					previous_user_message_content: str = None

					# instead of hardcoding the content, we'd ideally have a `Message` class
					# or an internal `.get_content(…)`
					if isinstance(self.client, GoogleAIClient):
						previous_user_message_content = previous_user_message.get('parts')[0].get('text')

					elif isinstance(self.client, EndorClient):

						if 'contents' in previous_user_message:
							previous_user_message_content = previous_user_message.get('contents')[0].get('text')
						else:
							previous_user_message_content = previous_user_message.get('content') or previous_user_message.get('text')

					else:
						previous_user_message_content = previous_user_message['content']

					# if the previous response_status is the same as this one, don't duplicate them
					if previous_user_message_content == response_status:
						previous_user_message_content = f'Think again. {response_status}'

					else:
						previous_user_message_content = f'Think again. {previous_user_message_content} and {response_status}'

					# instead of hardcoding the content, we'd ideally have a `Message` class
					# or an internal `.get_content(…)`
					if isinstance(self.client, GoogleAIClient):
						previous_user_message['parts'][0]['text'] = previous_user_message_content

					elif isinstance(self.client, EndorClient):

						if 'contents' in previous_user_message:
							previous_user_message['contents'][0]['text'] = previous_user_message_content

						elif 'content' in previous_user_message:
							previous_user_message['content'] = previous_user_message_content

						elif 'text' in previous_user_message:
							previous_user_message['text'] = previous_user_message_content

					else:
						previous_user_message['content'] = previous_user_message_content

					logger.warning(f'stuck in a loop {len(previous_response_statuses)} times. trying again…')

					# clear previous responses to avoid any edge-cases
					previous_response_statuses = []
					continue

				previous_response_statuses.append(response_status)

				response_truncated: str = observation.response[:50].replace('\n', ' ')
				logger.warning(f'response is not ok. trying again… ({response_status=}, response=\'{response_truncated}…\')')

				messages.append(self.client.create_message(role=self.client.ROLE_USER, content=response_status))
				continue

			break

		return observation

	@property
	def response(self) -> Response:
		return self._response

	@response.setter
	def response(self, value: Response|str):

		# we check `not isinstance` instead because `Response` is always an instance of `str`
		if value is not None and not isinstance(value, Response):
			value = Response(string=value, template=self.template)

		self._response = value

		# since this check is only for `response_as_dictionary`,
		# check `.raw` because `Response` may have files, without a string response
		if self._response and self._response.raw:

			# convert response to dictionary/list
			response_as_dictionary: dict[Any, Any]|list[Any] = self.response_as_dictionary

			if isinstance(response_as_dictionary, dict):

				for key, _value in self.response_as_dictionary.items():

					# if one of the keys is internal to `Observation`, suffix the name with `_`
					if key in {'response', 'template', 'is_valid'}:
						key = f'{key}_'

					setattr(self, str(key), _value)

	@property
	def response_status(self) -> str:
		"""
		Returns whether the response satisfies the template criteria

		For example, if the template has required fields and the response does not have them

		@return (bool): whether the response satisfies the template criteria
		"""

		if not self.template:
			return

		response_as_dictionary: dict[Any, Any]|list[Any] = self.response_as_dictionary

		# if the template type is JSON, ensure that the response is in JSON too
		if self.template.display_format == 'json' and not response_as_dictionary:

			try:
				dictionary_representation = Tool.get_dictionary_from_string(string=self.response)

			except Exception as exception:
				return 'Your response must only be a valid JSON format'

		for field_name, field in self.template.fields.items():

			# get the response value for this field
			# the only instance where response_as_dictionary is not a dict (list instead)
			# is if the response is just a list (if the user requested a list as a response)
			# depending on the field, `field_value` could be a string, int, Enum, etc
			field_value: Any = response_as_dictionary.get(field_name) if isinstance(response_as_dictionary, dict) else None
			is_valid, reason = True, None

			if field.required and field_value is None:

				_template_text: str = '\n'.join([f'{_field.name_raw}: {_field.description}' for _field in self.template.fields.values() if _field.required])

				# if any of the description of any of the fields contains placeholders,
				# populate them with the prompt
				# e.g. `Product: the name of the product mentioned in this <#display_classifcation#>`
				_template: Template = Template(text=_template_text)

				if self.prompt and isinstance(self.prompt, dict):
					_template_text = _template.get_populated_text(prompt=self.prompt)

				# if the template text still has placeholders, that's unexpected
				# it's on the caller to ensure that all placeholders are populated in the `prompt`
				if Template.get_has_placeholders(text=_template_text):
					raise Exception(f'the template has unexpected placeholders. Please make sure you populate them by defining their values in the `prompt` ({_template_text=})')

				is_valid = False
				reason = f'Please adjust your response to follow this exact format:\n{_template_text}'

			else:
				is_valid, reason = field.get_is_valid(value=field_value)

			if not is_valid:

				# TODO: this is not the correct place to do this
				# we need a way to only set field values, if the value is valid
				setattr(self, field_name, None)

				return reason

		return None

	@property
	def response_as_dictionary(self) -> dict[str, Any]|list[Any]:
		"""
		Converts the response from LLM to a dictionary of keys and their values

		This only works for prompts that ask AI to respond with prefixed lines. For example:
		>Write a title for a security vulnerability report. Prefix it with "Title: "

		AI's response:
		>Title: Lorem Ipsum Sit Dolor Imet

		This function will return:
		```
		{'title': 'Lorem Ipsum Sit Dolor Imet'}
		```

		@return (dict): dictionary representation of the response
		"""

		if self.response is None:
			raise Exception('The model did not response (this is likely an'
							f' {self.client.__class__.__name__.replace("Client", "") if self.client else "model"}-specific issue)')

		if self._response_as_dictionary is None:
			self._response_as_dictionary = self.__class__.get_dictionary_from_string(string=self.response, template=self.template)

		return self._response_as_dictionary

	@property
	def is_valid(self) -> bool:
		"""
		Use this to check if all of the fields you requested in `AI.ask`
		have the correct type.

		Interlinked has built-in "push back" logic to ensure that the values you get
		are in the exact type you requested (e.g. if you ask for an int, Interlinked will
		ensure that you receive an int). However, with some less performant models, the
		model may still not to respond with the requested fields. Interlinked applies this
		logic n number of times before stopping and returning the values the model responded
		with in `AI.ask(…)`. You can adjust how many times you'd like Interlinked to "push back"
		using `AI.ask(…, max_iterations=5)`.

		This will return `False` if any of the fields you requested does not match the type or criteria
		you requested (regardless of whether `required:true` is set).
		"""
		return self.response_status is None

	@classmethod
	def get_dictionary_from_string(cls, string: Response|str, template: Template = None,
								   nested: bool = True) -> dict[str, Any]:
		"""
		[Internal]
		Converts an arbitrary string to a dictionary
		"""

		# return early if there's no text
		if not string or (isinstance(string, Response) and not string.raw):
			return {}

		dictionary_representation: dict[str, Any] = {}

		lines: list[str] = string.split('\n')

		for line_index, line in enumerate(lines):

			if ':' in line or '?' in line:

				# clean up the line before splitting into key/value
				# e.g. `**Category:** **security_vulnerability**` -> `Category: security_vulnerability`
				line = re.sub(r'[*]+(.*?)[*]+\s*[*]+(.*?)[*]+', r'\1 \2', line)

				# e.g. `**Fits component:** Yes` -> `**Fits component`, `Yes`
				key, value = re.split(r'(?::|\?)(?:[*"\']+|)(?:\W|)', line, maxsplit=1)

				# clean up the key/value
				# e.g. `- Importance: …` -> `importance`
				key = re.sub(r'^[*#-]+\W', '', key.strip()).strip()
				key = re.sub(r'(?<!^)\s', '_', key).replace('/', '_').lower()


				# skip key if it's not valid
				if not key.replace(' ', '').replace('_', '').isalpha():
					continue

				field: Field = template.get(key) if template else None

				# if field is not defined in the template, and the key (field name) is too long, it's likely invalid
				if not field and len(key.split('_')) > 4:
					continue

				# skip if the key is empty
				if not key:
					continue

				# e.g. `## Safari` -> `Safari`
				#      `: Safari` -> `Safari`
				value = value.strip().strip('.').strip(':').strip()

				# strip characters if the value is wrapped with them (e.g. `"Safari"` -> `Safari`)
				# if only the first word is, don't strip (e.g. `"Safari" OOB`)
				for character in {'\'', '"', '**', '*'}:

					if value.startswith(character) and value.endswith(character):
						value = value.strip(character)

				value = value.strip()
				value = re.sub(r'^[*#-]+\W', '', value)
				value_lower: str = value.lower()

				# pre-process the value
				# convert the value based on the type, only if the field is not another type (e.g. enum)
				if not (field and field.type != Field.TYPE_BOOL) and value_lower in {'true', 'yes'}:
					value = True

				elif ((field and field.type == Field.TYPE_BOOL) or key.startswith('is_')) and  \
					 (value_lower.startswith(('true', 'yes')) or value_lower == '1'):
					value = True

				elif not (field and field.type != Field.TYPE_BOOL) and value_lower in {'false', 'no'}:
					value = False

				elif ((field and field.type == Field.TYPE_BOOL) or key.startswith('is_')) and  \
					 (value_lower.startswith(('false', 'no')) or value_lower == '0'):
					value = False

				# treat the following as `None` unless this is an enum field and one of the options is that value
				# e.g. skip if field.enum_options = {'yes', 'no'}
				elif (matches := re.findall(r'^(n/a|na|none|unknown|no|not specified)\b', value_lower)) and  \
					 not (field and field.type == Field.TYPE_ENUM and field.enum_options and Utilities.get_words_in_text(words=field.enum_options, text=matches[0])):
					value = None

				elif Utilities.get_is_numeric(value=value_lower):

					# if we're expecting a float/double and the number
					if field and field.type == Field.TYPE_FLOAT and '.' not in value_lower:
						value_lower = f'{value_lower}.0'

					if '.' in value_lower:
						value = float(value)

					else:

						# if the number is truly numeric but can't be cast to int
						# e.g. `六`, which is 6, we'll keep it a string
						try:
							value = int(value)

						except ValueError:
							pass

				elif value_lower.startswith(('{', '[')):

					# if the field is an enum, check if the value is any of the enum options
					if field and field.type == Field.TYPE_ENUM:

						# e.g. `[valid]` -> `valid`
						value = re.sub(r'(?:\[|\{)(.*?)(?:\}|\])', r'\1', value)

					else:

						try:
							value = Tool.get_dictionary_from_string(string=value)

						except Exception as exception:

							# if we have a template and this is an unknown field, just use the raw value/ignore it
							if field and field.type == Field.TYPE_JSON:
								logger.warning(f'could not get_dictionary_from_string ({string=})', exc_info=True)

				elif value is not None and field and field.type == Field.TYPE_CODE:

					# if the value starts with text, then code, focus only on the code
					# e.g. `Code: here's a function for this use-case ```swift…` -> ````swift…``
					if '`' in value and not value.startswith('`'):
						value = value[value.index('`') - 1:].strip()

					# used for tracking line index without impacting any code that is checking `line_index`
					code_line_index: int = line_index

					# if the code is on a new line, check the next line
					# we do not try lines after the next one as it can be undeterministic and could
					# lead to mismatching
					if not value.startswith('`') and len(lines) > line_index + 1 and (next_line := lines[line_index + 1].strip()):

						if next_line.startswith('`'):

							code_line_index += 1
							value = next_line

					# if the line is code (starts with '`', and is not just '```'), read the lines after
					# until we reach the end of the code block
					if value.startswith('`') and (value.count('`') == len(value) or not value.endswith('`')):

						for _line in lines[code_line_index + 1:]:

							# if the line starts with a backtick, we've reached the end of the codeblock
							if _line.startswith('`'):
								break

							value = f'{value}\n{_line}'

							if _line.endswith('`'):
								break

					# convert the string value to `Code`
					# which comes with properties and used for validation later
					value = Code(value)

				if value is not None and (value and isinstance(value, str) and len(value.split()) <= 3 or (field and field.type == Field.TYPE_ENUM))  \
					and not (field and field.type in {Field.TYPE_STRING, Field.TYPE_CODE, Field.TYPE_INT, Field.TYPE_FLOAT}):

					# this allows calling `observation.category.is_security_vulnerability`
					# if the value is "Security Vulnerability", replace it with `security_vulnerability`
					value = Enum(str(value))

					if field:

						if enum_options := field.enum_options:

							has_matched_option: bool = False

							for enum_option in enum_options:

								if '|' not in enum_option:
									continue

								# e.g. `Attacker type<!type:enum!>: must be any of [person|physical *, app]`
								# if the attacker_type is `physical_person`, we'll treat it as `person`
								enum_sub_options: list[str] = enum_option.split('|')
								first_enum_sub_option: str = enum_sub_options[0]

								# if the value is in the list
								if value != first_enum_sub_option:

									# if the value is listed in one of the options as-is
									if value in enum_sub_options:

										value = Enum(first_enum_sub_option.replace('*', ''))
										has_matched_option = True

									else:

										# e.g. ['xnu', '*kernel']
										for enum_sub_option in enum_sub_options:

											if '*' not in enum_sub_option:
												continue

											if enum_sub_option.replace('*', '') in value:

												value = Enum(first_enum_sub_option.replace('*', ''))
												has_matched_option = True
												break

							# if none of the options match and the enum value is invalid, try matching the first part
							# e.g. value = `security_vulnerability. This is an excellent report` -> `security_vulnerability`
							if not has_matched_option:

								# check if the field value or raw value is one of the enum options
								# we sort by the length of options, so we match the right one
								# e.g. if `enum_options = ['yes', 'no', 'not_specified']`, and `value` is `no`
								#	   `no` should not match `not_specified`
								for enum_option in sorted(enum_options, key=lambda enum_option: len(enum_option.split('|')[0]), reverse=True):

									if value.startswith(enum_option) or value.raw.startswith(enum_option):

										# we do the `.raw[…]` because value is lowercase but we want to pass
										# the actual value from the template (same casing) to `Enum`
										# e.g. `template = [macOS, …]` (`enum_option = macOS`)
										#      `value = 'macos'`, `value.raw = `macOS`
										value = Enum(Enum.get_clean_string(string=value.raw)[:len(enum_option)])
										break

						# if the enum value we have is not valid, try parsing the raw literal value again
						# e.g. `value = 'I think the component is "Safari"' -> `Safari`
						#      `value = 'The component is likely to be "Safari"' -> `Safari`
						#      `value = 'The component appears to be the "IGAccelVideoContextMedia" in "AppleIntelKBLGraphics"' -> `IGAccelVideoContextMedia`
						#      `value = 'The component appears to be the "IGAccel::VideoContextMedia" in "AppleIntelKBLGraphics"' -> `IGAccel::VideoContextMedia`
						if not field.get_is_valid(value=value)[0]:

							# TODO: handle "Radar component: The vulnerable component in the report seems to be the "bundle ID check" function within replayd."
							# " The issue arises from a faulty bundle ID check that allows incorrect Broadcast Upload Extensions to be launched and"
							# "receive screen recording data. Therefore, the radar component should be "bundle ID check" in replayd."
							# currently says "bundle ID check"
							# TODO: `This report is classified as a security vulnerability`
							likely_matches: list[str] = re.findall(r'\b(?:is(?: the|)|should be|to be(?: the|)|likely in|as a) (?:"|\*\*)([A-Z0-9_:. ]+)(?:"|\*\*)', value.raw_literal, flags=re.IGNORECASE)

							for match in likely_matches:

								_value: Enum = Enum(match)

								# if the field has `enum_options` or `invalid_enum_options`, the value will be checked
								# until one that is good matches
								if field.get_is_valid(value=_value)[0]:

									value = _value
									break

				elif value is not None and field and field.type == Field.TYPE_BOOL and not isinstance(value, bool):

					# if the value is a list or dict, check if there's only one element, and take the element
					_value: Any = value

					if isinstance(value, list) and len(value) == 1:
						_value = value[0]

					elif isinstance(value, dict) and len(value) == 1:
						_value = list(value.values())[0]

					# if the value is a string, convert to bool
					if isinstance(_value, str):

						_value = _value.strip()
						_value_lower: str = _value

						if _value_lower.startswith(('false', 'no')) or _value_lower == '0':
							value = False

						elif _value_lower.startswith(('true', 'yes')) or _value_lower == '1':
							value = True

					elif isinstance(_value, bool):
						value = _value

					elif isinstance(_value, int) and (_value == 0 or _value == 1):
						value = bool(_value)

				elif value is not None and field and field.type in {Field.TYPE_INT, Field.TYPE_FLOAT}:

					# if the value is a list or dict, check if there's only one element, and take the element
					if isinstance(value, list) and len(value) == 1:
						value = value[0]

					elif isinstance(value, dict) and len(value) == 1:
						value = list(value.values())[0]

					# if the value is a string (e.g. `1` or `9. Because it is the lowest number`)
					if isinstance(value, str):

						value_split: list[str] = value.split()

						# if one of the values ends with a period, remove it
						# e.g. `9. Because 9 is the lowest number` -> `9.` -> `9`
						value_split = [value.removesuffix('.').removesuffix(',') for value in value_split]

						# if any of the values is a number with a string prefix
						# e.g. `2inch` -> `2`
						# e.g. `1.2inch` -> `1.2`
						value_split = [re.sub(r'^([\d,\.]+).*', r'\1', value) for value in value_split]

						# Remove commas (and any other non-numeric or non-period characters)
						value_split = [re.sub(r'(?<=\d)[^\d\.]+(?=\d)', '', value) for value in value_split]

						numeric_values: list[int|float|complex] = list({_value for _value in value_split if Utilities.get_is_numeric(value=_value)})

						# check if the first value is a number
						# e.g. `9 is the lowest number` -> `9`
						# if there's another number in the string, we want to avoid automatically doing anything
						# since the response could be referring to the second number as the correct answer
						# e.g. `10 is the highest, and 9 is the lowest`
						if len(numeric_values) == 1:

							value = numeric_values[0]
							value = float(value) if '.' in value else int(value)

				elif value is not None and field and field.type == Field.TYPE_JSON and not isinstance(value, (dict, list)):

					if (matches := re.findall(r'(?:\[|{).*?(?:\]|})', string, flags=re.IGNORECASE|re.MULTILINE|re.DOTALL)):

						try:
							value = Tool.get_dictionary_from_string(string=matches[0])

						except Exception as exception:
							logger.warning(f'response to {field.name} has an invalid JSON ({string=})', exc_info=True)

				dictionary_representation[key] = value

				# if the field has an alternative name (for easy usage in code), also add the alternative name
				if field and field.alternative_name:
					dictionary_representation[field.alternative_name] = value

		if nested:

			# this is helpful for cases where the LLM response is something like
			# Line 1: Importance: …
			# we're actually interested in `Importance:` not `Line 1:`, so nesting
			# allows us to get `line_1` and `importance` keys
			for value in list(dictionary_representation.values())[:]:

				if value and isinstance(value, str):

					_dictionary_representation: dict|list = cls.get_dictionary_from_string(string=value, template=template, nested=False)

					# merge only if the types match
					if type(_dictionary_representation) == type(dictionary_representation):

						if isinstance(_dictionary_representation, dict):
							dictionary_representation |= _dictionary_representation

						else:
							dictionary_representation += _dictionary_representation

			# if we're the top-level call, and we have no values
			# try to see the response is a different format
			raw_string: str = string.raw if isinstance(string, Response) else string

			# if we have a template, and the template has fields, be extra cautious and don't populate `options`
			# unless the dictionary_representation from the response is empty
			# while this is not actually needed, (we can just always set options, regardless), we do it
			# to prevent user confusion when calling `.response_as_dictionary` and they see `options` when
			# their template asks for other fields
			has_template_fields: bool = template and isinstance(template, Template) and template.fields

			# 1. numbered list
			if (not dictionary_representation or not has_template_fields) and raw_string:

				# e.g. 1. "option 1…"
				#      2. "option 2…" -> `['option 1…', 'option 2…']`
				numbered_lines: list[str] = [line.strip().strip('"') for line in re.findall(r'^(?:\W+|)[0-9]+\. (.*)$', raw_string, re.MULTILINE | re.IGNORECASE)]

				if numbered_lines:
					dictionary_representation['options'] = numbered_lines

			# 2. dot list
			if (not dictionary_representation or not has_template_fields) and raw_string:
				# e.g. * option 1…
				#      * option 2…
				# or:
				#	   - option 1…
				#	   - option 2…
				dot_lines: list[str] = [line.strip() for line in re.findall(r'^(?:\W+|)(?:\*|\-) (.*)$', raw_string, re.MULTILINE)]

				if dot_lines:
					dictionary_representation['options'] = dot_lines

			# 3. the entire response is yes/no, with no specified field name in the response
			# e.g. `template = 'Accurate<!type:bool!>: Yes/No'` but `response = `yes, …`
			# in this case, treat the `yes` as the value of `accurate`, so `observation.accurate == True`
			if not dictionary_representation and template:

				# look to see if there's only one _required_ bool field
				required_fields: list[Field] = [field for field in template.fields.values() if field.required]

				if len(required_fields) == 1 and (field := next((field for field in template.fields.values() if field.type == Field.TYPE_BOOL), None)):

					string_lower: str = string.lower()
					value: bool = None

					if string_lower.startswith(('true', 'yes')):
						value = True

					elif string_lower.startswith(('false', 'no')):
						value = False

					if value is not None:
						dictionary_representation[field.name if not field.alternative_name else field.alternative_name] = value

		# if we don't have a dictionary but the string has one, treat the string
		# as a dictionary. the length check is to prevent trying to parse invalid strings like just `[`
		if not dictionary_representation and len(string) > 1 and  \
			re.findall(r'(?:\[|{).*?(?:\]|})', string, flags=re.IGNORECASE|re.MULTILINE|re.DOTALL):

			try:
				dictionary_representation = Tool.get_dictionary_from_string(string=string)

			except Exception as exception:

				if not template or template.display_format == 'json':
					logger.warning(f'the response contains an invalid JSON: ({string=})')

			if dictionary_representation and template:

				# if we did get a JSON-only response but without a field name in the response,
				# and our template has only one `Field(type=Field.TYPE_JSON)`, assign the response to that field
				if len([field for field in template.fields.values() if field.type == Field.TYPE_JSON]) == 1:
					dictionary_representation = {next(field.name for field in template.fields.values() if field.type == Field.TYPE_JSON): dictionary_representation}

				elif isinstance(dictionary_representation, dict):

					# if we got a JSON-only response for a template (when the template does not ask for JSON)
					# match the keys in the JSON to the field names
					# e.g. response = '{"Key1": "1.5"}'
					# 	   template = 'key<type:float>: a number'
					# 	   return -> `{"key1": 1.5}`
					for field_name, field_value in list(dictionary_representation.items()):

						if (field_name_lower := field_name.lower()) and (field := template.fields.get(field_name_lower)):

							# if the field type is not a string, but the value is
							# pass it again to `get_dictionary_from_string` to convert it (if needed)
							if field.type not in {Field.TYPE_STRING, Field.TYPE_CODE} and isinstance(field_value, str):
								field_value = cls.get_dictionary_from_string(string=f'{field_name_lower}: {field_value}', template=template).get(field_name_lower, field_value)

							# replace the field name with the new one
							dictionary_representation[field_name_lower] = field_value

							if field_name != field_name_lower:
								del dictionary_representation[field_name]

		return dictionary_representation

	""" Realtime """

	async def interrupt(self) -> None:
		"""
		[Preview/API may change]
		Interrupts AI mid-responding

		Call this when the user begins speaking and while AI is responding
		"""

		if not self.events:
			return

		await self.ask(events=self.client.create_events(type=self.client.EVENT_RESPONSE_CANCEL))

	async def handle_event(self, event: Event) -> Self | None:
		"""
		[Internal]
		[Preview/API may change]
		Handles realtime events when using `AI.ask(…, realtime=True)`.

		Used internally and is not considered an API, but can
		be overridden for more custom flows
		"""

		if event.error:
			raise Exception(f'Could not ask AI in realtime: {event.error}')

		# stream text-based responses
		# or the transcript of audio responses
		elif event.type in {self.client.EVENT_TEXT_DELTA, self.client.EVENT_AUDIO_TRANSCRIPT_DELTA}:
			return Observation(response=event.text, event=event, tools=self.tools, call=self.call,
							   events=self.events, client=self.client)

		# stream audio responses (the bytes of each chunk)
		elif event.type == self.client.EVENT_AUDIO_DELTA:

			observation: Observation = Observation(response=event.text, tools=self.tools, event=event, call=self.call, messages=self.events,
												   client=self.client)
			observation.audio_bytes = event.audio_bytes
			return observation

		# AI would like to call a tool/function
		# we then return the response from the tool/function to AI
		elif event.type == self.client.EVENT_TOOL_CALL:

			for tool_call in event.tool_calls:

				response_status = None
				tool: Tool = next((tool for tool in self.tools if tool_call.name in {tool.name, tool.alternative_name}), None)

				# if the `tool_call` calls an invalid tool name (should never occur)
				if not tool:
					raise Exception(f'The model you are using hallucinated and called a tool that does not exist: {tool_call.name}')

				# TODO:
				# if tool_call_validator:
				# 	validator_response = tool_call_validator(tool_call=tool_call, events=self.events)
				# 	if isinstance(validator_response, str):
				# 		# Handle validator response as tool response in realtime mode
				# 		pass

				try:

					logger.section(f'calling {tool.alternative_name or tool.name}({tool_call.display_arguments})')
					tool_response = await asyncio.to_thread(tool.run, tool_call.arguments)

					if not isinstance(tool_response, ToolResponse):
						tool_response = ToolResponse(content=tool_response if tool_response is not None else 'Success')

				except MissingArguments as error:

					# if the call is missing an argument (or multiple), push-back with the argument names
					logger.warning(f'response is missing "{error.argument_names}" arguments. trying again…')
					response_status = f'Error: Try again with "{", ".join(error.argument_names)}"'

				content: Any = None

				if response_status:
					content = response_status

				else:

					content = tool_response.content

					# convert to dict, if the function returned a class
					if isinstance(content, BaseModel):
						content = content.model_dump()

					# convert to JSON, if the function returned a dict
					# this allows parsing the function/tool's response later from `.events`
					if isinstance(content, (dict, list, )):
						content = json.dumps(content, default=lambda obj: str(obj))

					else:
						content = str(content)

				# reply with the tool response
				for _event in self.client.create_events(type=self.client.EVENT_TOOL_RESPONSE, tool_call=tool_call, content=content):

					# queue the tool response, so `__anext__` returns it back to the caller later
					if not self._queued_events:
						self._queued_events = []

					self._queued_events.append(_event)
					await self.call.send(event=_event.raw)

			observation: Observation = Observation(response=event.text, tools=self.tools, event=event, call=self.call,
												   messages=self.events, client=self.client)
			return observation

		else:
			return Observation(tools=self.tools, event=event, call=self.call,
							   events=self.events, client=self.client)

	def __aiter__(self) -> 'Observation':
		return self

	async def __anext__(self) -> Event:

		event: Event = None

		if self._queued_events:
			event = self._queued_events.pop(0)

		else:
			event = await self.call.__anext__()

		if not event:
			raise StopAsyncIteration

		self.events.append(event)

		return await self.handle_event(event=event)

	def __getattr__(self, attr: Any) -> Any:
		"""
		Returns an property/function without throwing an exception

		This is needed to allow callers to get arbitrary values from the response
		without dealing with an exception if the LLM did not return the value

		e.g.
		response: `Do I need to use a tool? Yes`
		`observation.do_i_need_to_use_a_tool` (`True`)
		"""

		try:
			return object.__getattribute__(self, attr)

		except AttributeError as error:

			# IntelliJ has a bug that accesses these invalid attributes
			# to avoid extra logging, these attributes can be safely ignored
			if attr in {'_ipython_canary_method_should_not_exist_', '_ipython_display_', '__pydantic_validator__'} or attr.startswith('_repr_'):
				return

			logger.warning(f'no attribute: {attr}')

			if attr in {'response_as_dictionary', 'response_status', 'is_valid'}:
				raise error

			if self.response:
				return None

	def model_dump(self) -> dict[str, Any]:
		"""
		[Internal]
		Used by `@AI.track`

		Only includes relevant fields
		"""
		return {'response': str(self.response) if self.response else None,
				'template': self.template.text if self.template else None,
				'prompt': self.prompt, 'messages': self.messages}

	def __str__(self):
		return f'<Observation response="{self.response}">'

	__repr__ = __str__


""" AI """

class AI:
	"""
	Interact with AI.

	Use `AI.ask(…)` to get a response to your question/prompt. By default, Ajax (AFM) is used,
	but you can use OpenAI, Google AI, Anthropic and many open source models (like, Mixtral, Zephyr, OpenHermes, and more…)

	See [here](https://interlinked.apple.com/documentation) for full documentation.
	"""

	# a list of available AI clients
	CLIENTS: dict[str, type[BaseAIClient]] = {

		'mlxclient': MLXClient,
		'ajaxclient': AJAXClient,
		'endorclient': EndorClient,
		'openaiclient': OpenAIClient,
		'ollamaclient': OllamaClient,
		'googleaiclient': GoogleAIClient,
		'anthropicclient': AnthropicClient
	}

	# the default AI client used
	# it reads from the `INTERLINKED_AI_CLIENT_NAME` environment variable and defaults to `ajaxclient`
	client: BaseAIClient = CLIENTS[Config.current.AI_CLIENT_NAME](model_name=Config.current.AI_MODEL_NAME,
														 embedding_model_name=Config.current.AI_EMBEDDING_MODEL_NAME)

	@classmethod
	def ask(cls, prompt: str|dict[str, Any] = None, template: Template|str = None, tools: list[Tool|Callable] = None,
			messages: list[dict[str, str]] = None, files: list[str|bytes|File] = None, validator: Callable = None,
			max_iterations: int = 3, temperature: float = 0.6, dsid: int = None, client: BaseAIClient|None = None, realtime: bool = False,
			events: list[Event] = None, stream: bool = False, completion: bool = False, tool_call_validator: Callable = None) -> Observation | Iterator[Observation]:
		"""
		This is the entry point/main function for interacting with AI

		See https://github.pie.apple.com/Interlinked/interlinked/tree/main/examples for example usage.
		Or check out our Prompt Playground at https://interlinked.apple.com/playground.

		@param prompt(str|dict): This can be either a string (e.g. "Say hello world") or a dictionary.
								 You most likely want to pass a dictionary in real use-cases that contains a list of keys and values
								 which Interlinked will use to populate the `template`. e.g. `{"title": "iOS bug"}`
		@param template(str): This is a template string that contains your instructions
		@param tools(list): If you are looking to give AI access to call Python functions autonomously,
							pass a list of Python functions
		@param messages(list): Useful if you'd like to continue an existing conversation
		@param files(list): A list of paths (or bytes) to images / files
		@param validator(Callable): Pass a function that will be called by Interlinked, if you would
									like to run validation on the response. This will allow you to "push-back"
									on the response
		@param max_iterations(int): The maximum number of times Interlinked will "push-back" if the response doesn't
									match the expected format or criteria defined in `template`
		@param temperature(float): How creative the responses should be. Keep same or lower for more structured responses,
								   and higher for more creative responses
		@param dsid(int): (Internal) Used by the Interlinked Platform as it is required by the Ajax API
		@param realtime(bool): (Internal) Whether to use Realtime call with AI, like a phone call
		@param client(BaseAIClient): Pass any of the supported clients (e.g. `OpenAIClient` or `OllamaClient`) to use other models
		@param events(list): (Internal) Used to keep track of events while on a call with AI (when `realtime=True`).
							 Can be a list of `Event` class or a raw dictionary
		@param stream(bool): Whether the response should be streamed chunk by chunk. When set, this function will return an
							 iterator that yields an `Observation`, which has the word/chunk (e.g. `observation.response`)
		@param completion(bool): Useful for code completions. Returns completion instead of chat response
		@param tool_call_validator(Callable): (Optional) Pass a function that will be called to confirm before AI calls a tool/function
		"""

		# make sure we have at least a prompt or template
		if not prompt and not template and not messages and not realtime:
			raise UserError('Neither `prompt` and `template` is set. Did you mean to set either or both?')

		# use the default AI client if no custom client is passed
		client = client or cls.client

		observation: Observation = Observation(messages=messages, tools=tools, client=client, events=events)
		return observation.ask(prompt=prompt, template=template, files=files,
							   validator=validator, max_iterations=max_iterations,
							   temperature=temperature, dsid=dsid, stream=stream, realtime=realtime,
							   completion=completion, tool_call_validator=tool_call_validator)

	@classmethod
	def learn(cls, from_: str|Section, knowledge_source_id: str|int = None, metadata: dict[str, Any] = None, fine_tune: bool|dict[str, Any] = False,
			  client: Any = None, store_client: BaseVectorStoreClient = None, knowledge_class: Knowledge = None, options: dict[str, Any] = None,
			  **kwargs) -> list[Knowledge]|str:
		"""
		AI can learn from given a knowledge source (e.g. text file, Quip, GitHub, etc)

		Once you call this function, you can use `Knowledge.search(query='how to…')`, which returns a list of knowledge learned.
		Each knowledge has a `.content` property that contains the exact snippet of text that
		is similar to your search query. This unlocks many powerful use-cases, such as searching across thousands of documents
		using plain language, finding similar radars, logs, events, and more.

		@param from_(str|Section): The text content to learn from (e.g. documentation), or a `Section`
		@param knowledge_source_id(str|int): (optional) If running in a production environment, you most likely want to set this
											 This is used to assosciate this learned Knowledge with a source (e.g. the ID of a Quip doc)
		@param metadata(dict): (optional) If you'd like to store any additional data with the Knowledge (e.g. `{'created_at': '2024-03-19…'}`)
		@param fine_tune(bool|dict): (internal) (optional) If you'd like to fine-tune the model. This can also be a dictionary of hyperparameters
									 to pass to the AI client's `fine_tune` function
		@param client(Any): (optional) Any AI client (e.g. `OpenAIClient`, `OllamaClient`, or `AJAXClient`)
		@param store_client(Any): (optional) a custom `QdrantClient`, `ElasticSearchClient`, or `MilvusClient`
		@param knowledge_class(Any): (optional) If you'd like to store the knowledge learned in a collection/table with a different name,
									 pass a custom class that inherits `Knowledge`
		@param options(dict): (optional) Pass custom arguments to functions `AI.learn(…)` calls into (e.g. `{'get_all': {'split_by': 'character'}}`)
		@return (list|str): a list of `Knowledge` that represents the file/document you passed in `from_`. The document gets split
							into chunks/`Knowledge`
		"""

		# use Qdrant as default
		if not store_client:
			store_client = QdrantClient.shared

		# default to `Knowledge` class
		if not knowledge_class:
			knowledge_class = Knowledge

		if fine_tune and not isinstance(from_, (str, list, Section)):
			raise UserError('To fine-tune, please set `from_` to a JSONL file or a list of dictionaries')

		# cast to `Section` if `from_` is a string and whether it's a path
		if isinstance(from_, str):

			# if this is a path, auto-read the file and convert it into `Section`
			# this is done for user convenience but does mean that we need to perform a list
			# of checks to make sure that this is actually a path
			if from_.startswith(('~/', '/', './')) and '\n' not in from_:

				path: str = os.path.realpath(os.path.expanduser(from_))

				# auto-detect file type
				file_extension: str = path.rsplit('.')[-1] if '.' in path else None

				if not os.path.exists(path):

					if file_extension and ' ' not in file_extension:
						raise UserError(f'"{path}" does not exist.')

				else:

					# only some file types are supported out-of-the-box
					if file_extension not in {'md', 'rst', 'txt', 'jsonl'}:
						raise UserError(f'Unsupported file type. If you want to parse it as raw text, use `open(\'{path}\', \'r\').read()`')

					# convert `from_` to the contents of the file
					with open(path, 'r') as file:
						from_ = file.read()

					if file_extension == 'md':
						from_ = Section.from_markdown(content=from_)

					elif file_extension == 'rst':
						from_ = Section.from_restructured(content=from_)

					elif file_extension == 'txt':
						logger.info('Use structured formats, such as Markdown (.md), ReStructured (.rst), and more for best results')

					elif file_extension == 'jsonl' and not fine_tune:
						raise UserError('Please pass `AI.learn(…, fine_tune=True)`')

			if not isinstance(from_, Section):
				from_ = Section(content=[from_])

		elif isinstance(from_, bytes):
			from_ = Section(content=[from_])

		# `AI.learn(…)` expects certain types
		elif not isinstance(from_, Section) and not fine_tune:
			raise UserError('`AI.learn(from_=…)` expects a file path, text, bytes, or `Section`')

		# if we're fine-tuning
		if fine_tune:

			lines: list[str] = []
			client = client or AI.client
			example_sections: list[Section] = []

			if not hasattr(client, 'fine_tune'):
				raise UserError(f'{client.__class__.__name__} does not support fine-tuning. Please switch to another AI client')

			logger.info('ℹ️ fine-tuning is in beta. For most use-cases, it is recommended to use `AI.learn(…)` without `fine_tune=True`')

			if isinstance(from_, Section) and not from_.is_example:

				logger.info(f'creating example questions/answers… (title={from_.title=!r})')
				example_sections = from_.get_examples(client=client)

			elif isinstance(from_, list):

				if not from_:
					raise UserError('The `from_=…` argument has an empty list')

				for _from_ in from_:

					# if `from_` is a list of `Section`, check if
					# the sections were created by `.get_examples`
					if isinstance(_from_, Section):

						if _from_.is_example:
							example_sections.append(_from_)

						else:

							logger.info(f'creating example questions/answers… (title={_from_.title=!r})')
							example_sections += _from_.get_examples(client=client)

					# if it's already in a `.jsonl` format
					elif isinstance(_from_, dict):
						lines.append(_from_)

					else:
						raise UserError('The `from_=…` argument can be a list of Section or dictionaries')

			if isinstance(fine_tune, dict):
				options: dict[str, Any] = fine_tune
			else:
				options: dict[str, Any] = {}

			if not lines:

				# convert sections to a question/response format
				# that matches the AI client's format
				for example_section in example_sections:

					messages: list[dict] = []
					messages.append(client.create_message(role=client.ROLE_USER, content=example_section.title))
					messages.append(client.create_message(role=client.ROLE_ASSISTANT, content=' '.join(example_section.content)))
					lines.append({'messages': messages})

			# ensure that we have enough data to fine-tune on
			if len(lines) < 20:
				raise UserError(f'Additional data is needed to fine-tune (at least 20 questions are needed, but got {len(lines)})')

			return client.fine_tune(data=lines, options=options)

		# if the section is fully empty
		if not from_.title and from_.is_content_empty and not from_.subsections:

			logger.warning('the knowledge is empty. skipping learning…')
			return []

		knowledges: list[Knowledge] = []
		sections: list[Section] = from_.get_all(**(options or {}).get('get_all', {}))

		for section in sections:

			# skip sections that have no content as they'd not be very relevant
			# when looking up knowledge
			# if they have subsections, the titles of the skipped sections would be included
			# so, there's no context loss
			if section.is_content_empty:

				logger.warning(f'skipping section with no content… ({section.title=!r})')
				continue

			knowledge, is_new = knowledge_class.get_or_create(content=section.full_content, external_id=section.external_id,
															  metadata=metadata, knowledge_source_id=knowledge_source_id,
															  store_client=store_client, client=client)

			if is_new:
				logger.info(f'created knowledge for section ({section.title=!r})')

			knowledges.append(knowledge)

		return knowledges

	""" Utilities """

	@classmethod
	def get_embedding(cls, input: str | list[str], client: BaseAIClient = None) -> list[float] | list[list[float]]:
		"""
		Returns an embedding for a given input string or strings.
		You most likely want to use `AI.learn(…)` instead

		@param input(str): The text/string you would like to generate embedding for
		@param client(BaseAIClient): (optional) Any AI client (e.g. `OpenAIClient`, `OllamaClient`, or `AJAXClient`)
		"""

		if not client:
			client = cls.client

		return client.get_embedding(input=input)

	@classmethod
	def track(cls, function: callable = None, external_id: str|int = None,
			  enabled: bool = Config.current.is_development, callback: Callable = None):
		"""
		Decorator that allows tracking all of the steps AI took
		in your code to help you debug, observe results, demo your work, and more

		It can be used via:
		```
		from interlinked import AI

		@AI.track
		def run():
			AI.ask(prompt='hi')
		```

		Then view live output in Playground by running `interlinked` command, then
		navigating to localhost:8000/playground

		@param external_id(str|int): (optional) an ID of your choice, used to link any future call to AI
									 to the same Steps. Useful for debugging chat interfaces
		@param enabled(bool): (optional) conditionally control whether to enable this decorator
							  by default, it is turned off when running in a production environment
		"""

		if not callback:

			# data storage
			import sqlite3
			from pathlib import Path

			connection: Any = sqlite3.connect(database=f'{Path.home()}/.interlinked-database.db', check_same_thread=False)
			cursor: sqlite3.Cursor = connection.cursor()

			cursor.execute('''
				CREATE TABLE IF NOT EXISTS run(
					id INTEGER,
					pid INTEGER,
					file_path TEXT,
					class_name TEXT,
					external_id TEXT,
					function_name TEXT,
					created_at DATETIME,
					finished_at DATETIME,
					PRIMARY KEY ("id" AUTOINCREMENT)
			)''')
			cursor.execute('''
				CREATE TABLE IF NOT EXISTS step(
					id INTEGER,
					stack JSON,
					kwargs JSON,
					level INTEGER,
					run_id INTEGER,
					class_name TEXT,
					return_value JSON,
					function_name TEXT,
					created_at DATETIME,
					finished_at DATETIME,
					PRIMARY KEY ("id" AUTOINCREMENT),
					FOREIGN KEY (run_id) REFERENCES run (id)
			)''')
			cursor.execute('''
				CREATE TABLE IF NOT EXISTS log(
					content TEXT,
					step_id INTEGER,
					created_at DATETIME,
					id INTEGER PRIMARY KEY AUTOINCREMENT,
					FOREIGN KEY (step_id) REFERENCES step (id)
			)''')

			cursor.execute('PRAGMA foreign_keys = ON')
			connection.commit()

			# migration 1: add stack to step
			cursor.execute(f'PRAGMA table_info(step)')

			if 'stack' not in [row[1] for row in cursor.fetchall()]:

				cursor.execute(f'ALTER TABLE step ADD COLUMN stack TEXT')
				connection.commit()

			def callback(event_type: int, **kwargs) -> Any | None:

				if event_type == Steps.EVENT_NEW_RUN:

					cursor.execute('INSERT INTO run (file_path, pid, created_at, function_name, class_name, external_id) VALUES (?, ?, ?, ?, ?, ?)',
									(kwargs['caller_file_path'], kwargs['pid'], kwargs['time'].isoformat(), kwargs['function_name'],
									 kwargs['class_name'], kwargs['external_id']))
					connection.commit()

					return cursor.lastrowid

				elif event_type == Steps.EVENT_GET_RUN_ID:

					if run := cursor.execute('SELECT id FROM run WHERE external_id = ?', (kwargs['external_id'], )).fetchone():
						return run[0]

					return None

				elif event_type in {Steps.EVENT_NEW_STEP, Steps.EVENT_EXCEPTION}:

					step_kwargs: dict[str, Any] = kwargs['step_kwargs']

					cursor.execute('INSERT INTO step (class_name, function_name, kwargs, stack, level, run_id, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)',
								   (kwargs['class_name'], kwargs['function_name'], json.dumps(step_kwargs, default=lambda object: str(object)) if step_kwargs else None,
									json.dumps(kwargs['stack']), kwargs['level'], kwargs['run_id'], kwargs['time'].isoformat(), ))
					connection.commit()

					return cursor.lastrowid

				elif event_type == Steps.EVENT_ROOT_CALLER:

					cursor.execute('UPDATE run SET function_name=(?) WHERE id=(?)',
								   (kwargs['function_name'], kwargs['run_id'], ))
					connection.commit()

				elif event_type == Steps.EVENT_FINISHED:

					cursor.execute('UPDATE run SET finished_at=(?) WHERE id=(?)',
								   (kwargs['time'].isoformat(), kwargs['run_id'], ))
					connection.commit()

				elif event_type == Steps.EVENT_STEP_RETURN:

					return_value: Any = kwargs['return_value']

					cursor.execute('UPDATE step SET return_value=(?), finished_at=(?) WHERE id=(?)',
								   (json.dumps(return_value, default=lambda object: str(object)) if return_value else None, kwargs['time'].isoformat(), kwargs['step_id'], ))
					connection.commit()

				elif event_type == Steps.EVENT_LOG:

					try:
						cursor.execute('INSERT INTO log (content, step_id, created_at) VALUES (?, ?, ?)', (kwargs['content'], kwargs['step_id'], kwargs['time'].isoformat(), ))
						connection.commit()

					except sqlite3.IntegrityError:

						# there was a bug previously, where log.id did not have an autoincrement primary key
						logger.error('⚠️ There was an issue storing `@AI.track` logs in the database. Please run `rm -rf ~/.interlinked-database.db` then try again')

		return Steps.track(function=function, external_id=external_id, enabled=enabled,
						   callback=callback)


""" Tracking steps """

class Steps(logging.Handler):
	"""
	Used for tracking all Steps that AI takes when you use `AI.ask(…)`.
	You can view the results either in Playground or accessing the
	results directly via code
	"""

	# marks the beginning of a conversation with AI.
	# the very first time `AI.ask(…)` is called
	EVENT_NEW_RUN: int = 1

	# similar to `EVENT_NEW_RUN`, but contains a missing
	# dictionary with the name of the function that begain
	# the conversation
	# you likely do not need to use this Step
	EVENT_ROOT_CALLER: int = 2

	# when any of the Interlinked APIs are called
	# or functions/Tools in your code
	# e.g. `AI.ask(…)`, `AI.learn(…)`
	EVENT_NEW_STEP: int = 3

	# the value the function from `EVENT_NEW_STEP` returned
	EVENT_STEP_RETURN: int = 4

	# when any `print` or log is made from the current Step
	EVENT_LOG: int = 5

	# if any exception is raised from the current Step
	EVENT_EXCEPTION: int = 6

	# when the code finishes running
	EVENT_FINISHED: int = 7

	# [internal] used in web apps, when `@AI.track(external_id=…)`
	# is set, to differentiate between conversations with AI
	EVENT_GET_RUN_ID: int = 8

	# [internal] same as `EVENT_NEW_STEP` but called for
	# every internal function Python calls (may be called thousands of times)
	EVENT_NEW_INTERNAL_STEP: int = 9

	# [internal]
	level: int = None
	tools: list[Tool] = []
	callback: Callable = None
	old_trace: Callable = None
	previous_exception: dict[str, Any] = {}

	# [internal] this tracks any function, including ones that callback does not get called on
	root_caller: dict[str, Any] = None

	# [internal] all steps that we are interested in tracking
	steps: list[dict[str, Any]] = []

	# [internal] needed in `__init__` since we override `stdout`
	isatty: Callable = lambda self: False

	# [internal] used for tracking all internal Steps
	TRACK_INTERNAL_STEPS: bool = False

	def __init__(self, callback: Callable):

		self.old_trace = None
		self.callback = callback

		self.level = -1
		self.tools = []

		self.old_trace = None
		self.old_stdout = None

		super(Steps, self).__init__()

	@classmethod
	def track(cls, function: Callable, external_id: str|int, enabled: bool,
			  callback: Callable):
		"""
		[Internal]
		Decorator used to track all calls to `AI.ask(…)`

		Use `@AI.track` instead
		"""

		from functools import wraps

		if enabled:

			import json
			import traceback
			from datetime import datetime, timezone

			def decorator(inner_function: Callable):

				@wraps(inner_function)
				def wrapper(*args, **kwargs):
					"""
					Note: this class does not support `print` or `logger.info`
					"""

					nonlocal external_id

					# get the name of the file calling us
					stack = inspect.stack()
					caller_frame = stack[1]
					caller_file_path: str = caller_frame.filename

					run_id: int = None
					last_exception: dict[str, Any] = None

					# e.g. `{'class_name:function_name': 123}`
					step_step_id_map: dict[str, int] = {}

					# if `external_id` is set, find the related `Run`
					if external_id is not None:

						if (run_id := callback(event_type=cls.EVENT_GET_RUN_ID, external_id=external_id)) and run_id is not None:
							run_id = run_id

					def _callback(event_type: int, **kwargs):
						"""
						Called when a new event a new event is posted
						"""

						nonlocal run_id
						nonlocal last_exception
						nonlocal step_step_id_map

						if event_type in {cls.EVENT_NEW_STEP, cls.EVENT_EXCEPTION}:

							level: str = kwargs['level']
							stack: list[str] = kwargs['stack']
							class_name: str = kwargs['class_name']
							function_name: str = kwargs['function_name']
							step_kwargs: dict[str, Any] = kwargs['kwargs']

							if event_type == cls.EVENT_EXCEPTION and last_exception and  \
							   class_name == last_exception['class_name'] and str(step_kwargs) == str(last_exception['kwargs']):
								return

							if not run_id:

								# if we have a run with the same pid, reuse it
								run_id = callback(event_type=cls.EVENT_NEW_RUN, caller_file_path=caller_file_path,
												  pid=os.getpid(), time=datetime.now(timezone.utc), function_name=function_name,
												  class_name=class_name, external_id=external_id)

								if run_id is None:
									raise UserError('Your @AI.track callback should have returned an ID that it can use to track AI\'s steps')

							# remove self and cls
							if step_kwargs:

								step_kwargs.pop('cls', None)
								step_kwargs.pop('self', None)

							# remove this function from the stack
							# e.g. `File "…/interlinked/core/ai.py", line 4234, in wrapper`
    						# 	   `_callback(event_type=cls.EVENT_EXCEPTION,…`
							while stack and '/ai.py' in stack[-1] and 'in _' in stack[-1]:
								stack.pop()

							stack = [frame.strip() if frame.startswith('  File') else frame for frame in stack]

							step_id: int = callback(event_type=event_type, class_name=class_name, function_name=function_name, stack=stack,
													step_kwargs=step_kwargs, level=level, run_id=run_id, time=datetime.now(timezone.utc))

							# cache the step ID, so we can populate its return value later
							step_step_id_map[f'{class_name}:{function_name}'] = step_id

							if event_type == cls.EVENT_EXCEPTION:
								last_exception = {'class_name': class_name, 'kwargs': step_kwargs}

						elif event_type == cls.EVENT_STEP_RETURN:

							class_name: Any = kwargs['class_name']
							return_value: Any = kwargs['return_value']
							function_name: Any = kwargs['function_name']

							if return_value:

								if isinstance(return_value, Observation):

									return_value = {'response': return_value.response.raw,
													'response_as_dictionary': return_value.response_as_dictionary,
													'prompt': return_value.prompt}

								elif isinstance(return_value, BaseModel):
									return_value = return_value.model_dump(exclude_none=True)

							# get the step ID
							callback(event_type=cls.EVENT_STEP_RETURN, return_value=return_value, time=datetime.now(timezone.utc),
									 step_id=step_step_id_map.pop(f'{class_name}:{function_name}'))

						elif event_type == cls.EVENT_ROOT_CALLER:

							class_name: str = kwargs['class_name']
							function_name: str = kwargs['function_name']

							callback(event_type=event_type, class_name=class_name, function_name=function_name,
									 run_id=run_id)

						elif event_type == cls.EVENT_LOG:

							# attach the log to the most recent step, since it is required
							step_id: int = list(step_step_id_map.values())[-1] if step_step_id_map else None

							if step_id != None:

								callback(event_type=event_type, content=kwargs['content'], level_name=kwargs['level_name'],
										 level_number=kwargs['level_number'], step_id=step_id,
										 time=datetime.now(timezone.utc))

						elif event_type == cls.EVENT_FINISHED:
							callback(event_type=event_type, time=datetime.now(timezone.utc), run_id=run_id)

						# [internal] can be used to stop the code while running
						# used for stopping Runs at any time
						elif event_type == cls.EVENT_NEW_INTERNAL_STEP:

							if cls.TRACK_INTERNAL_STEPS:
								callback(event_type=event_type, function_name=kwargs['function_name'], run_id=run_id)

						else:
							raise NotImplementedError(event_type)

					try:
						with cls(callback=_callback):
							return inner_function(*args, **kwargs)

					except Exception as exception:

						stack: list[str] = traceback.format_exception(type(exception), exception, exception.__traceback__)
						_callback(event_type=cls.EVENT_EXCEPTION, class_name=exception.__class__.__name__, function_name='_exception_',
								  kwargs={'value': str(exception)}, stack=stack, level=0)

						raise exception

				return wrapper

		else:

			def decorator(inner_function: Callable):

				@wraps(inner_function)
				def wrapper(*args, **kwargs):
					return inner_function(*args, **kwargs)

				return wrapper

		# `@AI.track` without arguments
		if function and callable(function):
			return decorator(function)

		# `@AI.track` without arguments
		return decorator

	def _track(self, frame: Any, event: str, arg: Any) -> Callable:
		"""
		[Internal]
		Called every time a function is called, used to track AI functions
		"""

		import traceback

		if event in {'call', 'return', 'exception'}:

			code = frame.f_code
			function_name: str = code.co_name

			# ignore write() calls from print statements
			if code.co_name in {'write', 'emit'}:
				return

			caller_frame = frame.f_back
			kwargs: list[str] = frame.f_locals

			class_name: str = kwargs.get('self', None).__class__.__name__ if 'self' in kwargs else None

			if not class_name and (_cls := kwargs.get('cls')):
				class_name = getattr(_cls, '__name__', None)

			is_ai_client: bool = issubclass(kwargs.get('self', None).__class__, BaseAIClient) if 'self' in kwargs else None

			should_track: bool = class_name == 'AI' or (is_ai_client and function_name in {'get_response', 'get_embedding', 'start_call'}) or  \
								 (class_name == 'Knowledge' and function_name == 'search') or  \
								 class_name == 'Observation' and function_name == 'ask'

			# `AnthropicClient` has a special case, where it calls into `OpenAIClient` behind-the-scenes
			# to avoid showing both in Steps, we will skip `OpenAIClient`
			if class_name == 'OpenAIClient' and self.steps and (previous_step := self.steps[-1]) and previous_step['class_name'] == 'AnthropicClient':
				should_track = False

			# check if it is a `Tool` (from `AI.ask(…, tools=[…])`)
			# tools don't currently have class names
			# this check prevents accidentally tracking functions, such as an Python-internal `Request.send`,
			# which is similar to `Mail.send`
			if not should_track and not class_name:

				tool_names: set[str] = set()

				for tool in self.tools:

					if isinstance(tool, Tool):
						tool_names.add(tool.name)

					# skip bundle IDs, since they are not function names
					elif isinstance(tool, str) and tool.startswith('com.'):
						continue

					else:
						tool_names.add(tool.__name__)

					# tools can have an alternative name
					if alternative_name := getattr(tool, 'alternative_name', None):
						tool_names.add(alternative_name)

				should_track = function_name in tool_names

			if event == 'call':

				if function_name == 'ask':

					tools: list[Tool] = kwargs.get('tools') or []
					self.tools += list(set(tools))

				if class_name == 'AI' and function_name == 'ask' and not self.root_caller:

					self.root_caller = {'function_name': caller_frame.f_code.co_name}
					self.callback(event_type=self.EVENT_ROOT_CALLER, class_name=None, function_name=caller_frame.f_code.co_name)

				# ignore `Observation.ask(…)` if it's from `AI.ask(…)`
				if class_name == 'Observation' and function_name == 'ask' and self.steps and (previous_step := self.steps[-1]):

					if previous_step['class_name'] == 'AI' and previous_step['function_name'] == 'ask':
						return

				# ignore `AI.get_embedding(…)` if it's from `AI.learn(…)` or `Knowledge.search(…)`
				# TODO: support `Knowledge` subclasses
				if class_name == 'AI' and function_name == 'get_embedding' and self.steps and (previous_step := self.steps[-1]):

					if (previous_step['class_name'] == 'AI' and previous_step['function_name'] == 'learn') or  \
					   (previous_step['class_name'] == 'Knowledge' and previous_step['function_name'] == 'search'):
						return

				if should_track:

					self.level += 1
					self.steps.append({'class_name': class_name, 'function_name': function_name})

					stack: list[str] = traceback.format_stack()
					self.callback(event_type=self.EVENT_NEW_STEP, class_name=class_name, function_name=function_name,
								  kwargs=kwargs.copy(), stack=stack, level=self.level)

				else:
					self.callback(event_type=self.EVENT_NEW_INTERNAL_STEP, function_name=function_name)

			elif event == 'exception':

				exc_type, exc_value, _ = arg

				if exc_type.__name__ == 'Pause':

					stack: list[str] = traceback.format_stack()
					self.callback(event_type=self.EVENT_EXCEPTION, class_name=exc_type.__name__, function_name='_exception_',
								  kwargs={'value': str(exc_value)}, stack=stack, level=self.level)

			else:

				if function_name == 'ask' and self.steps and (previous_step := self.steps[-1]):

					if previous_step['class_name'] == 'AI' and previous_step['function_name'] == 'ask':
						return

				# track any Tool from Applets that we missed earlier
				# because `.get_tools()` gets usually called after `AI.ask(…)`
				if class_name == 'Applet' and function_name == 'get_tools':
					self.tools += arg

				if should_track:

					self.callback(event_type=self.EVENT_STEP_RETURN, class_name=class_name, function_name=function_name,
								  level=self.level, return_value=arg)
					self.level -= 1

			return self._track

	def handle_print(self, content: str, level_name: str = None, level_number: int = None):
		"""
		[Internal]
		"""

		# capture logs, if they were not made by the `Steps` class (to prevent infinite recursion)
		if content.strip() and not any(frame.function == self.callback.__name__ for frame in inspect.stack()):
			self.callback(event_type=self.EVENT_LOG, content=content, level_name=level_name, level_number=level_number)

	def __enter__(self):

		self.old_stdout = sys.stdout
		sys.stdout = self

		# capture logging
		logging.root.addHandler(self)

		self.old_trace = sys.gettrace()
		sys.settrace(self._track)
		return self

	def __exit__(self, exc_type, exc_val, exc_tb):

		# restore the original stdout
		sys.settrace(self.old_trace)
		sys.stdout = self.old_stdout

		# remove our logging handler
		logging.root.removeHandler(self)

		self.callback(event_type=self.EVENT_FINISHED)

	""" `print` override """

	def write(self, message: str|Any) -> None:
		"""
		[Internal]
		"""

		self.old_stdout.write(message)
		self.handle_print(content=message)

	def flush(self) -> None:
		"""
		[Internal]
		"""
		self.old_stdout.flush()

	def emit(self, record):
		"""
		[Internal]
		"""
		self.handle_print(content=self.format(record), level_name=record.levelname, level_number=record.levelno)


""" Exceptions """

class Pause(Exception):
	"""
	Use this exception inside your `tool_call_validator` that you pass
	to `AI.ask(…, tools=[my_function], tool_call_validator=my_validator)` to pause before AI
	calls your functions, and allows you to resume later.

	## Example use-case
	You have a `send_email` function and want to get the user's approval before AI calls it.
	In your `tool_call_validator` function, raise this exception (e.g. `raise Pause(messages=messages)`),
	catch the exception, ask the user for approval, then resume by passing messages back to
	`observation = Observation(messages=messages)`, then do `observation.ask(tools=tools)`

	You can pass any arguments to this exception and access them when you catch it.
	"""

	def __init__(self, **kwargs):
		"""
		Initializes a Pause via `raise Pause(…)`

		This accepts arbitrary arguments to allow you to pass custom variables
		and maintain the state of your code.

		The recommended arguments is `raise Pause(messages=messagaes)`, which is given to you automatically as an
		argument in `tool_call_validator`
		"""

		for key in kwargs.keys():
			setattr(self, key, kwargs[key])


class UserError(Exception):
	"""
	[Internal]
	Raised by Interlinked if the user made an error calling an API
	"""

	def __str__(self):
		return f'⚠️ {super().__str__().strip(".")}. If you have questions, we are here to help in #help-interlinked'
