import os
import re
import difflib
import logging
from pydantic import BaseModel
from typing import Any, Optional

from interlinked.core.tool import Tool
from interlinked.core.utilities import Utilities
from interlinked.core.workflows.agent.tools import *
from interlinked.core.dynamic.functions import Function
from interlinked.core.workflows.agent.templates import *
from interlinked.core.clients.ajaxclient import AJAXClient
from interlinked.core.ai import AI, Observation, Template, Response, Pause


class AgentNonUI:
	"""
	This class is the same as the one in `workflow.py` but does not require any
	UI dependency
	"""

	@classmethod
	def handle_nonui_message(cls, prompt: str = None, messages: list[dict] = [], client: 'BaseAIClient' = None,
							 tools: list[Tool] = None, knowledge_source_ids: list[int|str] = None, user: 'User' = None) -> tuple[Observation, list]:
		"""
		Handles (and replies to) a message from the UI
		"""

		used_knowledges: list[Any] = []
		response_statuses: list[Any] = []

		def _lookup_knowledge_callback(knowledges: list[Knowledge], response_status: str):
			"""
			Inner callback function for `lookup_knowledge`
			"""

			if knowledges:
				used_knowledges.extend(knowledges)

			if response_status:
				response_statuses.append(response_status)

		def _tool_call_validator(tool_call: 'ToolCall', messages: list[dict, Any], **kwargs):

			if tool_call.name in {'send_email', 'file_radar'}:
				raise Pause(messages=messages)

		# pass messages and add tools
		# only add tools if we already have a conversation or the first message longer than one word (e.g. `hi`)
		if messages or (prompt and len(prompt.strip().split()) > 1):

			tools += [

				# file_radar,
				# lookup_person,
				# Tool(function=send_email, function_kwargs={'from_address': user.email if user else 'interlinked@apple.com'}),
				Tool(function=lookup_knowledge, function_kwargs={'prompt': prompt, 'knowledge_source_ids': knowledge_source_ids,
																 'callback': _lookup_knowledge_callback, 'client': client, 'limit': 4}),
			]

		# add system message
		if not messages:

			template: Template = Template(text=UI_TEMPLATE)
			messages = [(client or AI.client).create_message(role=(client or AI.client).ROLE_SYSTEM,
															 content=template.get_populated_text(prompt={'first_name': user.first_name if user else 'User'}))]

		# ask AI
		if prompt:

			observation: Observation = AI.ask(prompt=prompt, tools=tools, messages=messages, client=client,
											  tool_call_validator=_tool_call_validator)

		else:

			observation: Observation = Observation(messages=messages, client=client)
			observation = observation.ask(tools=tools)

		if observation:

			# reduce verbosity of response (helpful for less-capable models)
			# currently designed to trim the last line in the response, if it's too similar to previous lines
			if (response_lines := observation.response.raw.split('\n')) and len(response_lines) > 2:

				# tokenize and normalize words from the last line and the rest of the text
				previous_lines_chunks: set[str] = {re.sub(r'\W+', '', word.lower()) for word in ' '.join(response_lines[:-1]).split()}
				last_line_chunks: set[str] = {re.sub(r'\W+', '', word.lower()) for word in response_lines[-1].split()}

				intersected_chunks: set[str] = last_line_chunks.intersection(previous_lines_chunks)

				if intersected_chunks:

					similarity: float = (len(intersected_chunks) / len(last_line_chunks)) * 100

					# if it's too similar to the overall string
					if similarity > 0.65:
						observation.response = Response(string=observation.response, raw='\n'.join(response_lines[:-1]).strip())

			observation.response = Response(string=observation.response, raw=observation.response.raw.split('Is there anything else')[0])

		return observation, used_knowledges
