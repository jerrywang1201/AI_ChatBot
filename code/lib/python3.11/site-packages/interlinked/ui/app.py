import io
import os
import re
import sys
import uuid
import json
import shutil
import base64
import asyncio
import logging
import pathlib
import tempfile
import requests
import importlib
import mimetypes
from typing import Callable
from sqlmodel import select, Session
from sqlalchemy.sql import or_, and_
from sqlalchemy.orm import aliased, defer
from contextlib import asynccontextmanager
from typing import Any, Optional, Annotated
from fastapi.encoders import jsonable_encoder
from starlette.staticfiles import StaticFiles
from dateutil.relativedelta import relativedelta
from starlette.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from datetime import datetime, timedelta, timezone
from sqlalchemy.orm.attributes import flag_modified
from apple_oidc import AppleConnectOIDC, ASGIMiddleware
from sqlalchemy.orm import aliased, joinedload, selectinload
from interlinked.core.clients.quipclient import QuipClient, Blob
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy import create_engine, Column, Integer, String, func, func, delete, desc
from qdrant_client.models import Filter, Field, FieldCondition, MatchText, MatchValue, MatchAny
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse, Response, StreamingResponse, RedirectResponse
from fastapi import FastAPI, BackgroundTasks, Form, Request, Path, Header, Query, Body, HTTPException, Depends, UploadFile, File as FastAPIFile

# import Config early so we can use it for logging setup
from interlinked.core.config import Config

# configure file logging for Splunk early (before other loggers are created)
# Force logging setup if /logs directory exists (Kubernetes environment)
if os.path.exists('/logs'):

	os.makedirs('/logs', exist_ok=True)

	# get the root logger and add a file handler
	root_logger = logging.getLogger()

	if not any(isinstance(handler, logging.FileHandler) for handler in root_logger.handlers):

		file_handler = logging.FileHandler('/logs/app.log')
		file_handler.setLevel(logging.INFO)

		formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
		file_handler.setFormatter(formatter)
		root_logger.addHandler(file_handler)
		root_logger.setLevel(logging.INFO)
		
		# also add file handler to the 'interlinked' logger since it has propagate=False
		interlinked_logger = logging.getLogger('interlinked')

		if not any(isinstance(handler, logging.FileHandler) for handler in interlinked_logger.handlers):

			interlinked_file_handler = logging.FileHandler('/logs/app.log')
			interlinked_file_handler.setLevel(logging.INFO)
			interlinked_file_handler.setFormatter(formatter)
			interlinked_logger.addHandler(interlinked_file_handler)

from interlinked.core.tool import Tool
from interlinked.ui.tools import Tools
from interlinked.core.utilities import Utilities
from interlinked.core.classes import ExternalItem
from interlinked.core.clients.ajaxclient import AJAXClient
from interlinked.core.parsers.codeparser import CodeParser
from interlinked.core.clients.mailclient import MailClient
from interlinked.core.clients.slackclient import SlackClient
from interlinked.core.clients.googleaiclient import GoogleAIClient
from interlinked.core.clients.databaseclient import DatabaseClient
from interlinked.core.clients.customradarclient import RadarClient
from interlinked.core.clients.anthropicclient import AnthropicClient
from interlinked.core.ai import AI, Observation, Knowledge, Pause, Section
from interlinked.core.dynamic.interpreter import DynamicInterpreter, Dynamic
from interlinked.core.clients.accessmanagerclient import AccessManagerClient
from interlinked.core.clients.baseaiclient import File, AIModel, BaseAIClient
from interlinked.core.clients.disclosurecentralclient import DisclosureCentralClient
from interlinked.core.clients.appledirectoryclient import AppleDirectoryClient, Person
from interlinked.ui.schemas import Item, ItemUpdate, Entry, Attachment, ConfigurationUsage,  \
								   SuggestionResultEnum, ConfigurationUpdate, AskRequest, ToolUpdate, APIKeyCache,  \
								   Questionnaire, QuestionnaireResponse
from interlinked.ui.models import BaseSQLModel, BaseWorkflow, User, Suggestion, BaseConfiguration, Configuration, KnowledgeSource,  \
								  KnowledgeSourceEnum, Post, PostMember, PostComment, PostReaction, PostKnowledge, RunStatusEnum, Run, Step,  \
								  Log, Event, EventAttendee, EventsSubscriber, APIKey, APIKeyUsage, ModelReview, DocumentationKnowledge, Applet,  \
								  AppVersion, AppletComment, Asset, AppletSubscriber, ResearchApplication, Setup

logger = logging.getLogger(__name__)

""" Applets """

BASE_PATH: str = pathlib.Path(__file__).resolve().parent
STATIC_PATH: str = pathlib.Path(f'{BASE_PATH}/static')
assets_cache: dict[str, Any] = {}

""" Database """

SessionDep = Annotated[Session, Depends(DatabaseClient.shared.get_session)]


@asynccontextmanager
async def lifespan(fast_api_app: FastAPI):

	try:
		BaseSQLModel.create_all_models()

	except Exception as exception:
		logger.error('could not create models', exc_info=True)

	redis_instance: 'Redis' = None

	try:

		from redis import Redis
		redis_instance = Redis.from_url(Config.current.REDIS_URL)

	except ModuleNotFoundError:
		pass

	# check if Redis is installed but not running locally
	if Config.current.is_development and redis_instance:

		from redis.exceptions import ConnectionError as RedisConnectionError

		try:
			redis_instance.ping()
			fast_api_app.state.redis = redis_instance

		except RedisConnectionError:
			logger.error('Redis is installed but not running')

	elif not Config.current.is_development:
		fast_api_app.state.redis = redis_instance

	# clear cached clients
	if not Config.current.is_development and redis_instance:
		redis_instance.delete('ai_clients')

	yield

	DatabaseClient.shared.engine.dispose()


fast_api_app: FastAPI = FastAPI(title='Interlinked', description='Interlinked, a powerful AI platform and library', lifespan=lifespan)
fast_api_app.mount('/static', StaticFiles(directory=STATIC_PATH), name='static')

# make sure the directory always exists
# may sometimes occur when switching branches
os.makedirs(f'{STATIC_PATH}/assets', exist_ok=True)
fast_api_app.mount('/assets', StaticFiles(directory=pathlib.Path(f'{STATIC_PATH}/assets')), name='assets')

# for development, allows the UI (served by Vite on another port) to reach this port
fast_api_app.add_middleware(CORSMiddleware, allow_origins=['*'], allow_credentials=True, allow_methods=['*'], allow_headers=['*'])

security: HTTPBearer = HTTPBearer(auto_error=False)
oidc_client: AppleConnectOIDC = AppleConnectOIDC(client_id=Config.current.IDMS_APP_OIDC_CLIENT_ID,
							   					 client_secret=Config.current.IDMS_APP_OIDC_CLIENT_SECRET,
							   					 scope=['openid', 'dsid', 'profile', 'email', 'groups', 'extended_profile'],
							   					 redirect_uri='https://interlinked.apple.com/__login')

fast_api_app.add_middleware(ASGIMiddleware, oidc_client=oidc_client, exclude_paths={
	'/__health', '/openapi.json', '/static/manifest.json', '/api/debug-noauth',
	'/api/v1/applets/*', '/api/v1/assets/*', '/api/v1/models', '/api/v1/v1beta/*',
	'/api/v1/embeddings', '/api/v1/keys/appleconnect', '/api/v1/keys/usage',
	'/api/v1/threads/*'
})


def validate_api_key(request: Request, session: SessionDep, credentials: HTTPAuthorizationCredentials = Depends(security)) -> APIKeyCache:
	"""
	Redis keys:
	- id: APIKey.id
	- dn: APIKey.department_number
	- rl: APIKey.rate_limits
	- ue: Usage (same as rl, but contains usage values)
	"""

	# get the API Key from either the params to match Google AI (e.g. `/?api_key=in-â€¦`)
	# or the headers (e.g. `Authorization: Bearer in-â€¦`)
	value: str = request.query_params.get('key')

	if not value and credentials and credentials.scheme == 'Bearer':
		value = credentials.credentials

	if not value:
		raise HTTPException(status_code=401, detail='No API Key is provided in the headers')

	cache_key: str = f'{APIKeyCache.__CACHE_KEY_PREFIX__}:{value}'

	# look up if we have it cached in Redis first
	if api_key_cache_raw := fast_api_app.state.redis.get(cache_key):

		api_key_cache_raw: dict[str, Any] = json.loads(api_key_cache_raw)
		api_key_cache: APIKeyCache = APIKeyCache(**api_key_cache_raw, **{'value': value})

		if not api_key_cache.enabled:
			raise HTTPException(status_code=403, detail='The API Key was invalidated')

		return api_key_cache

	# if not cached, look up in the database
	api_key: APIKey = APIKey.select().where(APIKey.value == value).first(session=session)

	if not api_key:
		raise HTTPException(status_code=401, detail='Invalid API Key')

	api_key_cache: APIKeyCache = APIKeyCache(**{**api_key.model_dump(), **{'value': value}})

	if api_key and api_key.enabled and not api_key.is_for_platform and not api_key.is_transient:
		fast_api_app.state.redis.set(cache_key, json.dumps(api_key_cache.model_dump()), ex=28800)

	else:
		raise HTTPException(status_code=403, detail='Invalid API Key')

	return api_key_cache


def validate_platform_api_key(current_user: User, session: 'SessionDep') -> APIKeyCache:
	"""
	[Used in chat.apple.com]

	Redis keys:
	- id: APIKey.id
	- rl: APIKey.rate_limits
	- ue: Usage (same as rl, but contains usage values)
	"""
	# e.g. `in-123456`
	value: str = f'in-{current_user.dsid}'

	cache_key: str = f'{APIKeyCache.__CACHE_KEY_PREFIX__}:{value}'

	# look up if we have it cached in Redis first
	if api_key_cache_raw := fast_api_app.state.redis.get(cache_key):

		api_key_cache_raw: dict[str, Any] = json.loads(api_key_cache_raw)
		api_key_cache: APIKeyCache = APIKeyCache(**api_key_cache_raw, **{'value': str(value)})

		return api_key_cache

	# if not cached, create a new key
	api_key: APIKey = APIKey.select().where(APIKey.value == value).first(session=session)

	# API key doesn't exist in the database for the user: create a new one
	if not api_key:

		# get the cost center of the user
		person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=current_user.dsid)
		department_number: str = person.department_number
		svp: Person = person.svp

		api_key: APIKey = APIKey.create(value=value, description='Interlinked Platform use',
										created_by=current_user.dsid, department_number=department_number,
										svp_dsid=svp.dsid, svp_full_name=f'{svp.first_name} {svp.last_name}',
										rate_limits={}, is_for_platform=True, session=session)

	api_key_cache: APIKeyCache = APIKeyCache(**{**api_key.model_dump(), **{'value': value}})

	if api_key and api_key.is_for_platform:
		fast_api_app.state.redis.set(cache_key, json.dumps(api_key_cache.model_dump()), ex=28800)

	else:
		raise HTTPException(status_code=403, detail='Invalid API key')

	return api_key_cache


def apply_rate_limits(client_name: str, model_name: str, type: str, api_key_cache: APIKeyCache,
					  image_count: int = 0, audio_length: int = 0, video_length: int = 0, enforce_limits: bool = True):
	"""
	Applies rate limits to requests
	"""

	# e.g. `{'*': {'hour': 100, 'day': 1000}}`
	# use the default rate-limits if none are set
	rate_limits: dict[str, Any] = (api_key_cache.rate_limits or APIKeyCache.get_default_rate_limits()).get(type, {})

	# e.g. `googleaiclient:gemini-â€¦`
	rate_limit_key: str = f'{client_name}:{model_name}'

	if not rate_limits and enforce_limits:
		rate_limits = APIKeyCache.get_default_rate_limits()

	# get the rate limit for this model
	# e.g. `{'hour': 100, 'day': 1000}`
	rate_limit: dict[str, int] = rate_limits.get(rate_limit_key)

	if rate_limit_key not in rate_limits and '*' in rate_limits:
		rate_limit = rate_limits.get('*')

	pipeline = fast_api_app.state.redis.pipeline()
	exact_hour_key: str = f'akue:{api_key_cache.id}:{type}:{rate_limit_key}:{datetime.now(timezone.utc).strftime("%Y-%m-%dT%H")}'
	hour_key: str = f'aku:{api_key_cache.id}:{type}:{rate_limit_key}:hour'
	day_key: str = f'aku:{api_key_cache.id}:{type}:{rate_limit_key}:day'
	week_key: str = f'aku:{api_key_cache.id}:{type}:{rate_limit_key}:week'
	image_count_day_key: str = f'aku:{api_key_cache.id}:{type}:image:{rate_limit_key}:day'
	audio_length_day_key: str = f'aku:{api_key_cache.id}:{type}:audio:{rate_limit_key}:day'
	video_length_day_key: str = f'aku:{api_key_cache.id}:{type}:video:{rate_limit_key}:day'

	# lock edits to the keys
	pipeline.watch(exact_hour_key, hour_key, day_key, week_key, image_count_day_key, audio_length_day_key, video_length_day_key)

	exact_hour_usage: int = int(pipeline.get(exact_hour_key) or 0)
	hour_usage: int = int(pipeline.get(hour_key) or 0)
	day_usage: int = int(pipeline.get(day_key) or 0)
	week_usage: int = int(pipeline.get(week_key) or 0)
	image_count_day_usage: int = int(pipeline.get(image_count_day_key) or 0)
	audio_length_day_usage: int = int(pipeline.get(audio_length_day_key) or 0)
	video_length_day_usage: int = int(pipeline.get(video_length_day_key) or 0)

	logger.info(f'rate limits (id={api_key_cache.id}, {model_name=}, hour={hour_usage + 1}/{(rate_limit or {}).get("hour", 0)}, day={day_usage + 1}/'
				f'{(rate_limit or {}).get("day", 0)}, week={week_usage + 1}/{(rate_limit or {}).get("week", 0)}, image_count={image_count_day_usage + image_count}'
				f'/{(rate_limit or {}).get("day_image_count", 0)}, audio_count={audio_length_day_usage + audio_length}'
				f'/{(rate_limit or {}).get("day_audio_count", 0)}, video_count={video_length_day_usage + video_length}'
				f'/{(rate_limit or {}).get("day_video_count", 0)})')

	if enforce_limits:

		if rate_limit is None:
			raise Exception(f'You do not have access to {model_name}')

		# check weekly limit only if it is defined
		if (week_limit := rate_limit.get('week')) and week_usage + 1 > week_limit:
			raise HTTPException(status_code=429, detail=f'Reached maximum number of API calls per-week ({model_name}, {hour_usage}/{rate_limit.get("hour")}hr, {rate_limit.get("day")}/{day_usage}day, {rate_limit.get("week")}/{week_usage}week)')

		# check if usage is too high for the hour and day
		if day_usage + 1 > rate_limit.get('day', 0):
			raise HTTPException(status_code=429, detail=f'Reached maximum number of API calls per-day ({model_name}, {hour_usage}/{rate_limit.get("hour")}hr, {rate_limit.get("day")}/{day_usage}day)')

		if hour_usage + 1 > rate_limit.get('hour', 0):
			raise HTTPException(status_code=429, detail=f'Reached maximum number of API calls per-hour ({model_name}, {hour_usage}/{rate_limit.get("hour")}hr, {rate_limit.get("day")}/{day_usage}day)')

		if image_count and image_count > 0 and image_count_day_usage + image_count > rate_limit.get('day_image_count', 0):
			raise HTTPException(status_code=429, detail=f'Too many images per-day ({model_name}, {hour_usage}/{rate_limit.get("hour")}hr, {rate_limit.get("day")}/{day_usage}day, {image_count_day_usage=})')

	# always enforce limits for audio and video
	if audio_length and audio_length > 0 and audio_length_day_usage + audio_length > rate_limit.get('day_audio_length', 0):
		raise HTTPException(status_code=429, detail=f'Reached maximum audio length per-day ({model_name}, {hour_usage}/{rate_limit.get("hour")}hr, {rate_limit.get("day")}/{day_usage}day, {audio_length_day_usage + audio_length}/{rate_limit.get("day_audio_length")}s audio)')

	if video_length and video_length > 0 and video_length_day_usage + video_length > rate_limit.get('day_video_length', 0):
		raise HTTPException(status_code=429, detail=f'Reached maximum video length per-day ({model_name}, {hour_usage}/{rate_limit.get("hour")}hr, {rate_limit.get("day")}/{day_usage}day, {video_length_day_usage + video_length}/{rate_limit.get("day_video_length")}s videos)')

	# increase usage
	pipeline.multi()
	pipeline.set(day_key, day_usage + 1, ex=86400)
	pipeline.set(hour_key, hour_usage + 1, ex=3600)
	pipeline.set(week_key, week_usage + 1, ex=604800)
	pipeline.set(exact_hour_key, exact_hour_usage + 1)

	if image_count and image_count > 0:
		pipeline.set(image_count_day_key, image_count_day_usage, ex=86400)

	if audio_length and audio_length > 0:
		pipeline.set(audio_length_day_key, audio_length_day_usage, ex=86400)

	if video_length and video_length > 0:
		pipeline.set(video_length_day_key, video_length_day_usage, ex=86400)

	pipeline.execute()


""" Events """

@fast_api_app.exception_handler(500)
async def internal_exception_handler(request: Request, exception: Exception) -> JSONResponse:
	return JSONResponse(status_code=500, content=jsonable_encoder({'message': str(exception)}))


# exclude `/__health` logs
class EndpointFilter(logging.Filter):

	def filter(self, record: logging.LogRecord) -> bool:
		return record.args and len(record.args) >= 3 and record.args[2] != '/__health'

logging.getLogger('uvicorn.access').addFilter(EndpointFilter())


""" Proxy """

class FloodGateProxy:

	BASE_URL: str = 'https://floodgate.g.apple.com'


""" Authentication """

def get_current_user(request: Request) -> User:
	"""
	Returns the current user
	"""

	if Config.current.is_development:
		return User(first_name='User', last_name='Last name', email='email@apple.com', dsid=Config.current.DSID)

	else:

		# ensure that logging headers are in the request
		headers: EnvironHeaders = request.headers

		nick_name: str = ''
		last_name: str = ''
		email: str     = ''
		dsid: str      = ''

		if Config.current.USE_OIDC:

			nick_name = request.auth.get('nickname')
			last_name = request.auth.get('family_name')
			email = request.auth.get('email')
			dsid = request.auth.get('dsid', 0)

		# get the user info from Shield
		else:

			headers: EnvironHeaders = request.headers

			nick_name = request.headers.get('shield-ds-nickname')
			last_name = request.headers.get('shield-ds-lastname')
			email = request.headers.get('shield-ds-emailaddress')
			dsid = request.headers.get('shield-ds-prsid')

		logger.info(f'{nick_name} {last_name} ({email})')
		return User(first_name=nick_name, last_name=last_name, email=email,
					dsid=dsid, daw_token=request.cookies.get('acack'))

""" Users """

@fast_api_app.get('/api/users/me', include_in_schema=False)
async def me(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep) -> User:

	# load core groups
	# if not Config.current.is_development:
	current_user.core_group_dsids

	if Config.current.is_development and not current_user.has_latest_version:

		from interlinked.ui.cli import CLI
		cli_configuration: dict[str, Any] = CLI.get_cli_configuration()

		package_name: str = 'apple-interlinked'
		latest_version: str = cli_configuration.get('latest_version')
		installed_version: str = Utilities.get_installed_version(package_name=package_name)

		if latest_version and latest_version > installed_version:

			current_user.has_latest_version = True
			current_user.latest_version = latest_version

	# always maintain the same order, for the UI
	current_user.setups = current_user.get_setups(session=session)
	current_user.setups.sort(key=lambda setup: setup.id)
	current_user.setups = [setup.dict() for setup in current_user.setups]

	return current_user


""" People """

@fast_api_app.get('/api/persons', summary='Returns a list of persons for a given name')
def get_persons(name: str = None, dsid: int = None) -> list[Person]:

	if name:

		try:
			return AppleDirectoryClient.shared.get_persons_by_name(name=name)

		except Exception as exception:
			logger.error(f'could not look up person ({name})', exc_info=True)

	elif dsid:

		try:

			person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=dsid)

			if person:
				return [person]

		except Exception as exception:
			logger.error(f'could not look up person ({dsid})', exc_info=True)

	return []


""" Groups """

@fast_api_app.get('/api/groups', summary='Returns a list of Apple Directory groups')
def get_groups(name: str) -> list[dict]:
	return AppleDirectoryClient.shared.get_groups_by_name(name=name)


@fast_api_app.get('/api/groups/{dsid}/persons', summary='Returns a list of persons/members in an Apple Directory group for a given ID')
def get_group_persons(dsid: int = Annotated[int, Path(title='The numeric ID of the group')]) -> list[Person]:
	return sorted(AppleDirectoryClient.shared.get_persons_for_group_dsid(dsid=dsid, nested=True), key=lambda person: person.name)


""" Disclosures """

@fast_api_app.get('/api/disclosures/status', summary='Returns a boolean of whether a person is disclosed')
def get_is_disclosed(current_user: Annotated[User, Depends(get_current_user)], project_name: str) -> list[Person]:
	return False

""" Knowledge """
# previously removed in be84da48f6fbff2d0a177b7fc3eb14f847d8d44f

@fast_api_app.get('/api/knowledges/sources', summary='Returns a list of Knowledge Sources')
def get_knowleges_sources(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep) -> dict:

	knowledge_sources: list[KnowledgeSource] = []

	# if we're in production, limit by Knowledge the user has access to
	if Config.current.is_development:
		knowledge_sources = session.query(KnowledgeSource).all()

	else:
		knowledge_sources = KnowledgeSource.get_for_user(user=current_user)

	return {'knowledge_sources': [{**knowledge_source.model_dump(),
								   'display_source': knowledge_source.display_source} for knowledge_source in knowledge_sources]}


@fast_api_app.post('/api/knowledges/search', summary='Searches Knowledge')
def search_knowledge(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, data: dict) -> dict[str, list]:

	# find all the Knowledge sources the user has access to
	knowledge_sources: list[KnowledgeSource] = session.query(KnowledgeSource).where(KnowledgeSource.created_by == current_user.dsid).all()

	client: GoogleAIClient = GoogleAIClient(model_name='gemini-2.0-flash', embedding_model_name='text-embedding-005')

	# add the labels
	client.labels = {

		'interlinked:api_key_id': None,
		'interlinked:dsid': str(current_user.dsid),
		'interlinked:location': 'search_playground',

		# TODO:
		# 'interlinked:svp_dsid': str(api_key_cache.svp_dsid),
		# 'interlinked:svp_full_name': str(api_key_cache.svp_full_name),
	}

	knowledges: list[Knowledge] = Knowledge.search(query=data['query'], knowledge_source_ids=[knowledge_source.id for knowledge_source in knowledge_sources],
												   min_score=data['min_score'], limit=data['limit'], max_gap=data['max_gap'],
												   client=client)
	knowledge_dicts: list[dict] = []

	for knowledge in knowledges:

		display_source: str = None
		knowledge_source: KnowledgeSource = None

		if knowledge.knowledge_source_id:

			knowledge_source = next((knowledge_source for knowledge_source in knowledge_sources if knowledge_source.id == knowledge.knowledge_source_id), None)

			if knowledge_source:

				if knowledge.external_id:
					display_source = knowledge_source.get_display_source_for_knowledge(knowledge=knowledge)

				else:
					display_source = knowledge_source.display_source

		knowledge_dicts.append({'content': knowledge.content, 'external_id': display_source,
								'knowledge_source_id': knowledge.knowledge_source_id, 'id': knowledge.id,
								'knowledge_source_name': knowledge_source.name if knowledge_source else None,
								'source': knowledge_source.source if knowledge_source else None,
								'score': knowledge.score,
								'created_at': knowledge_source.external_created_at if knowledge_source else None,
								'modified_at': knowledge_source.external_modified_at if knowledge_source else None})

	return {'knowledges': knowledge_dicts}


@fast_api_app.post('/api/knowledges/sources/upload', summary='Uploads a file', include_in_schema=False)
async def upload_knowledge_file(current_user: Annotated[User, Depends(get_current_user)], file: UploadFile = FastAPIFile(...)) -> KnowledgeSource:

	knowledge_source: KnowledgeSource = None

	client: GoogleAIClient = GoogleAIClient(model_name='gemini-2.0-flash', embedding_model_name='text-embedding-005')

	# add the labels
	client.labels = {

		'interlinked:api_key_id': None,
		'interlinked:dsid': str(current_user.dsid),
		'interlinked:location': 'search_playground',

		# TODO:
		# 'interlinked:svp_dsid': str(api_key_cache.svp_dsid),
		# 'interlinked:svp_full_name': str(api_key_cache.svp_full_name),
	}

	# save the file to a temporary path
	with tempfile.TemporaryDirectory() as temporary_path:

		file_path: str = os.path.join(temporary_path, file.filename)
		content: str = await file.read()
		content = content.decode()

		# create a Knowledge Source
		knowledge_source, is_new = KnowledgeSource.get_or_create(external_id=file.filename, source=KnowledgeSourceEnum.file,
																 content=content, created_by=current_user.dsid)

		with open(file_path, 'w') as write_file:
			write_file.write(content)

		# learn
		knowledges: list[Knowledge] = AI.learn(from_=file_path, knowledge_source_id=knowledge_source.id, client=client)

		# delete all outdated knowledge
		if not is_new:
			Knowledge.delete_all(knowledge_source_ids=[knowledge_source.id], keep=knowledges)

	return knowledge_source


@fast_api_app.get('/api/knowledges/sources/{id}/section', summary='Returns the Section of a Knowledge Source', include_in_schema=False)
async def get_knowledge_source_section(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int) -> Section:

	knowledge_source: KnowledgeSource = KnowledgeSource.get_or_none(id=id, created_by=current_user.dsid, session=session)

	if not knowledge_source:
		raise HTTPException(status_code=404, detail='Knowledge Source not found or you do not have access')

	if knowledge_source.source == 'file':

		if knowledge_source.external_id.endswith('.md'):

			from interlinked.core.parsers.markdownparser import MarkdownParser
			return MarkdownParser.get_section_for_markdown(content=knowledge_source.content)

		elif knowledge_source.external_id.endswith('.rst'):

			from interlinked.core.parsers.restructuredparser import RestructuredParser
			return RestructuredParser.get_section_for_restructured(content=knowledge_source.content)

		else:
			return Section(title=None, content=knowledge_source.content)

	raise HTTPException(status_code=404, detail=f'Knowledge Source is not supported ({knowledge_source.source})')


@fast_api_app.delete('/api/knowledges/sources/{id}', summary='Deletes a Knowledge Source and its Knowledge', include_in_schema=False)
async def delete_knowledge_source(current_user: Annotated[User, Depends(get_current_user)], id: int, session: SessionDep) -> KnowledgeSource:

	knowledge_source: KnowledgeSource = KnowledgeSource.get_or_none(id=id, created_by=current_user.dsid, session=session)

	if not knowledge_source:
		raise HTTPException(status_code=404, detail='Knowledge Source not found or you do not have access')

	# delete all of the Knowledge first
	Knowledge.delete_all(knowledge_source_ids=[knowledge_source.id])

	session.delete(knowledge_source)
	session.commit()
	return knowledge_source


""" AI """

@fast_api_app.post('/api/ai/ask', summary='Asks AI then returns the response')
def ask_ai(request: Request, current_user: Annotated[User, Depends(get_current_user)],
		   session: SessionDep, ask_request: Annotated[AskRequest, Body(embed=False)]):

	# forward models
	# e.g. `gemini-2.0-flash-exp` â€º `gemini-2.0-flash`
	if ask_request.client_model_name and (forwarded_model_name := APIKeyCache.__FORWARDED_MODEL_NAME_MAP__.get(ask_request.client_model_name)):
		ask_request.client_model_name = forwarded_model_name

	if ask_request.client_name == 'endorclient':

		if ask_request.client_model_name.startswith(('endor-gemini-', 'endor-afm-')):
			ask_request.client_model_name = ask_request.client_model_name.removeprefix('endor-')

	elif ask_request.client_name == 'ajaxclient' and not Config.current.is_development:

		ask_request._client = AJAXClient(model_name=ask_request.client_model_name, app_id=Config.current.IDMS_APP_ID,
										 app_password=Config.current.IDMS_APP_PASSWORD,
										 dsid=current_user.dsid, **(ask_request.options or {}))

	logger.info(f'{current_user.first_name} {current_user.last_name} ({current_user.dsid}), {ask_request.client_model_name}')

	# set system message
	if ask_request.setup_id:

		setup: Setup = Setup.get_or_none(id=ask_request.setup_id, created_by=current_user.dsid, session=session)

		if not setup:
			raise HTTPException(status_code=403, detail='You do not have access to this Setup or it is not found')

		ask_request.system_message = setup.system_message

	observation: Observation = None

	# track usage count
	# get image, audio and video count (for rate limits, in seconds)
	image_count: int = 0
	audio_length: int = 0
	video_length: int = 0

	is_video_only: bool = getattr(ask_request, 'is_video_only', False)

	if not is_video_only and 'veo' in ask_request.client_model_name.lower():

		is_video_only = True
		ask_request.is_video_only = True

	# googleai
	if ask_request.client_name == 'googleaiclient':

		# floodgate in production
		if not Config.current.is_development and not (ask_request.is_web_search or ask_request.is_video_only or ask_request.is_image_only):

			cert = ('/narrative/kube-actor/cert.pem', '/narrative/kube-actor/private.pem')
			ask_request._client = GoogleAIClient(model_name=ask_request.client_model_name, cert=cert, dsid=str(current_user.dsid),
													use_floodgate=True, headers={'X-Forwarded-For': request.headers.get('X-Forwarded-For', request.client.host)}, options=ask_request.options)

		_messages: list[str, Any] = ask_request.messages_for_client
		_messages.append(ask_request.client.create_message(role=ask_request.client.ROLE_USER, content=ask_request.prompt,
															  files=ask_request.files_for_client))

		counts: dict[str, Any] = get_counts_in_messages(messages=_messages, client_name=ask_request.client_name)
		image_count = counts['image_count']
		audio_length = counts['audio_length']
		video_length = counts['video_length']

	# anthropic + floodgate
	elif ask_request.client_name == 'anthropicclient' and not Config.current.is_development:

		cert = ('/narrative/kube-actor/cert.pem', '/narrative/kube-actor/private.pem')
		ask_request._client = AnthropicClient(model_name=ask_request.client_model_name, cert=cert, dsid=str(current_user.dsid),
												 headers={'X-Forwarded-For': request.headers.get('X-Forwarded-For', request.client.host)}, options=ask_request.options)

	# thinking
	if ask_request.is_thinking_enabled:

		if ask_request.client_name == 'anthropicclient':

			ask_request.temperature = 1
			ask_request.client.options['max_tokens'] = 32000
			ask_request.client.options['thinking'] = {'type': 'enabled', 'budget_tokens': 25000}

	# only apply rate-limits for image creation, video creation and web search
	if hasattr(fast_api_app.state, 'redis'):

		api_key_cache: APIKeyCache = validate_platform_api_key(current_user=current_user, session=session)
		apply_rate_limits(client_name=ask_request.client_name, model_name=ask_request.client_model_name, type='chat',
						  api_key_cache=api_key_cache, enforce_limits=(ask_request.is_image_only or ask_request.is_video_only or ask_request.is_web_search))

		# add the API Key's DSID
		if ask_request.client_name == 'googleaiclient':

			ask_request.client.labels = {

				'interlinked:location': 'ask_ai',
				'interlinked:api_key_id': str(api_key_cache.id),
				'interlinked:dsid': str(api_key_cache.created_by),
				'interlinked:svp_dsid': str(api_key_cache.svp_dsid),
				'interlinked:svp_full_name': str(api_key_cache.svp_full_name),
			}

	# web search
	tools: list[Tool] = []

	if ask_request.is_web_search:

		if ask_request.client_name == 'googleaiclient':
			tools.append(Tool(function='google_search'))

		else:
			raise HTTPException(status_code=400, detail='Web search is only supported by Google AI')

	if ask_request.is_video_only:

		logger.info(f'{current_user.first_name} {current_user.last_name} ({current_user.dsid}). Video ðŸŽ¬')

		default_options = {

			'sampleCount': 1,
			'durationSeconds': 8,
			'aspectRatio': '16:9',
			'enhancePrompt': True,
			'generateAudio': 'veo-3' in ask_request.client_model_name.lower()
		}

		# apply merged options to client
		for key, value in default_options.items():
			ask_request.client.options[key] = value

		try:
			observation: Observation = AI.ask(prompt=ask_request.prompt, client=ask_request.client)

		except Exception as exception:

			logger.error(f'Video creation failed ({current_user.dsid=}, {exception})', exc_info=True)

			error_message: str = str(exception).replace(Config.current.GOOGLEAI_PROJECT_ID, "[redacted]")
			raise HTTPException(status_code=500, detail=error_message)

	# image creation/editing
	elif ask_request.is_image_only:

		logger.info(f'{current_user.first_name} {current_user.last_name} ({current_user.dsid}). Image ðŸŒŒ')

		# limit the number of images
		ask_request.client.options['sampleCount'] = 2
		ask_request.client.options['enhancePrompt'] = True
		ask_request.client.options['personGeneration'] = 'allow_all'
		ask_request.client.options['safetySetting'] = 'block_only_high'

		observation: Observation = AI.ask(prompt=ask_request.prompt, client=ask_request.client)

	# stream
	elif ask_request.stream:

		def _get_response_generator(ask_request: AskRequest):

			try:

				for observation in AI.ask(prompt=ask_request.prompt, template=ask_request.template, temperature=ask_request.temperature,
										  messages=ask_request.messages_for_client, files=ask_request.files_for_client,
										  dsid=current_user.dsid if not Config.current.is_development else None,
										  client=ask_request.client, stream=True):

					# thinking
					if observation.response.metadata and ask_request.client_name == 'anthropicclient' and 'thinking' in observation.response.metadata:
						yield f'data: {json.dumps({"type": "thinking", "content": observation.response.metadata.get("thinking")})}\n\n'

					else:
						yield f'data: {json.dumps({"type": "content", "content": observation.response.raw})}\n\n'

			except requests.exceptions.HTTPError as error:

				if error.response.status_code == 429:

					error_msg = f'We are experiencing high demand on {ask_request.client_model_name} at the moment. Please try again in a few minutes{" or use a different AI model from the AIs menu at the top-right" if "exp" in ask_request.client_model_name.lower() else ""}.'
					yield f'data: {json.dumps({"type": "error", "content": error_msg})}\n\n'

				logger.error(error.response.text, exc_info=True)

				if ask_request.client_name == 'googleaiclient':

					display_error: str = str(error.response.json().get('error', {}).get('message', error.response.text)).replace('mimeType', 'file extension').replace(Config.current.GOOGLEAI_PROJECT_ID, "[redacted]")

					if 'visibility check was unavailable' in display_error.lower():
						display_error = 'Google is currently experiencing an outage. You can use an alternative AI model from the "AIs" menu at the top-right.'

					yield f'data: {json.dumps({"type": "error", "content": display_error})}\n\n'

				else:
					yield f'data: {json.dumps({"type": "error", "content": error_msg})}\n\n'

			except Exception as exception:

				logger.error(exception, exc_info=True)
				error_msg = str(exception).replace(Config.current.GOOGLEAI_PROJECT_ID, "[redacted]")

				if 'Unsupported prompt format' in error_msg:
					error_msg = 'Error: This AI model does not support files yet. Switch to an alternative AI model in "AIs" at the top-right, then try again.'

				yield f'data: {json.dumps({"type": "error", "content": error_msg})}\n\n'

			yield f'data: {json.dumps({"type": "done"})}\n\n'

		try:

			return StreamingResponse(_get_response_generator(ask_request=ask_request), media_type='text/event-stream', headers={
				'Content-Type': 'text/event-stream',
				'Cache-Control': 'no-cache',
				'X-Accel-Buffering': 'no'
			})

		except HTTPException as exception:
			raise exception

	# with websearch
	elif ask_request.is_web_search:

		logger.info(f'{current_user.first_name} {current_user.last_name} ({current_user.dsid}). Websearch ðŸŒ')

		# Google Search the response
		if ask_request.client_name == 'googleaiclient':

			observation: Observation = AI.ask(prompt=ask_request.prompt, template=ask_request.template, temperature=ask_request.temperature,
											  messages=ask_request.messages_for_client,
											  dsid=current_user.dsid if not Config.current.is_development else None, client=ask_request.client,
											  tools=[Tool(function='google_search')])
			# grounding sources
			metadata = [
				{'title': source['web']['title'], 'url': source['web']['uri']}
				for source in observation.response.metadata.get('groundingMetadata', {}).get('groundingChunks', [])
			]

		else:
			raise HTTPException(status_code=505, detail='Web search is supported only by Google AI.')

	# with tools
	else:

		try:
			observation = AI.ask(prompt=ask_request.prompt, template=ask_request.template, files=ask_request.files_for_client,
								 temperature=ask_request.temperature, messages=ask_request.messages_for_client,
								 dsid=current_user.dsid if not Config.current.is_development else None, client=ask_request.client, tools=tools)

		except requests.exceptions.HTTPError as error:
			raise HTTPException(status_code=error.response.status_code, detail=f'{str(error).replace(Config.current.GOOGLEAI_PROJECT_ID, "[redacted]")}')

	# add sources
	sources: dict[str, Any] = None

	if ask_request.is_web_search:
		sources = [{'title': source['web']['title'], 'url': source['web']['uri']}
					for source in observation.response.metadata.get('groundingMetadata', {}).get('groundingChunks', [])]

	# the status of the prompt (only applies to non-image-only and non-video-only)
	if not ask_request.is_image_only and not ask_request.is_video_only:
		logger.info(f'{current_user.first_name} {current_user.last_name} ({current_user.dsid}). Prompt: {"âŒ" if not observation or observation.response_status else "âœ…"}')

	# there's a bug in `starlette` where `Enum` types in `response_as_dictionary` aren't getting parsed as a string correctly
	return {'messages': observation.messages, 'response_as_dictionary': json.loads(json.dumps(observation.response_as_dictionary)),
			'response_status': observation.response_status, 'response': observation.response.raw, 'sources': sources,
			'files': [{'content': file.base64_representation, 'mime_type': file.mime_type} for file in observation.response.files]}



@fast_api_app.post('/api/ai/feedback', summary='Provides feedback for a conversation')
def provide_feedback(current_user: Annotated[User, Depends(get_current_user)]) -> dict:
	pass


@fast_api_app.post('/api/ai/clients', summary='Returns a list of available clients and models')
def get_ai_clients(current_user: Annotated[User, Depends(get_current_user)], data: dict, request: Request) -> dict:

	# look up if we have it cached in Redis first
	cache_key: str = 'ai_clients:next' if '-next.' in str(request.url) else 'ai_clients'

	if not data.get('skip_cache') and hasattr(fast_api_app.state, 'redis') and (cached_clients := fast_api_app.state.redis.get(cache_key)):
		return {'clients': json.loads(cached_clients)}

	clients: list[dict] = []
	filtered_client_name: str = data.get('client_name')

	for client_name, client in AI.CLIENTS.items():

		models: list[dict] = []
		base_url: str = data.get('options', {}).get(client_name, {}).get('base_url')

		try:

			# OllamaClient requires a non-null model name
			if client_name == 'ollamaclient':

				# skip if we're in production
				if not Config.current.is_development:
					continue

				models = client(model_name='model', base_url=base_url).get_models(dsid=current_user.dsid)

			elif client_name == 'mlxclient':

				# skip if we're in production
				if not Config.current.is_development:
					continue

				models = client(model_name='model', base_url=base_url).get_models()

			elif client_name == 'endorclient':

				models = client().get_models()

				# remove the anthropic/googleai clients
				# since they are already available through floodgate
				models = [model for model in models if 'claude' not in model.name and 'gemini' not in model.name]

				for model in models:

					if model.name.startswith(('afm-', 'gemini-')):
						model.name = f'endor-{model.name}'

			elif client_name == 'anthropicclient':

				import socket
				cert = ('/narrative/kube-actor/cert.pem', '/narrative/kube-actor/private.pem')
				models = client(cert=cert, dsid=str(Config.current.DSID),
								headers={'X-Forwarded-For': socket.gethostbyname(socket.gethostname())}).get_models()

			else:
				models = client(base_url=base_url).get_models(dsid=current_user.dsid)

		except Exception as exception:

			if base_url:
				logger.error(f'could not get models ({client_name}) with custom base_url', exc_info=True)

			else:
				logger.warning(f'could not get models ({client_name}). using cached listâ€¦ ({exception})')

				try:

					with open(f'{STATIC_PATH}/clients.json', 'r') as file:

						_clients: dict[str, list[str]] = json.loads(file.read())
						models = [AIModel(**model_raw) for model_raw in _clients.get(client_name, [])]

				except Exception as exception:
					logger.error(f'could not get cached list ({client_name})', exc_info=True)

		# remove embedding models
		models = [model for model in models if 'embedding' not in model.name]

		# remove fine-tuned models
		if not Config.current.is_development:
			models = [model for model in models if not model.is_fine_tuned]

		# hide clients that do not have models
		# only in production, for user experience since users can't download
		# Ollama or MLX models on-demand in production
		if client_name in {'mlxclient'} and not models and not Config.current.is_development:
			continue

		# check if the client supports fine-tuning
		supports_fine_tuning: bool = True

		if client_name in {'ajaxclient', 'ollamaclient', 'googleaiclient', 'endorclient', 'anthropicclient'}:
			supports_fine_tuning = False

		clients.append({

			'name': client_name,
			'class_name': client.__name__,
			'supports_fine_tuning': supports_fine_tuning,
			'models': [model.model_dump(exclude={'raw'}) for model in models],

			'roles': {

				'user': client.ROLE_USER,
				'tool': client.ROLE_TOOL,
				'system': client.ROLE_SYSTEM,
				'assistant': client.ROLE_ASSISTANT,
			}
		})

	if hasattr(fast_api_app.state, 'redis'):
		fast_api_app.state.redis.setex(cache_key, 43200, json.dumps(clients))

	return {'clients': clients}


""" Workflows """

@fast_api_app.post('/api/workflows/{workflow_name}/{path:path}', summary='Posts data to a given Workflow (e.g. review_pull_requests)')
def post_api_workflows(request: Request, workflow_name: str, path: str, data: dict) -> dict:

	module = importlib.import_module(f'interlinked.core.workflows.{workflow_name}.workflow')
	WorkflowClass: Any = next((cls for name, cls in vars(module).items() if isinstance(cls, type) and issubclass(cls, BaseWorkflow) and  \
							   cls is not BaseWorkflow), None)

	return WorkflowClass.handle_api(path=path, data=data, request=request)


""" Suggest Features """

@fast_api_app.post('/api/suggestions', summary='Submit a feature suggestion')
def post_suggestion(current_user: Annotated[User, Depends(get_current_user)], data: dict) -> dict:
	"""
	Handle feature suggestion submissions
	@param request(Request): FastAPI request object
	@param data(dict): Request data containing suggestion and timestamp
	"""

	try:

		suggestion_text: str = data.get('suggestion', '').strip()

		if not suggestion_text:
			raise HTTPException(status_code=400, detail='Suggestion text is required')

		client = GoogleAIClient(model_name='gemini-2.0-flash')

		prompt = f'''
		Suggest one line title for below description
		{suggestion_text}

		Let's respond in below format:
		Title<!type:string,required:true,max_length:200!>: Title for below description
		'''

		observation = AI.ask(prompt=prompt, client=client)

		external_item: ExternalItem = RadarClient.shared.create_item(title=observation.title or 'Interlinked Feature Enhancement', description=suggestion_text, component_id=1580462)

		return {
			'success': True,
			'message': 'Feature suggestion submitted successfully'
		}

	except HTTPException:
		raise

	except Exception as error:
		raise HTTPException(status_code=500, detail=f'Failed to submit suggestion: {str(error)}')


""" Tools """


@fast_api_app.get('/api/files/open', summary='Opens a given file', include_in_schema=False)
def open_file(path: str) -> None:

	if not Config.current.is_development:
		return

	os.system(f'open "{path}"')


""" Community """

@fast_api_app.get('/api/posts', summary='Returns a list of Community Posts')
def get_posts(session: SessionDep) -> dict:

	post_dicts: list[dict] = []
	posts: list[Post] = []

	# current event
	current_voting_event: Event = Event.select().where(Event.can_vote == True).order_by(-Event.id).first(session=session)

	if Config.current.USE_POSTGRES:

		post_member_sub_query = session.query(
			PostMember.post_id,
			func.array_agg(PostMember.created_by).label('member_dsids_')
		).group_by(PostMember.post_id).subquery()

		if current_voting_event:

			post_reaction_sub_query = session.query(
				PostReaction.post_id,
				func.array_agg(PostReaction.created_by).label('reaction_dsids_')
			).where(PostReaction.is_vote == True).group_by(PostReaction.post_id).subquery()

		else:

			post_reaction_sub_query = session.query(
				PostReaction.post_id,
				func.array_agg(PostReaction.created_by).label('reaction_dsids_')
			).group_by(PostReaction.post_id, PostReaction.is_vote).subquery()

		post_comment_sub_query = session.query(
			PostComment.post_id,
			func.count(PostComment.id).label('comment_count_')
		).group_by(PostComment.post_id).subquery()

		posts = session.query(Post, post_member_sub_query.c.member_dsids_,
							  post_reaction_sub_query.c.reaction_dsids_,
							  post_comment_sub_query.c.comment_count_).  \
						outerjoin(post_member_sub_query, Post.id == post_member_sub_query.c.post_id).  \
						outerjoin(post_reaction_sub_query, Post.id == post_reaction_sub_query.c.post_id).  \
						outerjoin(post_comment_sub_query, Post.id == post_comment_sub_query.c.post_id).all()

		for post, member_dsids, reaction_dsids, comment_count in posts:

			post_dicts.append({**post.model_dump(), 'member_dsids': member_dsids or [],
							   'reaction_dsids': reaction_dsids or [], 'comment_count': comment_count})
	else:

		posts = session.query(Post).all()

		for post in posts:

			post_dicts.append({**post.model_dump(), 'member_dsids': post.member_dsids,
							   'reaction_dsids': post.reaction_dsids, 'comment_count': post.comment_count})

	return {'posts': post_dicts}


@fast_api_app.post('/api/posts', summary='Creates a Community Post')
async def create_post(current_user: Annotated[User, Depends(get_current_user)], post: Annotated[Post, Body(embed=False)],
					  session: SessionDep) -> Post:

	post.created_by = current_user.dsid

	session.add(post)
	session.commit()
	session.refresh(post)

	# add a reaction by default
	PostReaction.create(post_id=post.id, created_by=current_user.dsid, session=session)
	return post


@fast_api_app.get('/api/posts/search', summary='Returns search results Community Posts')
async def search_posts(current_user: Annotated[User, Depends(get_current_user)], query: str, session: SessionDep) -> list[Post]:

	posts: list[Post] = []
	post_knowledges: list[PostKnowledge] = PostKnowledge.search(query=query, min_score=0.1, limit=10)

	for post_knowledge in post_knowledges:

		if _post := Post.get_or_none(id=post_knowledge.knowledge_source_id, session=session):
			posts.append(_post)

	return posts


@fast_api_app.put('/api/posts/{id}', summary='Modifies a Community Post')
async def modify_post(current_user: Annotated[User, Depends(get_current_user)], id: int,
					  post_update: Annotated[Post, Body(embed=False)], session: SessionDep) -> Post:

	post: Post = None

	if current_user.is_admin or Config.current.is_development:
		post = Post.get_or_none(id=id, session=session)

	else:
		post = Post.select().where(Post.id == id, Post.created_by == current_user.dsid).first(session=session)

	if not post:
		raise HTTPException(status_code=404, detail='Post not found')

	had_code: bool = post.has_code

	values: dict[str, Any] = post_update.dict(exclude_unset=True)

	for key, value in values.items():

		if key.endswith(('_at', '_by')) or key == 'view_count':
			continue

		setattr(post, key, value)

	session.add(post)
	session.commit()
	session.refresh(post)

	# if the post has code, notify each person that reacted to the post
	if not had_code and post_update.has_code:

		for dsid in set(post.reaction_dsids + post.comment_dsids):

			if dsid == current_user.dsid:
				continue

			person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=dsid)

			if person and person.email:

				template: str = open(f'{STATIC_PATH}/community_post_email.html', 'r').read()

				template = template.replace('<#notes#>', '')
				template = template.replace('<#id#>', str(post.id))
				template = template.replace('<#post_title#>', post.title)
				template = template.replace('<#type#>', 'The idea you liked now has code you can try!')

				MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com', subject=f'Code available for "{post.title}"',
								body=template)

	return post


@fast_api_app.get('/api/posts/{id}', summary='Returns the contents of a Community Post')
async def get_post(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int) -> dict:

	post: Post = Post.get_or_none(id=id, session=session)
	created_by_person: Person = post.created_by_person

	post_dict: dict[str, Any] = {
		**post.model_dump(),

		'comments': [],
		'comment_count': post.comment_count,
		'reaction_dsids': post.reaction_dsids,
		'created_by': created_by_person.model_dump() if created_by_person else None,
	}

	# add member DSIDs
	post_dict['member_dsids'] = post.member_dsids if post.allows_members else []

	for comment in post.comments:

		_created_by_person: Person = comment.created_by_person

		post_dict['comments'].append({

			**comment.model_dump(),
			'created_by': _created_by_person.model_dump() if _created_by_person else None
		})

	post.view_count += 1
	post.save(session=session)

	return post_dict


@fast_api_app.delete('/api/posts/{id}', summary='Deletes a Community Post')
async def delete_post(current_user: Annotated[User, Depends(get_current_user)], id: int, session: SessionDep) -> Post:

	post: Post = Post.select().where(Post.id == id, Post.created_by == current_user.dsid).first(session=session)

	if not post:
		raise HTTPException(status_code=404, detail='Post not found')

	PostComment.delete().where(PostComment.post_id == post.id).execute()
	PostReaction.delete().where(PostReaction.post_id == post.id).execute()

	session.delete(post)
	session.commit()
	return post


@fast_api_app.post('/api/posts/{id}/react', summary='Reacts to a Community Post')
async def react_to_post(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int, data: dict) -> None:

	post: Post = Post.get_or_none(id=id, session=session)

	if not post:
		raise HTTPException(status_code=404, detail='Post not found')

	event: Event = Event.get_or_none(id=post.event_id, session=session) if post.event_id else None

	if data.get('action', 'add') == 'add':

		# if we're voting, check if we already voted
		if event and event.can_vote:

			if not session.query(PostReaction).where(PostReaction.post_id == post.id, PostReaction.created_by == current_user.dsid, PostReaction.is_vote == True).all():
				PostReaction.create(post_id=id, created_by=current_user.dsid, is_vote=True, session=session)

		elif not session.query(PostReaction).where(PostReaction.post_id == post.id, PostReaction.created_by == current_user.dsid).all():
			PostReaction.create(post_id=id, created_by=current_user.dsid, session=session)

	else:

		# if we're voting, remove the vote
		if event and event.can_vote:
			PostReaction.delete().where(PostReaction.post_id == post.id, PostReaction.created_by == current_user.dsid, PostReaction.is_vote == True).execute()

		else:
			PostReaction.delete().where(PostReaction.post_id == post.id, PostReaction.created_by == current_user.dsid).execute()


@fast_api_app.post('/api/posts/{id}/membership', summary='Joins/leaves a Community Post', include_in_schema=False)
async def toggle_post_membership(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep,
								 id: int, data: dict) -> None:

	post: Post = Post.get_or_none(id=id, session=session)

	if not post:
		raise HTTPException(status_code=404, detail='Post not found')

	# check if the post allows people to join
	if not post.allows_members:
		raise HTTPException(status_code=403, detail='This idea is not accepting new members')

	dsid: int = data.get('dsid', current_user.dsid)

	if data.get('action', 'join') == 'join':

		if not post.created_by == current_user.dsid and not session.query(PostMember).where(PostMember.post_id == post.id, PostMember.created_by == dsid).all():
			PostMember.create(post_id=id, created_by=dsid, session=session)

		# send a notification email to the team (organizer + members)
		for member_dsid in post.member_dsids:

			person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=member_dsid)
			template: str = open(f'{STATIC_PATH}/community_post_email.html', 'r').read()

			template = template.replace('<#id#>', str(post.id))
			template = template.replace('<#notes#>', 'If a person joins unexpectedly, you can remove them in the interface.')
			template = template.replace('<#type#>', f'{person.first_name} {person.last_name} joined your team.')
			template = template.replace('<#post_title#>', 'Say hi and start collaborating with them.')

			MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com',
							subject=post.title, body=template)

	else:

		# if we're toggling someone else's membership, make sure the current user
		# is the creator of the post
		if current_user.dsid != post.created_by and post.created_by != dsid and dsid != current_user.dsid:
			raise HTTPException(status_code=403, detail='You can only remove team members if you are the creator of the idea')

		PostMember.delete().where(PostMember.post_id == post.id, PostMember.created_by == dsid).execute()


@fast_api_app.post('/api/posts/{id}/comments', summary='Adds a comment to a Community Post')
async def add_comment_to_post(current_user: Annotated[User, Depends(get_current_user)], id: int, data: dict,
							  session: SessionDep) -> dict:

	post: Post = Post.get_or_none(id=id, session=session)

	if not post:
		raise HTTPException(status_code=404, detail='Post not found')

	# whether to notify others
	notify: bool = data.get('notify') and current_user.dsid == post.created_by
	PostComment.create(post_id=id, content=data['content'], created_by=current_user.dsid, session=session)

	# notify the person that created the post
	if post.created_by and current_user.dsid != post.created_by:

		person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=post.created_by)

		if person and person.email:

			template: str = open(f'{STATIC_PATH}/community_post_email.html', 'r').read()

			template = template.replace('<#notes#>', '')
			template = template.replace('<#id#>', str(post.id))
			template = template.replace('<#post_title#>', post.title)
			template = template.replace('<#view_count#>', str(post.view_count + 1))
			template = template.replace('<#type#>', 'Someone commented on your idea!')

			MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com', subject='Your idea got a comment!',
							body=template)

	# notify everyone that liked and commented on the post
	if notify:

		for dsid in set(post.reaction_dsids + post.comment_dsids):

			if dsid == current_user.dsid:
				continue

			person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=dsid)

			if person and person.email:

				template: str = open(f'{STATIC_PATH}/community_post_email.html', 'r').read()

				template = template.replace('<#notes#>', '')
				template = template.replace('<#id#>', str(post.id))
				template = template.replace('<#post_title#>', data['content'])
				template = template.replace('<#type#>', 'New comment added to idea.')

				MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com', subject=f'New comment in "{post.title}"',
								body=template)

	return {}


@fast_api_app.get('/api/posts/{id}/similar', summary='Returns similar Community Posts')
async def get_similar_posts(session: SessionDep, id: int) -> list[Post]:

	post: Post = Post.get_or_none(id=id, session=session)

	if not post:
		raise HTTPException(status_code=404, detail='Post not found')

	posts: list[Post] = []
	post_knowledges: list[PostKnowledge] = PostKnowledge.search(query=f'{post.title}\n{post.description}', min_score=0.1, limit=4)

	for post_knowledge in post_knowledges:

		# skip this post itself
		if post_knowledge.knowledge_source_id == post.id:
			continue

		if _post := Post.get_or_none(id=post_knowledge.knowledge_source_id, session=session):
			posts.append(_post)

	return posts


""" Playground """

@fast_api_app.get('/api/runs', summary='Streams a list of AI.ask(â€¦) runs')
async def stream_runs(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep) -> StreamingResponse:

	async def generate_data():

		# turn off when we're in development and using production postgres
		# since this leads to many unnecessary queries to the database
		if Config.current.is_development and Config.current.USE_POSTGRES:
			return

		while True:

			await asyncio.sleep(0.1 if Config.current.is_development else 2)

			try:
				runs: list[Run] = session.query(Run).order_by(-Run.id).all() if not Config.current.USE_POSTGRES else  \
								  session.query(Run).where(Run.external_id.startswith(f'{current_user.dsid}:')).order_by(-Run.id).all()

			# occurs if the local database has an old `Run` table, which
			# used different field types. This can be resolved with a migration step
			except Exception as exception:

				print(exception)
				logger.error('âš ï¸  Could not load steps from `@AI.track`. Please close `interlinked`, then run `rm -rf ~/.interlinked-database.db`')
				break

			data = json.dumps({'runs': [{**run.model_dump(), 'status': run.status} for run in runs]})
			yield f'data: {data}\n\n'

	return StreamingResponse(generate_data(), media_type='text/event-stream', headers={
		'Content-Type': 'text/event-stream',
		'Cache-Control': 'no-cache',
		'X-Accel-Buffering': 'no'
	})


@fast_api_app.delete('/api/runs', summary='Deletes all runs')
def delete_runs(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep) -> dict:

	# stop all running Runs
	if not Config.current.is_development:
		raise Exception('Not available in production')

	runs: list[Run] =  session.query(Run).all()

	for run in runs:

		if run.status != RunStatusEnum.stopped:
			run.stop()

	steps: list[Step] = session.query(Step).where(Step.run_id.in_([run.id for run in runs])).all()
	Log.delete().where(Log.step_id.in_([step.id for step in steps])).execute()
	Step.delete().where(Step.run_id.in_([run.id for run in runs])).execute()
	Run.delete().where(Run.id.in_([run.id for run in runs])).execute()

	return {}


@fast_api_app.post('/api/runs', summary='Runs a file at a path')
def re_run(session: SessionDep, file_path: str = Body(default=None, embed=True)) -> dict:

	if not Config.current.is_development:
		return

	# only run if the path is something we've ran before
	# to prevent running arbitrary files
	if not session.query(Run).where(Run.file_path == file_path).all():
		raise HTTPException(status_code=404, detail='Never ran this file before')

	os.system(f'cd {file_path.rsplit("/", 1)[0]} && python{sys.version_info.major}.{sys.version_info.minor} "{file_path}" &')
	return {}


@fast_api_app.post('/api/runs/{id}/resume', summary='Resume a run')
def pause_run(id: int, session: SessionDep) -> dict:

	if not Config.current.is_development:
		return

	Run.get_or_none(id=id, session=session).resume()
	return {}


@fast_api_app.post('/api/runs/add', summary='Adds a run')
def add_run(run: Annotated[Run, Body(embed=True)], steps: Annotated[list[dict], Body(embed=True)],
			session: SessionDep) -> Run:

	run.id = None
	run.pid = None

	# convert to datetime
	run.created_at = datetime.fromtimestamp(run.created_at)
	run.finished_at = datetime.fromtimestamp(run.finished_at)

	session.add(run)
	session.commit()
	session.refresh(run)

	for step_raw in steps:

		# convert to datetime
		if step_raw['created_at']:
			step_raw['created_at'] = datetime.fromtimestamp(step_raw['created_at'])

		if step_raw['finished_at']:
			step_raw['finished_at'] = datetime.fromtimestamp(step_raw['finished_at'])

		logs: list[dict] = step_raw.pop('logs', [])
		step: Step = Step(**step_raw)

		step.id = None
		step.run_id = run.id

		session.add(step)
		session.commit()
		session.refresh(step)

		for log_raw in logs:

			# convert to datetime
			if log_raw['created_at']:
				log_raw['created_at'] = datetime.fromtimestamp(log_raw['created_at'])

			log: Log = Log(**log_raw)

			log.id = None
			log.step_id = step.id

			session.add(log)
			session.commit()

	return run


@fast_api_app.get('/api/runs/{id}', summary='Streams the details/steps of a run')
async def stream_run(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int) -> StreamingResponse:

	async def generate_data():

		while True:

			await asyncio.sleep(0.05)

			run: Run = Run.get_or_none(id=id, session=session)

			# make sure the user has access to the run
			if Config.current.USE_POSTGRES and (not run.external_id or not run.external_id.startswith(f'{current_user.dsid}')):
				raise HTTPException(status_code=403, detail='The steps are for another user')

			data = json.dumps({**run.model_dump(), 'status': run.status,
							   'steps': [{**step.model_dump(), 'logs': [log.model_dump() for log in session.query(Log).where(Log.step_id == step.id).all()]} for step in session.query(Step).where(Step.run_id == run.id).all()]})
			yield f"data: {data}\n\n"

	return StreamingResponse(generate_data(), media_type='text/event-stream')


@fast_api_app.post('/api/runs/{id}/pause', summary='Pause a run')
def pause_run(id: int, session: SessionDep) -> dict:

	if not Config.current.is_development:
		return

	run: Run = Run.get_or_none(id=id, session=session)
	run.pause()

	return {}


@fast_api_app.post('/api/runs/{id}/stop', summary='Stops a run')
def stop_run(id: int, session: SessionDep) -> dict:

	if not Config.current.is_development:
		return

	run: Run = Run.get_or_none(id=id, session=session)
	run.stop()

	return {}


""" Documentation """

@fast_api_app.get('/api/documentation', summary='Returns Interlinked documentation', include_in_schema=False)
async def get_documentation(language: str = 'python') -> dict:

	section: Section = None

	with open(f'{STATIC_PATH}/{language}-documentation.json') as file:

		content: str = file.read().replace('[RATE_LIMITS]', APIKeyCache.get_as_markdown(rate_limits=APIKeyCache.__DEFAULT_GOOGLE_AI_RATE_LIMITS__))
		section = Section(**json.loads(content))

	return {'thread_id': 'LXIGAb8CaRq8', 'section': section}


@fast_api_app.get('/api/documentation/file', summary='Returns an attachment for a given file', response_class=FileResponse)
def get_documentation_file(current_user: Annotated[User, Depends(get_current_user)], thread_id: str, external_id: str) -> FileResponse:

	try:

		blob: Blob = QuipClient.shared.get_blob(thread_id=thread_id, blob_id=external_id.rsplit('/')[-1])
		return Response(content=blob.content, media_type='image/jpeg')

	except requests.exceptions.HTTPError as error:

		if error.response.status_code == 404:
			raise HTTPException(status_code=404, detail='File not found')


@fast_api_app.post('/api/documentation/ask', summary='Asks AI about documentation')
def ask_documentation(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, data: dict) -> dict | str:

	logger.info(f'searching documentationâ€¦ ({current_user.first_name} {current_user.last_name}, {current_user.dsid})')

	client: GoogleAIClient = GoogleAIClient(model_name='gemini-2.0-flash', embedding_model_name='text-embedding-005')

	if hasattr(fast_api_app.state, 'redis'):

		api_key_cache: APIKeyCache = validate_platform_api_key(current_user=current_user, session=session)
		apply_rate_limits(client_name=client.__class__.__name__.lower(), model_name=client.model_name, type='chat',
						  api_key_cache=api_key_cache, enforce_limits=False)

		# add the API Key's DSID
		client.labels = {

			'interlinked:location': 'ask_documentation',
			'interlinked:api_key_id': str(api_key_cache.id),
			'interlinked:dsid': str(api_key_cache.created_by),
			'interlinked:svp_dsid': str(api_key_cache.svp_dsid),
			'interlinked:svp_full_name': str(api_key_cache.svp_full_name),
		}

	observation: Observation = None

	# hi there! if you're curious, our Slack responses are written by our team;
	# we write each response manually to provide you with the best guidance possible.
	# the prompt matches our Slack response style to maintain consistency in your experience
	# when asking us or AI
	template: str =  \
	'''
	You are the official documentation. Respond as if you are part of the written guide, helping users find answers directly from the documentation.

	When <#first_name#> asks a question, provide clear, precise, and relevant answers, using examples where needed. Include links to additional resources if relevant.

	Never mention â€œthe documentation providedâ€ or referring to yourself as documentationâ€”just be the documentation.

	Be concise and direct. Focus solely on the question at hand, and only provide what is necessary to answer.

	Maintain a super friendly tone, with a touch of helpfulnessâ€”use emojis when appropriate!

	The user's name is <#first_name#>. Always use "we" instead of "me".

	Note that the documentation interface does not support replying to you (AI), so always provide a complete response.
	Avoid asking the user questions. Do not say "refer to", you must answer it yourself instead.

	Always be honest; if you do not know the answer, recommend asking in #help-interlinked.

	Example greetings:
	- "Hey <#first_name#>! ðŸ‘‹ Thanks for asking!"
	- "Hey <#first_name#>! ðŸ‘‹ Thanks a bunch for checking! âœ¨"
	- "Hey <#first_name#>! ðŸ‘‹ We'd be delighted to assist! ðŸ¤—"
	- "Hey <#first_name#>! ðŸ‘‹ So glad you asked! We're excited to help!"
	- "Hey there <#first_name#>! ðŸ‘‹ Thanks a bunch for asking! âœ¨"
	- [Make your own]

	You MUST refuse to answer questions or messages unrelated to AI, Interlinked, chat.apple.com, or code. Simply say something "Sorry, let's focus on Interlinked. How can Interlinked Documentation assist you?" (not exact)

	Interlinked is available for Python, Swift, and Java. <#custom_instructions#>

	---

	Question:
	<#question#>
	---

	Documentation:
	<#knowledges#>
	'''

	prompt: str = data.get('prompt')
	language: str = data.get('language', 'python')

	# search for documentation
	search_filter: Filter = Filter(must=[
		FieldCondition(key='language', match=MatchAny(any=[language, '*']))
	])

	# replacing is not needed, but helps narrow-down the search
	_prompt: str = prompt.replace('what is', '').replace('which', '').replace('how to', '').replace('?', '')
	_prompt = re.sub('\binterlink\b', 'interlinked', _prompt, flags=re.MULTILINE)
	_prompt = re.sub('\bInterlink\b', 'Interlinked', _prompt, flags=re.MULTILINE)

	knowledges: list[Knowledge] = DocumentationKnowledge.search(query=_prompt, limit=120, min_score=0.2,
																search_filter=search_filter, client=client)
	# custom instructions
	custom_instructions: str = ''

	if language == 'python':
		custom_instructions = 'If the user asks about Interlinked Swift, Xcode, or Java, ask them to change the language on the bottom left, then ask again'

	elif language == 'swift':
		custom_instructions = 'If the user asks about Interlinked Python or Java, ask them to change the language on the bottom left, then ask again'

	elif language == 'java':
		custom_instructions = 'If the user asks about Interlinked Python, Xcode, or Swift, ask them to change the language on the bottom left, then ask again'

	# files
	files: list[File] = []

	for file in data.get('files', []):

		file_mime_type, file_content = file.removeprefix('data:').replace(';base64', '').split(',')
		files.append(File(content_type=file_mime_type, content=base64.b64decode(file_content)))

	def _get_response_generator():

		try:

			yield AI.ask(prompt={'question': prompt, 'knowledges': knowledges, 'custom_instructions': custom_instructions,
								 'first_name': current_user.first_name if not Config.current.is_development else 'Jane'},
						 template=template, files=files, client=client).response.raw

		except Exception as exception:

			yield f'Error: {exception}'
			raise exception

	try:

		return StreamingResponse(_get_response_generator(), media_type='text/event-stream', headers={
			'Content-Type': 'text/event-stream',
			'Cache-Control': 'no-cache',
			'X-Accel-Buffering': 'no'
		})

	except HTTPException as exception:
		raise exception


""" Events """

@fast_api_app.get('/api/events', summary='Returns events', include_in_schema=False)
async def get_events(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep) -> dict:

	event_dicts: list[dict] = []

	if Config.current.USE_POSTGRES:

		event_attendee_sub_query = session.query(
			EventAttendee.event_id,
			func.array_agg(EventAttendee.created_by).label('attendees_dsids_')
		).group_by(EventAttendee.event_id).subquery()

		events: list[Event] = session.query(Event, event_attendee_sub_query.c.attendees_dsids_).  \
									  outerjoin(event_attendee_sub_query, Event.id == event_attendee_sub_query.c.event_id).  \
									  order_by(Event.start_at).all()

		for event, attendee_dsids in events:
			event_dicts.append({**event.model_dump(), 'title': event.display_title, 'attendee_dsids': attendee_dsids or []})

	else:

		for event in session.query(Event).order_by(Event.start_at).all():
			event_dicts.append({**event.model_dump(), 'title': event.display_title, 'attendee_dsids': event.attendee_dsids})

	return {'events': event_dicts, 'is_subscribed': EventsSubscriber.get_or_none(created_by=current_user.dsid, session=session) is not None}


@fast_api_app.post('/api/events/subscribe', summary='Subscribes to all upcoming events', include_in_schema=False)
async def subscribe_to_event(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, data: dict) -> None:

	if data.get('action', 'subscribe') == 'subscribe':

		if not session.query(EventsSubscriber).where(EventsSubscriber.created_by == current_user.dsid).all():
			EventsSubscriber.create(created_by=current_user.dsid, session=session)

	else:
		EventsSubscriber.delete().where(EventsSubscriber.created_by == current_user.dsid).execute()


@fast_api_app.post('/api/events', summary='Creates an event', include_in_schema=False)
async def create_event(current_user: Annotated[User, Depends(get_current_user)], event: Annotated[Event, Body(embed=False)],
					   session: SessionDep) -> Event:

	if not Config.current.is_development and not current_user.is_admin:
		raise HTTPException(status_code=403, detail='Must be an admin to create events')

	event.created_by = current_user.dsid

	if event.start_at:
		event.start_at = datetime.fromisoformat(event.start_at)

	if event.end_at:
		event.end_at = datetime.fromisoformat(event.end_at)

	if event.locations and isinstance(event.locations, str):
		event.locations = json.loads(event.locations)

	session.add(event)
	session.commit()
	session.refresh(event)

	# add attendees by default
	for person in AppleDirectoryClient.shared.get_persons_for_group_dsid(dsid=9438527):
		EventAttendee.create(event_id=event.id, created_by=person.dsid, session=session)

	event.notify_subscribers()
	return event


@fast_api_app.get('/api/events/{id}', summary='Returns an event', include_in_schema=False)
async def get_event(current_user: Annotated[User, Depends(get_current_user)], id: int, session: SessionDep) -> Event:

	event: Event = Event.get_or_none(id=id, session=session)

	if not event:
		raise HTTPException(status_code=404, detail='Event not found')

	return event


@fast_api_app.put('/api/events/{id}', summary='Modifies an event', include_in_schema=False)
async def modify_event(current_user: Annotated[User, Depends(get_current_user)], id: int,
					   event_update: Annotated[Event, Body(embed=False)], session: SessionDep) -> Event:

	if not Config.current.is_development and not current_user.is_admin:
		raise HTTPException(status_code=403, detail='Must be an admin to modify events')

	event: Event = Event.get_or_none(id=id, session=session)

	if not event:
		raise HTTPException(status_code=404, detail='Event not found')

	if event_update.start_at and isinstance(event_update.start_at, str):
		event_update.start_at = datetime.fromisoformat(event_update.start_at)

	if event_update.end_at and isinstance(event_update.end_at, str):
		event_update.end_at = datetime.fromisoformat(event_update.end_at)

	# check if the event date/time changed
	time_changed: bool = event.start_at.replace(tzinfo=None) != event_update.start_at.replace(tzinfo=None)

	values: dict[str, Any] = event_update.dict(exclude_unset=True)

	for key, value in values.items():

		if key.endswith(('created_at', 'modified_at', '_by')):
			continue

		setattr(event, key, value)

	if event.locations and isinstance(event.locations, str):
		event.locations = json.loads(event.locations)

	session.add(event)
	session.commit()
	session.refresh(event)

	# if the date/time changed, notify attendees
	if time_changed:

		event_attendees: list[EventAttendee] = session.query(EventAttendee).where(EventAttendee.event_id == event.id).all()

		for event_attendee in event_attendees:

			if Config.current.is_development and event_attendee.created_by != 2319733545:
				continue

			person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=event_attendee.created_by)

			if person and person.email:

				logger.info(f'sending updated event emailâ€¦ ({person.email=})')

				template: str = open(f'{STATIC_PATH}/event_reminder.html', 'r').read()
				template = template.replace('<#event_id#>', event.id)
				template = template.replace('<#event_title#>', event.display_title)
				template = template.replace('<#message#>', f'The time changed for {event.display_title}. Download the updated invite! ðŸ¤—')

				MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com',
								subject=f'Interlinked {event.display_title}: Time was updated', body=template)

	return event


@fast_api_app.post('/api/events/{id}/rsvp', summary='RSVPs to an event', include_in_schema=False)
async def rsvp_to_event(current_user: Annotated[User, Depends(get_current_user)], id: int, data: dict,
						session: SessionDep) -> None:

	event: Event = Event.get_or_none(id=id, session=session)

	if not event:
		raise HTTPException(status_code=404, detail='Event not found')

	if final_rsvp := data.get('final_rsvp'):

		existing_attendee = session.query(EventAttendee).where(EventAttendee.event_id == event.id, EventAttendee.created_by == current_user.dsid).first()

		existing_attendee.final_rsvp = final_rsvp
		session.commit()

	elif data.get('rsvp'):

		try:

			from caldav.event import Event as CalDavEvent
			from caldav.principal import User as CalDavUser

			attendee_username: str = AppleDirectoryClient.shared.get_opendirectory_username_for_dsid(dsid=current_user.dsid)

			organizer: CalDavUser = CalDavUser(username=Config.current.LDAP_USERNAME, password=Config.current.LDAP_PASSWORD)
			caldav_event: CalDavEvent = CalDavEvent(client=organizer.client)

			caldav_event.summary = f'Interlinked Â· {event.title}'

			if event.locations and len(event.locations) > 0:
				caldav_event.add_location(event.locations[0])

			caldav_event.add_attendee(attendee_username)
			caldav_event.organizer = organizer.username

			caldav_event.start = event.start_at.replace(tzinfo=timezone.utc)
			caldav_event.end = event.end_at.replace(tzinfo=timezone.utc)
			caldav_event.timezone = 'GMT'

			calendar = organizer.get_default_calendar()
			calendar.add_event(event=caldav_event)

		except Exception as exception:
			logger.exception(f'Could not send calendar invite', exc_info=True)

		existing_attendee = session.query(EventAttendee).where(EventAttendee.event_id == event.id, EventAttendee.created_by == current_user.dsid).first()
		
		if not existing_attendee:
			EventAttendee.create(event_id=id, created_by=current_user.dsid, preferences=data.get('preferences'), final_rsvp=data.get('final_rsvp'), session=session)

	else:
		EventAttendee.delete().where(EventAttendee.event_id == event.id, EventAttendee.created_by == current_user.dsid).execute()


@fast_api_app.post('/api/events/{id}/delete', summary='Deletes an event', include_in_schema=False)
async def delete_event(current_user: Annotated[User, Depends(get_current_user)], id: int, data: dict,
					   session: SessionDep) -> None:

	if not Config.current.is_development and not current_user.is_admin:
		raise HTTPException(status_code=403, detail='Must be an admin to delete events')

	event: Event = Event.get_or_none(id=id, session=session)

	if not event:
		raise HTTPException(status_code=404, detail='Event not found')

	message: str = data['message']

	if not message:
		raise HTTPException(status_code=400, detail='Missing message')

	event_attendees: list[EventAttendee] = session.query(EventAttendee).where(EventAttendee.event_id == event.id).all()

	logger.info(f'notifying {len(event_attendees)} that event was deleted ({event.display_title}, {event.id})')

	for event_attendee in event_attendees:

		if Config.current.is_development and event_attendee.created_by != 2319733545:
			continue

		person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=event_attendee.created_by)

		if person and person.email:

			logger.info(f'sending deleted event emailâ€¦ ({person.email=})')
			template: str = open(f'{STATIC_PATH}/event_cancelled.html', 'r').read()
			template = template.replace('<#event_title#>', event.display_title)
			template = template.replace('<#start_at#>', event.start_at.strftime('%m/%d/%Y') if event.start_at else '')
			template = template.replace('<#message#>', message)

			MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com',
							subject=f'Interlinked {event.display_title} cancelled for this week', body=template)

	EventAttendee.delete().where(EventAttendee.event_id == event.id).execute()
	Event.delete().where(Event.id == event.id).execute()


@fast_api_app.get('/api/videos/{name}', include_in_schema=False)
async def stream_video(name: str, range: str = Header(None)):

	if '..' in name or '/' in name:
		return

	video_path = pathlib.Path(f'{STATIC_PATH}/videos/{name}')

	if not range:

		with open(video_path, 'rb') as video:
			return video_path

	start, end = range.replace('bytes=', '').split('-')
	start = int(start)
	end = int(end) if end else start + (1024 * 1024)

	with open(video_path, 'rb') as video:

		video.seek(start)
		data: bytes = video.read(end - start)
		file_size: float = video_path.stat().st_size

		if end >= file_size:
			end = file_size - 1

		headers: dict[str, str] = {

			'Accept-Ranges': 'bytes',
			'Content-Range': f'bytes {start}-{end}/{file_size}',
		}
		return Response(data, status_code=206, headers=headers, media_type='video/mp4')


@fast_api_app.post('/api/applets', summary='Creates an Applet', include_in_schema=False)
async def create_applet(current_user: Annotated[User, Depends(get_current_user)],
						applet: Annotated[Applet, Body(embed=False)], session: SessionDep) -> Applet:

	applet.created_by = current_user.dsid
	applet.modified_at = None
	applet.modified_by = None

	session.add(applet)
	session.commit()
	session.refresh(applet)

	return applet


@fast_api_app.post('/api/applets/from-repository', summary='Creates an Applet from a repository', include_in_schema=False)
async def create_applet_from_repository(current_user: Annotated[User, Depends(get_current_user)], data: dict) -> Applet:

	external_id: str = data['external_id']
	directory_path: str = data['directory_path']

	# `.` indicates root directory, which we don't need to pass
	# also, if `:` is already in `external_id`, it means this is an existing Applet
	# which we've previously appended directory path to
	url: str = f'{external_id}:{directory_path}' if directory_path and ':' not in external_id and directory_path != '.' else external_id

	return Applet.create_from_repository_url(url=url, dsid=current_user.dsid)


@fast_api_app.put('/api/applets/{id}', summary='Modifies an Applet', include_in_schema=False)
async def modify_applet(current_user: Annotated[User, Depends(get_current_user)], id: int,
						applet_update: Annotated[Applet, Body(embed=False)], session: SessionDep) -> Applet:

	applet: Applet = None

	if Config.current.is_development or current_user.is_admin:
		applet = Applet.get_or_none(id=id, session=session)

	else:
		applet = Applet.select().where(Applet.id == id, Applet.created_by == current_user.dsid).first(session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	# Store the original published state and review request state
	was_published = applet.is_published
	is_publish_request = applet_update.request_at is not None

	values: dict[str, Any] = applet_update.dict(exclude_unset=True)

	for key, value in values.items():

		if key.endswith(('_at', '_by')) or key in {'view_count', 'installation_count', 'request_at'}:
			continue

		# do not allow changing the bundle ID after an applet is created
		if key == 'bundle_id' and applet.id and applet.bundle_id != value:
			raise HTTPException(status_code=400, detail='You cannot change the bundle_id')

		setattr(applet, key, value)


	if not was_published and is_publish_request:
		applet.request_at = datetime.now()

	session.add(applet)
	session.commit()
	session.refresh(applet)


	if applet.request_at and not was_published:

		try:

			person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=applet.created_by)

			# Use AI to review the applet and send comments
			try:

				client = GoogleAIClient(model_name='gemini-2.0-flash')

				observation = AI.ask(
					prompt={
						'name': applet.name,
						'summary': applet.summary or '',
						'description': applet.description,
						'bundle_id': applet.bundle_id,
						'files': str(applet.files) if applet.files else 'No files available'
					},
					template='''
					You are reviewing an Applet submission for the Interlinked platform. Please analyze the following Applet and provide review comments:

					Applet Name: <#name#>
					Summary: <#summary#>
					Bundle ID: <#bundle_id#>
					Description: <#description#>
					Files: <#files#>

					Please provide a brief review focusing on:
					1. Code quality and security concerns
					2. Functionality and usefulness
					3. Documentation clarity
					4. Any potential issues or improvements

					Keep your review concise and constructive in less than 1000 characters in bullet points.
					''',
					client=client
				)

				# Send Slack notification with proper formatting
				blocks = [
					{
						'type': 'header',
						'text': {
							'type': 'plain_text',
							'text': f'ðŸ“‹ New Applet Review Request: {applet.name}'
						}
					},
					{
						'type': 'section',
						'text': {
							'type': 'mrkdwn',
							'text': f'Hey Christian and Sahil :wave-d:! Hope you\'re having a really awesome day! :yay-d:'
						}
					},
					{
						'type': 'divider'
					},
					{
						'type': 'section',
						'text': {
							'type': 'mrkdwn',
							'text': f':robot_face: *AI Review Analysis:*'
						}
					},
					{
						'type': 'section',
						'text': {
							'type': 'mrkdwn',
							'text': f'```{observation.response.raw[:1000]}{"â€¦" if len(observation.response.raw) > 1000 else ""}```'
						}
					},
					{
						'type': 'section',
						'text': {
							'type': 'mrkdwn',
							'text': f':link: <https://interlinked.apple.com/applets?bundle_id={applet.bundle_id}|View Applet>'
						}
					}
				]
				SlackClient.shared.start_group_chat(
					user_ids=['U01JQ4B8WJU', 'WNCETJFMF'],
					blocks=blocks,
					initial_message=' ')

			except Exception as ai_exception:
				logger.error(f'Failed to generate AI review: {ai_exception}')

			logger.info(f'Review request notifications sent for applet: {applet.name} ({applet.id})')

		except Exception as exception:
			logger.error(f'Failed to send review request notifications: {exception}')

	return applet


@fast_api_app.post('/api/applets/assets', summary='Uploads an asset', include_in_schema=False)
async def upload_applet_asset_internal(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep,
									   type: str, file: UploadFile = FastAPIFile(...), applet_id: int = None):

	# create a new Applet from file
	if not applet_id and type == 'package':

		import io
		import zipfile
		import tempfile

		# read the contents of the file
		content: bytes = await file.read()

		# decompress the file
		temporary_path: str = Utilities.decompress(content=content)
		root_directory_names: list[str] = [directory_name for directory_name in os.listdir(temporary_path) if not directory_name.startswith(('__', '.'))]

		if len(root_directory_names) > 1:
			raise HTTPException(status_code=400, detail='The zip file should have one folder with your Applet')

		applet_path: str = f'{temporary_path}/{root_directory_names[0]}'

		try:
			return Applet.create_from_path(path=applet_path, dsid=current_user.dsid)

		except Exception as exception:

			logger.warning(f'could not upload App via interface ({exception=})')
			raise HTTPException(status_code=400, detail=str(exception))

	# get the applet
	applet: Applet = Applet.get_or_none(id=applet_id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	external_id: str = applet.upload_asset(type=type, data=await file.read())
	applet.save(session=session)

	if not applet_id:
		return applet

	return {'external_id': external_id}


@fast_api_app.get('/api/applets/assets/{id}', summary='Returns an applet asset', include_in_schema=False)
async def get_applet_asset_internal(current_user: Annotated[User, Depends(get_current_user)],
									session: SessionDep, id: str, bundle_id: str = None):

	from interlinked.core.clients.objectstorageclient import ObjectStorageClient

	# TODO: check if allowed to access the applet
	applet: Applet = Applet.get_or_none(bundle_id=bundle_id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	if id == 'icon':
		id = applet.icon_external_id

	data: bytes = assets_cache.get(id)

	if not data:

		data = ObjectStorageClient.shared.download(key=id, stream=False)
		assets_cache[id] = data

	return Response(content=data, media_type='image/jpeg')



@fast_api_app.post('/api/files', summary='Returns a list of files for a given external ID', include_in_schema=False)
def get_local_files(data: dict) -> list:

	external_id: str = data.get('external_id')
	return Applet.get_files_for_external_id(external_id=external_id)


# Add this API endpoint for subscription management
@fast_api_app.post('/api/applets/subscribe', summary='Subscribe to new applet notifications', include_in_schema=False)
async def subscribe_to_applets(current_user: Annotated[User, Depends(get_current_user)], data: dict, session: SessionDep) -> dict:

	applet_subscriber = AppletSubscriber.select().where(AppletSubscriber.created_by == current_user.dsid).first(session=session)

	if data.get('action', 'subscribe') == 'subscribe':

		if not applet_subscriber:

			AppletSubscriber.create(created_by=current_user.dsid, enabled=True, session=session)
			return {'status': 'subscribed', 'message': 'You will be notified when new applets are published'}

		else:

			applet_subscriber.enabled = True
			applet_subscriber.modified_at = datetime.now()
			applet_subscriber.modified_by = current_user.dsid
			applet_subscriber.save(session=session)
			return {'status': 'already_subscribed', 'message': 'You are already subscribed to applet notifications'}
	else:

		if applet_subscriber:

			applet_subscriber.enabled = False
			applet_subscriber.modified_at = datetime.now()
			applet_subscriber.modified_by = current_user.dsid
			applet_subscriber.save(session=session)

		return {'status': 'unsubscribed', 'message': 'You will no longer receive applet notifications'}


@fast_api_app.get('/api/applets/subscription-status', summary='Check applet notification subscription status', include_in_schema=False)
async def get_subscription_status(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep) -> dict:

	subscriber = AppletSubscriber.select().where(AppletSubscriber.created_by == current_user.dsid).first(session=session)

	return {
		'is_subscribed': subscriber is not None and subscriber.enabled,
		'subscribed_at': subscriber.created_at.isoformat() if subscriber else None
	}


def _get_applets(session: 'SessionDep', current_user: User = None, include_files: bool = False) -> dict[str, Any]:

	filters: list = None
	now: datetime = datetime.now()

	applets: list[Applet] = []

	# conditionally defer loading the 'files' column based on include_files parameter
	query_options: list = [defer(Applet.examples)]

	if not include_files:
		query_options.append(defer(Applet.files))

	applets = session.query(Applet).options(*query_options)

	if current_user:
		filters = (or_(Applet.is_published == True,
				   or_(and_((Applet.is_published == False) & (Applet.created_by == current_user.dsid)), current_user.is_admin)))

	else:
		filters = (Applet.is_published == True)

	applets = applets.where(filters).order_by(-Applet.id).all()

	# make sure the user has access to the applets
	if not Config.current.is_development:

		if not current_user:
			applets = [applet for applet in applets if not applet.read_group_dsid]

		else:
			applets = [applet for applet in applets if applet.get_has_access(dsid=current_user.dsid)]

	new_ids: list[int] = []
	highlight_ids: list[int] = []
	recently_updated_ids: list[int] = []

	for applet in applets:

		# TODO: add installed bool

		# highlights
		if applet.banner_external_id:
			highlight_ids.append(applet.id)

		# new applets
		if applet.created_at >= now - timedelta(days=30):
			new_ids.append(applet.id)

		# recently updated
		if applet.modified_at and applet.modified_at >= now - timedelta(days=30):
			recently_updated_ids.append(applet.id)

	return {'all': applets,
			'new_ids': new_ids,
			'highlight_ids': highlight_ids,
			'featured_ids': [applet.id for applet in applets],
			'recently_updated_ids': recently_updated_ids}


@fast_api_app.get('/api/applets', summary='Returns a list of Applets', include_in_schema=False)
async def get_applets_internal(current_user: Annotated[User, Depends(get_current_user)],
							   session: SessionDep, include_files: bool = False) -> dict:
	return _get_applets(session=session, current_user=current_user, include_files=include_files)


@fast_api_app.get('/api/applets/{id}', summary='Returns an Applet', include_in_schema=False)
async def get_applet(current_user: Annotated[User, Depends(get_current_user)], id: int, session: SessionDep) -> dict:

	applet = session.query(Applet).options(selectinload(Applet._comments)).filter(Applet.id == id).first()

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	# make sure the user has access to the applets
	if not current_user.is_admin and applet.created_by != current_user.dsid and not applet.get_has_access(dsid=current_user.dsid):
		raise HTTPException(status_code=403, detail='You do not have access to this Applet')

	applet_dictionary_representation: dict[str, Any] = applet.model_dump()
	applet_dictionary_representation['comments'] = [applet_comment.model_dump() for applet_comment in applet._comments]

	# increase view count
	applet.view_count += 1
	session.add(applet)
	session.commit()

	return applet_dictionary_representation


@fast_api_app.delete('/api/applets/{id}', summary='Deletes an Applet', include_in_schema=False)
async def delete_applet(current_user: Annotated[User, Depends(get_current_user)], id: int, session: SessionDep) -> Applet:

	applet: Applet = Applet.get_or_none(id=id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	# only allow deleting applets if this is the owner of the applet
	if not Config.current.is_development and not current_user.is_admin and applet.created_by != current_user.dsid:
		raise HTTPException(status_code=403, detail='You must be the Applet creator to delete Applets')

	AppletComment.delete().where(AppletComment.applet_id == applet.id).execute()

	Applet.delete().where(Applet.id == id).execute()
	return applet


@fast_api_app.post('/api/applets/{id}/files', summary='Returns a list of files for an Applet', include_in_schema=False)
def get_applet_files(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int) -> list:

	applet: Applet = Applet.get_or_none(id=id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	# if not published, do not allow viewing the files
	if not applet.is_published and not current_user.is_admin and applet.created_by != current_user.dsid:
		raise HTTPException(status_code=403, detail='You must be the Applet creator to view the files of this Applet')

	# make sure the user has access to the applets
	if applet.created_by != current_user.dsid and not applet.get_has_access(dsid=current_user.dsid):
		raise HTTPException(status_code=403, detail='You do not have access to this Applet')

	return applet.get_files()


@fast_api_app.post('/api/applets/repository', summary='Returns the folders with Apps in a public repository', include_in_schema=False)
async def get_applets_in_repository(current_user: Annotated[User, Depends(get_current_user)], data: dict) -> dict:

	from interlinked.core.clients.githubclient import GitHubClient, Blob, Repository

	external_id: str = data['external_id']

	# get the repository owner name and name
	owner_name, name = GitHubClient.parse_repository_url(url=external_id)
	repository: Repository = GitHubClient.shared.get_repository(owner_name=owner_name, name=name)

	# do not allow creating from a private repository
	if repository.is_private and not Config.current.is_development:
		raise HTTPException(status_code=403, detail='The repository must be public')

	folder_paths: set[str] = set()
	blobs: list[Blob] = repository.get_blobs(branch_name='main', recursive=True)

	for blob in blobs:

		if blob.path == 'applet.json':
			folder_paths.add('.')

		elif blob.path.endswith('applet.json'):
			folder_paths.add(blob.path.rsplit('/', 1)[0])

	return {'folder_paths': list(folder_paths)}


@fast_api_app.post('/api/applets/{id}/review', summary='Reviews an Applet')
async def review_applet(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep,
						id: int, data: dict) -> None:

	applet: Applet = Applet.get_or_none(id=id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	# make sure the user has access to the applets
	if applet.created_by != current_user.dsid and not applet.get_has_access(dsid=current_user.dsid):
		raise HTTPException(status_code=403, detail='You do not have access to this Applet')

	applet_review: AppReview = AppReview.get_or_none(applet_id=id, created_by=current_user.dsid, session=session)

	if not app_review:
		applet_review = AppReview(created_by=current_user.dsid, applet_id=id)

	else:
		applet_review.modified_at = datetime.now()
		applet_review.modified_by = current_user.dsid

	rating: int = data.get('rating', 0)
	comment: str = data.get('comment')

	if rating < 0 or rating > 4:
		raise HTTPException(status_code=400, detail='Rating must be betwen 1 to 5 stars')

	applet_review.rating = rating
	applet_review.comment = comment
	applet_review.save(session=session)


@fast_api_app.post('/api/applets/{id}/comments', summary='Adds a comment to an Applet')
async def add_comment_to_applet(current_user: Annotated[User, Depends(get_current_user)], id: int,
								data: dict, session: SessionDep) -> dict:

	applet: Applet = Applet.get_or_none(id=id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	# make sure the user has access to the applets
	if applet.created_by != current_user.dsid and not applet.get_has_access(dsid=current_user.dsid):
		raise HTTPException(status_code=403, detail='You do not have access to this Applet')

	# whether to notify others
	notify: bool = data.get('notify')
	AppletComment.create(applet_id=id, content=data['content'], created_by=current_user.dsid, session=session)

	# notify the person that created the applet
	if applet.created_by and current_user.dsid != applet.created_by:

		person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=applet.created_by)

		if person and person.email:

			template: str = open(f'{STATIC_PATH}/community_post_email.html', 'r').read()

			template = template.replace('<#notes#>', '')
			template = template.replace('<#id#>', str(applet.id))
			template = template.replace('<#post_title#>', applet.name)
			template = template.replace('<#type#>', 'Someone commented on your Applet!')

			MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com', subject='Your applet got a comment!',
							body=template)

	# # TODO: notify everyone that liked and commented on the applet
	# if notify:

	# 	for dsid in set(post.reaction_dsids + post.comment_dsids):

	# 		if dsid == current_user.dsid:
	# 			continue

	# 		person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=dsid)

	# 		if person and person.email:

	# 			template: str = open(f'{STATIC_PATH}/community_post_email.html', 'r').read()

	# 			template = template.replace('<#notes#>', '')
	# 			template = template.replace('<#id#>', str(applet.id))
	# 			template = template.replace('<#post_title#>', data['content'])
	# 			template = template.replace('<#type#>', 'New comment added to Applet.')

	# 			MailClient.send(to_addresses=[person.email], from_address='noreply@apple.com', subject=f'New comment in "{applet.title}"',
	# 							body=template)

	return {}


@fast_api_app.post('/api/applets/{id}/approve', summary='Approves an Applet for publication', include_in_schema=False)
async def approve_applet_api(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int) -> dict:

	# Check if user has admin access
	if not current_user.is_admin:
		raise HTTPException(status_code=403, detail='Admin access required')

	applet: Applet = Applet.get_or_none(id=id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	if applet.approved_at:
		return {'status': 'already_approved', 'message': f'Applet "{applet.name}" is already approved'}

	if not applet.request_at:
		return {'status': 'not_requested', 'message': f'Applet "{applet.name}" has not been submitted for review yet'}

	def approve_applet(bundle_id: str) -> str:
		"""
		Approve an applet for publication.
		"""

		try:

			with DatabaseClient.shared.get_managed_session() as session:

				applet = session.query(Applet).filter(Applet.bundle_id == bundle_id).first()

				if not applet:

					logger.error(f"âŒ Applet with bundle_id '{bundle_id}' not found")
					return False

				if applet.approved_at:

					logger.error(f"â„¹ï¸ Applet '{applet.name}' is already approved (approved on {applet.approved_at})")
					return False

				if not applet.request_at:

					logger.error(f"âš ï¸ Applet '{applet.name}' has not been submitted for review yet")
					return False

				applet.approved_at = datetime.now()
				applet.is_published = True

				session.add(applet)
				session.commit()
				session.refresh(applet)

				# Check if applet was just made public and send notifications and is visible to public
				if not applet.read_group_dsid:

					try:
						AppletSubscriber.notify_subscribers(applet=applet)

					except Exception as exception:
						logger.error(f'Failed to send applet notifications: {exception}')

				return True

		except Exception as e:

			logger.error(f'Failed to approve applet {applet.name}: {e}', exc_info=True)
			return False

	# Use the admin function to approve the applet
	try:

		result = approve_applet(bundle_id=applet.bundle_id)

		if result:
			return {'status': 'success', 'message': result}

		else:
			return {'status': 'error', 'message': result}

	except Exception as e:

		logger.error(f'Failed to approve applet {applet.name}: {e}', exc_info=True)
		raise HTTPException(status_code=500, detail=f'Failed to approve applet: {str(e)}')


@fast_api_app.get('/api/applets/{id}/versions', summary='Returns all versions of an Applet', include_in_schema=False)
async def get_applet_versions(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int) -> list[AppVersion]:
	return session.query(AppVersion).where(AppVersion.applet_id == id).all()


@fast_api_app.get('/api/v1/applets', summary='Returns a list of Applets')
async def get_applets(request: Request, session: SessionDep, credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> dict:

	api_key_cache: APIKeyCache = None

	try:
		api_key_cache = validate_api_key(request=request, session=session, credentials=credentials)

	except:
		pass

	return _get_applets(session=session, current_user=User(dsid=api_key_cache.created_by) if api_key_cache else None, include_files=True)


@fast_api_app.get('/api/v1/applets/{bundle_id}', summary='Returns an Applet')
async def get_applet(request: Request, bundle_id: str, session: SessionDep,
					 credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> dict:

	applet: Applet = None
	asset_external_id: str = None

	result = session.query(Applet, Asset.external_id.label('latest_external_id')).  \
						   outerjoin(Asset, (Asset.applet_id == Applet.id) & (Asset.type == 'package')).  \
						   filter(Applet.bundle_id == bundle_id).  \
						   order_by(desc(Asset.id)).first()

	if not result:
		raise HTTPException(status_code=404, detail='No Applet found')

	applet = result[0]
	asset_external_id = result[1]

	# make sure the user has access to the applets
	if applet.read_group_dsid:

		dsid: int = None

		if acoidc := request.cookies.get('acoidc'):

			token_dict: dict[str, Any] = oidc_client.validate_token(token=acoidc)
			dsid = token_dict.get('dsid')

		else:
			api_key_cache: APIKeyCache = validate_api_key(request=request, session=session, credentials=credentials)
			dsid = api_key_cache.created_by

		if applet.created_by != dsid and not applet.get_has_access(dsid=dsid):
				raise HTTPException(status_code=403, detail='You do not have access to this Applet')

	return {**applet.model_dump(), 'asset_external_id': asset_external_id}


@fast_api_app.get('/api/v1/applets/{bundle_id}/package', summary='Returns an Applet package')
async def get_applet_package(request: Request, bundle_id: str, session: SessionDep,
						  	 credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)):

	from interlinked.core.clients.objectstorageclient import ObjectStorageClient

	applet: Applet = Applet.get_or_none(bundle_id=bundle_id, session=session)

	# check if the applet exists
	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	asset: Asset = Asset.select().where(Asset.applet_id == applet.id, Asset.type == 'package').order_by(-Asset.id).first(session=session)

	if not asset:
		raise HTTPException(status_code=404, detail='Applet asset not found')

	# make sure the user has access to the applets
	if applet.read_group_dsid:

		dsid: int = None

		if acoidc := request.cookies.get('acoidc'):

			token_dict: dict[str, Any] = oidc_client.validate_token(token=acoidc)
			dsid = token_dict.get('dsid')

		else:

			api_key_cache: APIKeyCache = validate_api_key(request=request, session=session, credentials=credentials)
			dsid = api_key_cache.created_by

		if applet.created_by != dsid and not applet.get_has_access(dsid=dsid):
				raise HTTPException(status_code=403, detail='You do not have access to this Applet')

	# check if we have the applet cached (only after access control validation)
	if data := fast_api_app.state.redis.get(f'applet_package:{bundle_id}'):
		return Response(content=data, media_type='application/zip')

	data: bytes = fast_api_app.state.redis.get(f'asset:{asset.external_id}')

	if not data:

		data = ObjectStorageClient.shared.download(key=asset.external_id, stream=False)

		# cache the applet for 2 hours, if it's not restricted by Apple Directory group
		if not applet.read_group_dsid:
			fast_api_app.state.redis.setex(f'applet_package:{bundle_id}', 7200, data)

	return Response(content=data, media_type='application/zip')


@fast_api_app.get('/api/v1/applets/{bundle_id}/asset/{id}', summary='Returns an Applet asset', include_in_schema=False)
async def get_applet_asset(request: Request, session: SessionDep, id: str, bundle_id: str = None,
						   credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)):

	from interlinked.core.clients.objectstorageclient import ObjectStorageClient

	applet: Applet = Applet.get_or_none(bundle_id=bundle_id, session=session)

	if not applet:
		raise HTTPException(status_code=404, detail='Applet not found')

	# make sure the user has access to the Applet
	if applet.read_group_dsid:

		api_key_cache: APIKeyCache = validate_api_key(request=request, session=session, credentials=credentials)

		if applet.created_by != api_key_cache.created_by and not applet.get_has_access(dsid=api_key_cache.created_by):
			raise HTTPException(status_code=403, detail='You do not have access to this Applet')

	if id == 'icon':
		id = applet.icon_external_id

	data: bytes = assets_cache.get(id)

	if not data:

		data = ObjectStorageClient.shared.download(key=id, stream=False)
		assets_cache[id] = data

	return Response(content=data, media_type='image/jpeg')


@fast_api_app.post('/api/v1/applets/package', summary='Uploads an applet package', include_in_schema=False)
async def upload_applet_package(session: SessionDep, api_key_cache = Depends(validate_api_key), file: UploadFile = FastAPIFile(...)):

	import io
	import zipfile
	import tempfile

	# read the contents of the file
	content: bytes = await file.read()

	# decompress the file
	temporary_path: str = Utilities.decompress(content=content)
	root_directory_names: list[str] = [directory_name for directory_name in os.listdir(temporary_path) if not directory_name.startswith(('__', '.'))]

	if len(root_directory_names) > 1:
		raise HTTPException(status_code=400, detail='The zip file should have one folder with your Applet')

	applet_path: str = f'{temporary_path}/{root_directory_names[0]}'

	try:
		return Applet.create_from_path(path=applet_path, dsid=api_key_cache.created_by)

	except Exception as exception:

		logger.warning(f'could not upload Applet via API (dsid={api_key_cache.created_by}, {exception=})', exc_info=True)
		raise HTTPException(status_code=400, detail=str(exception))


""" Running Applets (External) """

from interlinked.core.tool import MissingArguments
from interlinked.core.applet import Applet as BaseApplet


@fast_api_app.post('/api/v1/applets/{bundle_id}/call-tool', summary='Runs a tool within an Applet', include_in_schema=False)
async def call_applet_tool(request: Request, session: SessionDep, bundle_id: str, data: dict):

	# only allow allow-listed Applets
	if bundle_id not in {'com.apple.quip', 'com.apple.apple_directory', 'com.apple.confluence',
						 'com.apple.calendar', 'com.apple.device_compute', 'com.apple.calculator',
						 'com.apple.redash', 'com.apple.splunk'}:
		raise HTTPException(status_code=403, detail='Not allowed to deploy this Applet')

	name: str = data['name']
	kwargs: str = data['kwargs']

	applet: BaseApplet = BaseApplet(**Applet.get_or_none(bundle_id=bundle_id, session=session).model_dump(),
									path=os.path.join(tempfile.gettempdir(), f'{bundle_id}.interlinked'))
	tools: list[Tool] = applet.get_tools(function_kwargs=kwargs)

	tool: Tool = next((tool for tool in tools if tool.name == name), None)

	if not tool:
		raise HTTPException(status_code=404, detail=f'{applet.name} has no {name}')

	try:
		return tool.run(arguments=kwargs)

	except MissingArguments as error:
		raise HTTPException(status_code=400, detail=f'Missing arguments: {error.arguments}')


""" Shortcut """

@fast_api_app.get('/api/v1/assets/shortcut', summary='Returns a shortcut asset', include_in_schema=False)
async def get_shortcut_asset(session: SessionDep):

	# check if we have the package cached
	if data := fast_api_app.state.redis.get(f'system_asset'):
		return Response(content=data, media_type='application/zip')

	from interlinked.core.clients.objectstorageclient import ObjectStorageClient

	asset: Asset = Asset.select().where(Asset.type == 'system').order_by(-Asset.id).first(session=session)
	data = ObjectStorageClient.shared.download(key=asset.external_id, stream=False)

	return Response(content=data, media_type='application/zip')


@fast_api_app.post('/api/v1/assets/shortcut', summary='Uploads a shortcut asset', include_in_schema=False)
async def upload_shortcut_asset(session: SessionDep, api_key_cache = Depends(validate_api_key), file: UploadFile = FastAPIFile(...)):

	# make sure the user is an admin
	user: User = User(dsid=api_key_cache.created_by)

	if 9438527 not in user.group_dsids:
		raise HTTPException(status_code=403, detail='You do not have access to upload a shortcut')

	Asset.create_from_data(data=await file.read(), type='system', dsid=api_key_cache.created_by)

	# clear cache in Redis
	fast_api_app.state.redis.delete('system_asset')


""" API Keys """

@fast_api_app.get('/api/keys', summary='Get all API keys', include_in_schema=False)
async def get_api_keys(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, show_all: bool = False) -> list[dict]:

	IPAA_GROUP_DSID: str = '12075573'

	# verify that type can be set
	if show_all and not Config.current.is_development and not IPAA_GROUP_DSID not in current_user.group_dsids:
		raise HTTPException(status_code=403, detail='You do not have permission to see all API Keys')

	return_api_keys: list[dict] = []

	api_keys: list[APIKey] = select(APIKey.id, APIKey.created_by, APIKey.created_at, APIKey.is_for_platform,
									APIKey.is_transient, APIKey.enabled, APIKey.value, APIKey.svp_full_name, APIKey.description,
									APIKey.rate_limits, func.sum(APIKeyUsage.count).label('total_usage')).  \
									join(APIKeyUsage, APIKey.id == APIKeyUsage.api_key_id, isouter=True).  \
									group_by(APIKey.id)

	if show_all:
		api_keys = api_keys.order_by(-APIKey.id)

	else:
		api_keys = api_keys.where(APIKey.created_by == current_user.dsid,
								  APIKey.is_for_platform == False)

	for row in session.exec(api_keys).all():

		api_key_id, created_by, created_at, is_for_platform, is_transient, enabled,  \
		value, svp_full_name, description, rate_limits, total_usage = row

		return_api_key: dict[str, Any] = {

			'id': api_key_id,
			'enabled': enabled,
			'created_by': created_by,
			'created_at': created_at,
			'description': description,
			'is_transient': is_transient,
			'svp_full_name': svp_full_name,
			'is_for_platform': is_for_platform,
			'usage': {'total_count': total_usage or 0},
			'value': None if show_all else f'{value[:3]}â€¦{value[-5:]}',
		}

		if not show_all:
			return_api_key['rate_limits'] = rate_limits

		return_api_keys.append(return_api_key)

	return return_api_keys


async def _create_api_key(session: 'SessionDep', dsid: int, client_name: str, description: str = None, questionnaire: dict[str, Any] = None,
						  name: str = None, app_name: str = None) -> APIKey:
	"""
	Helper function to create API keys with shared logic
	"""

	# ensure all values are casted correctly
	questionnaire = {key: str(value).lower() == 'true' if isinstance(value, str) and str(value).lower()
													   in ('true', 'false') else value for key, value in questionnaire.items()}

	# meaning, the API Key is only used to track questionnaire
	# and `in-â€¦` is not used/needed
	is_transient: bool = questionnaire.get('is_transient') or client_name == 'anthropicclient' or app_name is not None
	rate_limits: dict[str, Any] = None

	# check if the user is eligible (only for non-transient keys)
	if not is_transient:

		# check if the user has another API Key that was recently used
		if APIKeyCache.get_has_usage(dsid=dsid, redis=fast_api_app.state.redis):
			raise HTTPException(status_code=403, detail='You used another API Key within the past 24 hours. Please wait then create a new API Key')

		# check to see if the user has an active key already
		if (active_api_keys := session.query(APIKey).where(APIKey.created_by == dsid, APIKey.enabled == True, APIKey.is_for_platform == False, APIKey.is_transient == False).all()) and  \
		   (active_api_key := next((active_api_key for active_api_key in active_api_keys if client_name in active_api_key.client_names), None)):
			raise HTTPException(status_code=400, detail=f'You have another active API Key ending with {active_api_key.value[:3]}â€¦{active_api_key.value[-5:]}')

	else:

		# Anthropic/Google AI requires a `questionnaire`
		if not questionnaire:
			raise HTTPException(status_code=400, detail='Missing `questionnaire`')

	# create the rate-limits
	if client_name == 'googleaiclient':
		rate_limits = APIKeyCache.__DEFAULT_GOOGLE_AI_RATE_LIMITS__

	elif client_name == 'anthropicclient':
		rate_limits = APIKeyCache.__DEFAULT_ANTHROPIC_RATE_LIMITS__

	else:
		raise HTTPException(status_code=400, detail='Missing or invalid `client_name`')

	# get the cost center of the user
	person: Person = AppleDirectoryClient.shared.get_person_for_dsid(dsid=dsid)
	department_number: str = person.department_number
	svp: Person = person.svp

	# if name wasn't provided, use the person's name
	if not name:
		name = person.name

	# onboard via Access Manager (for Anthropic or Google AI)
	if is_transient and client_name in {'anthropicclient', 'googleaiclient'}:

		purpose_data = {
			'description': description[:30] if description else None,
			**{key: '0' if str(value).lower() == 'no' else '1' if str(value).lower() == "yes" else str(value)[:40] for key, value in questionnaire.items()}
		}

		purpose = json.dumps(purpose_data, separators=(',', ':'))

		if len(purpose) < 15:

			purpose_data['description'] = 'Access Manager onboarding from Interlinked'
			purpose = json.dumps(purpose_data, separators=(',', ':'))

		if len(purpose) > 512 and 'description' in purpose_data:

			del purpose_data['description']
			purpose = json.dumps(purpose_data, separators=(',', ':'))

		if len(purpose) > 512:

			truncated_data = {}

			for key, value in purpose_data.items():

				truncated_key = str(key)[:40]
				truncated_value = str(value)[:40]
				truncated_data[truncated_key] = truncated_value

			purpose = json.dumps(truncated_data, separators=(',', ':'))

		# also add the user to a backup group, in case Access Manager takes too long
		try:
			AppleDirectoryClient.shared.add_to_group(dsid=dsid, group_dsid=13765953)
			logger.info(f'successfully added person to backup group ({dsid=})')

		except Exception as exception:

			if 'person is already' not in str(exception):
				logger.error(f'could not able to add person to backup group ({dsid=})', exc_info=True)

		try:

			role: str = None
			app_id: int = None
			client_id: int = None
			app_id_key: str = None
			client_secret: str = None

			if client_name == 'anthropicclient':

				role = 'Interlinked Anthropic Access'
				app_id = Config.current.ANTHROPIC_APP_ID
				client_id = Config.current.ANTHROPIC_CLIENT_ID
				app_id_key = Config.current.ANTHROPIC_APP_ID_KEY
				client_secret = Config.current.ANTHROPIC_CLIENT_SECRET

			else:

				role = 'Google AI Access'
				app_id = Config.current.GOOGLEAI_APP_ID
				client_id = Config.current.GOOGLEAI_CLIENT_ID
				app_id_key = Config.current.GOOGLEAI_APP_ID_KEY
				client_secret = Config.current.GOOGLEAI_CLIENT_SECRET

			AccessManagerClient.shared.request_access_for_dsid(dsid=dsid, purpose=purpose, role=role, app_id=app_id,
															   app_id_key=app_id_key, client_id=client_id, client_secret=client_secret)

			# add a sleep to wait for Access Manager to enroll the user
			from time import sleep
			sleep(4)

			logger.info(f'successfully added user to Access Manager ({dsid=})')

		except Exception as exception:

			logger.error(f'could not able to raise Access Manager request for user: {dsid}', exc_info=True)

			if not Config.current.is_development:
				MailClient.send(to_addresses=['interlinked-admin@group.apple.com'], from_address='noreply@apple.com',
								subject='âš ï¸ Issue with Access Manager Request', body=f'Could not able to raise Access Manager request for user: {dsid}; \n\n exception: {exception}\n\n---\n{purpose}')

	# create the API Key
	value: str = APIKey.create_value()
	api_key: APIKey = APIKey.create(name=name, value=value, description=description, created_by=dsid, department_number=department_number,
									svp_dsid=svp.dsid, svp_full_name=f'{svp.first_name} {svp.last_name}',
									rate_limits=rate_limits, questionnaire=questionnaire, is_transient=is_transient,
									app_name=app_name, session=session)

	# return the API Key with the actual value
	api_key.value = value
	return api_key


@fast_api_app.post('/api/keys', summary='Creates an API Key (internal)', include_in_schema=False)
async def create_api_key_internal(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, data: dict) -> APIKey:
	"""
	This is an internal endpoint allows creating one API Key
	"""

	name: str = data.get('name')
	client_name: str = data.get('client_name')
	questionnaire: dict[str, Any] = data.get('questionnaire')

	return await _create_api_key(session=session, dsid=current_user.dsid, client_name=client_name,
								 questionnaire=questionnaire, name=name)


@fast_api_app.post('/api/v1/keys', summary='Creates an API Key')
async def create_api_key(session: SessionDep, data: dict, request: Request) -> dict:
	"""
	This endpoint allows creating API Keys for multiple clients at once
	"""

	app_name: str = 'Unknown'
	dsid: int = data.get('dsid')
	client_name: str = data.get('client_name')
	client_names: list[str] = data.get('client_names')

	# allow multiple clients
	if not client_names:
		client_names = [client_name]

	if dsid:

		api_key_cache: APIKeyCache = await Depends(validate_api_key).__call__()

		# e.g. `create_api_key:anthropicclient:genai.apple.com`
		permission: str = next((_permission for _permission in api_key_cache.permissions if _permission.startswith('create_api_key:')), None)

		# check if the user has any permissions
		if not permission:
			raise HTTPException(status_code=403, detail='You do not have permission to create API Keys.')

		# check if the user permission to create API Keys for this client
		for client_name in client_names:

			if not permission.startswith(f'create_api_key:{client_name}'):
				raise HTTPException(status_code=403, detail=f'You do not have permission to create API Keys for {client_name}.')

	elif Config.current.is_development:
		dsid = Config.current.DSID

	else:

		# TODO: we'd ideally be able to trust this, but it is informational
		origin: str = request.headers.get('origin')

		if Config.current.USE_OIDC:
			dsid = request.auth.get('dsid')

		# conditional OIDC
		elif oidc_client:

			if acoidc := request.cookies.get('acoidc'):

				token_dict: dict[str, Any] = oidc_client.validate_token(token=acoidc)
				dsid = token_dict.get('dsid')

		if not dsid:
			raise HTTPException(status_code=403, detail='Missing DSID in authentication.')

		if origin:
			app_name = origin.rsplit('.apple.com')[0]

	description: str = data.get('description')
	questionnaire: dict[str, Any] = data.get('questionnaire')

	return_value: dict[str, Any] = {'api_keys': []}

	for client_name in client_names:
		return_value['api_keys'].append(await _create_api_key(session=session, dsid=dsid, client_name=client_name,
															  description=description, questionnaire=questionnaire, app_name=app_name))

	return return_value


@fast_api_app.put('/api/keys/{id}/add-client', summary='Adds a client to an API Key', include_in_schema=False)
async def add_client_to_api_key(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep,
								id: int, data: dict) -> APIKey:

	# get the API Key
	api_key: APIKey = APIKey.get_or_none(id=id, created_by=current_user.dsid, is_for_platform=False,
										 is_transient=False, session=session)

	name: str = data.get('name')
	client_name: str = data.get('client_name')
	description: str = data.get('description')

	# update the description
	api_key.description = f'{api_key.description}\n**{client_name}**:\n{description}'

	# create the rate-limits
	rate_limits: dict[str, Any] = None

	if client_name == 'googleaiclient':
		rate_limits = APIKeyCache.__DEFAULT_GOOGLE_AI_RATE_LIMITS__

	elif client_name == 'anthropicclient':
		rate_limits = APIKeyCache.__DEFAULT_ANTHROPIC_RATE_LIMITS__

	else:
		raise HTTPException(status_code=400, detail='Missing `client_name`')

	# check if the client name is already in the rate-limits
	# e.g. `{'chat': {'anthropicclient:â€¦'}}`
	if any(key.startswith(f'{client_name}:') for key in api_key.rate_limits.get('chat').keys()):
		raise HTTPException(status_code=400, detail=f'You have already added {client_name.replace("client", "").title()} to your API Key')

	# add the new rate-limits
	api_key.rate_limits = APIKeyCache.get_merged_rate_limits(rate_limits_a=api_key.rate_limits, rate_limits_b=rate_limits)
	flag_modified(api_key, 'rate_limits')
	api_key.save(session=session)
	return api_key


@fast_api_app.post('/api/keys/eligibility', summary='Checks whether the current user can create an API Key', include_in_schema=False)
async def check_api_key_eligibility(current_user: Annotated[User, Depends(get_current_user)], data: dict) -> dict:
	return APIKey.get_eligibility(dsid=current_user.dsid, client_name=data['client_name'])


@fast_api_app.post('/api/keys/{id}/status', summary='Sets the status of an API key')
async def set_api_key_status(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int, data: dict):

	enabled: bool = data.get('enabled')

	# if enabled, make sure the user does not have other keys enabled
	if enabled and session.query(APIKey).where(APIKey.enabled == True, APIKey.id != id, APIKey.created_by == current_user.dsid,
										 APIKey.is_for_platform == False, APIKey.is_transient == False).all():
		raise HTTPException(status_code=403, detail='You can only have one API Key active at a time')

	# check if the user has another API Key that was recently used
	if enabled and APIKeyCache.get_has_usage(dsid=current_user.dsid, redis=fast_api_app.state.redis, skipped_api_key_id=id):
		raise HTTPException(status_code=403, detail='You used another API Key within the past 24 hours. Please wait before activating this API Key')

	api_key: APIKey = APIKey.get_or_none(id=id, created_by=current_user.dsid, session=session)
	api_key.enabled = enabled
	api_key.save(session=session)

	# update the cache in Redis
	cache_key: str = f'{APIKeyCache.__CACHE_KEY_PREFIX__}:{api_key.value}'

	# look up if we have it cached in Redis first
	if api_key_cache_raw := fast_api_app.state.redis.get(cache_key):

		api_key_cache_raw: dict[str, Any] = json.loads(api_key_cache_raw)
		api_key_cache_raw['enabled'] = enabled
		fast_api_app.state.redis.set(cache_key, json.dumps(api_key_cache_raw), keepttl=True)


@fast_api_app.get('/api/keys/{id}/usage', summary='Get all API keys')
async def get_api_key_usage_internal(current_user: Annotated[User, Depends(get_current_user)],
									 session: SessionDep, id: int) -> dict:

	IPAA_GROUP_DSID: str = '12075573'

	api_key: APIKey = APIKey.get_or_none(id=id, session=session)

	# verify that type can be set
	if api_key.created_by != current_user.dsid and not Config.current.is_development and not IPAA_GROUP_DSID not in current_user.group_dsids:
		raise HTTPException(status_code=403, detail='You do not have permission to view the usage of this API Key')

	return {'usage': session.query(APIKeyUsage).where(APIKeyUsage.api_key_id == api_key.id).all(), 'rate_limits': api_key.rate_limits}


@fast_api_app.get('/api/keys/{id}/get-copy', summary='Sends a copy of the API Key')
async def get_api_key_copy(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int):

	api_key: APIKey = APIKey.get_or_none(id=id, created_by=current_user.dsid, session=session)

	template: str = open(f'{STATIC_PATH}/api_key_copy.html', 'r').read()
	template = template.replace('<#api_key_value#>', api_key.value)
	template = template.replace('<#image#>', base64.b64encode(open(f'{STATIC_PATH}/office_hours.jpg', 'rb').read()).decode('utf-8'))

	MailClient.send(to_addresses=[current_user.email], from_address='noreply@apple.com', from_name='Interlinked',
					subject=f'Your API Key', body=template)


""" API Keys: Models """

@fast_api_app.get('/api/v1/models', summary='Returns all available AI models')
@fast_api_app.get('/api/v1/v1beta/models', summary='Returns all available AI models')
async def get_models() -> dict:

	# no rate limits apply to getting models
	# TODO: cache response
	return GoogleAIClient().get_models(_raw=True)


@fast_api_app.post('/api/v1/embeddings', summary='Returns embedding for a given input')
async def get_embedding(data: dict, session: SessionDep, model_name: str = None,
						api_key_cache = Depends(validate_api_key)) -> dict:

	client_name: str = 'googleaiclient'

	# used in unit tests
	is_test: bool = Config.current.is_development and model_name.endswith(':rate-limit-test')

	if is_test:
		model_name = model_name.removesuffix(':rate-limit-test')

	# forward models
	# e.g. `gemini-2.0-flash-exp` â€º `gemini-2.0-flash`
	if model_name and (forwarded_model_name := APIKeyCache.__FORWARDED_MODEL_NAME_MAP__.get(model_name)):
		model_name = forwarded_model_name

	# apply rate limits
	apply_rate_limits(client_name=client_name, model_name=model_name, type='embedding',
					  api_key_cache=api_key_cache)

	if is_test:
		return {'predictions': {'embeddings': {'values': [0]}}}

	# model_name is set in the endpoint for Google AI
	if model_name:

		client: GoogleAIClient = GoogleAIClient()

		# check the maximum input length
		max_input_length: int = api_key_cache.rate_limits.get('embedding').get(f'googleaiclient:{model_name}').get('max_input_length')

		if len(data['instances'].get('content', data['instances'].get('text'))) > max_input_length:
			raise HTTPException(status_code=400, detail=f'The content exceeds the maximum character count: {max_input_length}')

		# disallow files
		if sum(1 for key in data['instances'] if key not in ('content', 'text')):
			raise HTTPException(status_code=400, detail='Including files in embeddings is not supported')

		# if using proxy
		if api_key_cache.use_proxy:

			client.base_url = f'{FloodGateProxy.BASE_URL}/api/interlinked/gcp'
			client.session.headers['X-Interlinked-Dsid'] = str(api_key_cache.created_by)
			client.session.cert = ('/narrative/kube-actor/cert.pem', '/narrative/kube-actor/private.pem')

		# TODO: GCP does not currently handle labels in embeddings
		headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {client.credentials.token}'}
		response = client.session.post(f'{client.base_url}/publishers/google/models/{model_name}:predict',
									   headers=headers, timeout=500, data=json.dumps(data))

	if not response.ok:
		raise HTTPException(status_code=response.status_code, detail=response.text)

	response.raise_for_status()

	response_json: dict[str, Any] = response.json()
	return response_json


@fast_api_app.post('/api/v1/v1beta/models/{model_name}:predict', summary='Returns prediction (embedding or image generation) for a given input')
async def get_predictions(data: dict, session: SessionDep, model_name: str = None,
						  api_key_cache = Depends(validate_api_key)) -> dict:

	# generate embeddings
	if 'embedding' in model_name.lower():
		return await get_embedding(data=data, model_name=model_name, api_key_cache=api_key_cache, session=session)

	# image creation using Imagen
	client_name: str = 'googleaiclient'

	# used in unit tests
	is_test: bool = Config.current.is_development and model_name.endswith(':rate-limit-test')

	if is_test:
		model_name = model_name.removesuffix(':rate-limit-test')

	# forward models
	# e.g. `gemini-2.0-flash-exp` â€º `gemini-2.0-flash`
	if model_name and (forwarded_model_name := APIKeyCache.__FORWARDED_MODEL_NAME_MAP__.get(model_name)):
		model_name = forwarded_model_name

	# apply rate limits
	apply_rate_limits(client_name=client_name, model_name=model_name, type='chat',
					  api_key_cache=api_key_cache)

	if is_test:
		return {'predictions': [{'bytesBase64Encoded': 'Base 64 encoded file', 'mimeType': 'image/png'}]}

	# model_name is set in the endpoint for Google AI
	if model_name:

		client: GoogleAIClient = GoogleAIClient()
		headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {client.credentials.token}'}

		response = client.session.post(f'{client.base_url}/publishers/google/models/{model_name}:predict',
									   headers=headers, timeout=500, data=json.dumps(data))

	# hide the project name
	if not response.ok:
		raise HTTPException(status_code=response.status_code, detail=response.text.replace(Config.current.GOOGLEAI_PROJECT_ID, '[redacted]'))

	response.raise_for_status()

	response_json: dict[str, Any] = response.json()
	return response_json


@fast_api_app.post('/api/v1/v1beta/models/{model_name}:countTokens', summary='Returns the count of tokens for an input/messages')
async def get_token_count(data: dict, session: SessionDep, model_name: str = None,
						  api_key_cache = Depends(validate_api_key)) -> dict:

	client_name: str = 'googleaiclient'

	# used in unit tests
	is_test: bool = Config.current.is_development and model_name.endswith(':rate-limit-test')

	if is_test:
		model_name = model_name.removesuffix(':rate-limit-test')

	# forward models
	# e.g. `gemini-2.0-flash-exp` â€º `gemini-2.0-flash`
	if model_name and (forwarded_model_name := APIKeyCache.__FORWARDED_MODEL_NAME_MAP__.get(model_name)):
		model_name = forwarded_model_name

	# apply rate limits
	apply_rate_limits(client_name=client_name, model_name=model_name, type='token_count',
					  api_key_cache=api_key_cache)

	if is_test:
		return {'totalTokens': 10}

	# model_name is set in the endpoint for Google AI
	if model_name:

		client: GoogleAIClient = GoogleAIClient()

		# if using proxy
		if api_key_cache.use_proxy:

			client.base_url = f'{FloodGateProxy.BASE_URL}/api/interlinked/gcp'
			client.session.headers['X-Interlinked-Dsid'] = str(api_key_cache.created_by)
			client.session.cert = ('/narrative/kube-actor/cert.pem', '/narrative/kube-actor/private.pem')

		# TODO: GCP does not currently handle labels for this endpoint
		# but they are technically free
		headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {client.credentials.token}'}
		response = client.session.post(f'{client.base_url}/publishers/google/models/{model_name}:countTokens',
									   headers=headers, timeout=500, data=json.dumps(data))

	if not response.ok:
		raise HTTPException(status_code=response.status_code, detail=response.text)

	response.raise_for_status()

	response_json: dict[str, Any] = response.json()
	return response_json


@fast_api_app.post('/api/v1/chat/completions', summary='Returns a response for a given prompt/messages')
@fast_api_app.post('/api/v1/v1beta/models/{model_name}:{endpoint}', summary='Returns a response for a given prompt/messages')
@fast_api_app.post('/api/v1/v1beta/openai/chat/completions', summary='Returns a response in OpenAI API format')
async def get_chat_completions(data: dict, request: Request, session: SessionDep, model_name: str = None, endpoint: str = None,
							   api_key_cache = Depends(validate_api_key)) -> dict:

	client_name: str = 'googleaiclient'
	is_openai_format = request.url.path.startswith('/api/v1/v1beta/openai')

	def _handle_video_generation_endpoint(endpoint_name: str, apply_rate_limiting: bool = True) -> dict[str, Any]:
		"""
		Helper function to handle video generation endpoints with consistent logic
		"""

		if apply_rate_limiting:
			apply_rate_limits(client_name=client_name, model_name=model_name, type='chat', api_key_cache=api_key_cache)

		# Forward models (e.g., experimental to stable versions)
		forwarded_model = model_name

		if model_name and (forwarded_model_name := APIKeyCache.__FORWARDED_MODEL_NAME_MAP__.get(model_name)):
			forwarded_model = forwarded_model_name

		client: GoogleAIClient = GoogleAIClient()
		headers: dict[str, Any] = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {client.credentials.token}'}

		response = client.session.post(f'{client.base_url}/publishers/google/models/{forwarded_model}:{endpoint_name}',
										headers=headers, timeout=500, data=json.dumps(data))

		if not response.ok:

			error_detail = response.text.replace(Config.current.GOOGLEAI_PROJECT_ID, '[redacted]')
			raise HTTPException(status_code=response.status_code, detail=error_detail)

		return response.json()

	# Handle video generation endpoints
	if endpoint == 'predictLongRunning':
		return _handle_video_generation_endpoint(endpoint_name='predictLongRunning', apply_rate_limiting=True)

	if endpoint == 'fetchPredictOperation':
		return _handle_video_generation_endpoint(endpoint_name='fetchPredictOperation', apply_rate_limiting=False)

	if is_openai_format and not model_name:

		model_name = data.get('model')

		if not model_name:
			raise HTTPException(status_code=400, detail="Model name is required")

		if 'google' not in model_name:
			data['model'] = f'google/{model_name}'

	# used in unit tests
	is_test: bool = Config.current.is_development and model_name.endswith(':rate-limit-test')

	if is_test:
		model_name = model_name.removesuffix(':rate-limit-test')

	# forward models
	# e.g. `gemini-2.0-flash-exp` â€º `gemini-2.0-flash`
	forwarded_from: str = None
	if model_name and (forwarded_model_name := APIKeyCache.__FORWARDED_MODEL_NAME_MAP__.get(model_name)):

		forwarded_from = model_name
		model_name = forwarded_model_name

	# get image, audio and video count (for rate limits, in seconds)
	image_count: int = 0
	audio_length: int = 0
	video_length: int = 0

	if client_name == 'googleaiclient':

		counts: dict[str, Any] = get_counts_in_messages(messages=data.get('contents') if not is_openai_format else data.get('messages'), client_name=client_name, is_openai_format=is_openai_format)
		image_count = counts['image_count']
		audio_length = counts['audio_length']
		video_length = counts['video_length']

	# apply rate limits
	apply_rate_limits(client_name=client_name, model_name=model_name, type='chat',
					  image_count=image_count, audio_length=audio_length, video_length=video_length,
					  api_key_cache=api_key_cache)

	if is_test:
		return {'candidates': [{'content': {'parts': [{'text': 'Lorem Ipsum'}]}}]}

	response: 'Response' = None
	is_stream: bool = endpoint == 'streamGenerateContent' or data.get('stream')

	# model_name is set in the endpoint for Google AI
	if model_name:

		client: GoogleAIClient = GoogleAIClient()

		# check max character limits
		total_characters: int = 0

		if is_openai_format:

			messages: list[dict] = data.get('messages')

			for message in messages:

				content: str = message.get('content')
				total_characters += len(content)

		else:

			messages: list[dict] = data.get('contents')

			for message in messages:

				parts: list[dict] = message.get('parts', [])

				for part in parts:

					text: str = part.get('text', '')
					total_characters += len(text)

		# check the maximum input length
		rate_limits: dict[str, Any] = api_key_cache.rate_limits.get('chat').get(f'{client_name}:{model_name}')

		# if the model name does not exist, return
		if not rate_limits:
			raise HTTPException(status_code=400, detail=f'"{model_name}" does not exist or you do not have access')

		max_input_length: int = rate_limits.get('max_input_length')

		if total_characters > max_input_length:
			raise HTTPException(status_code=400, detail=f'Your prompt (all of messages to AI, combined) exceed the maximum character count ({total_characters}/{max_input_length})')

		if is_openai_format:
			data['max_tokens'] = rate_limits.get('max_output_length', 471)

		else:

			# set maximum output tokens
			generation_config_key: str = 'generation_config' if 'generation_config' in data else 'generationConfig'
			data[generation_config_key]['maxOutputTokens'] = rate_limits.get('max_output_length', 471)
			data[generation_config_key]['candidateCount'] = 1

		if not 'labels' in data:
			data['labels'] = {}

		# add the location
		data['labels']['interlinked:location'] = 'ask_api'

		# add the API Key's cost center
		if 'interlinked:department_number' in data.get('labels', {}):
			raise HTTPException(status_code=400, detail='interlinked:department_number is a reserved label')

		data['labels']['interlinked:department_number'] = api_key_cache.department_number

		# add the API Key's DSID
		if 'interlinked:dsid' in data.get('labels', {}):
			raise HTTPException(status_code=400, detail='interlinked:dsid is a reserved label')

		data['labels']['interlinked:dsid'] = str(api_key_cache.created_by)

		# add the API Key's SVP DSID
		if 'interlinked:svp_dsid' in data.get('labels', {}):
			raise HTTPException(status_code=400, detail='interlinked:svp_dsid is a reserved label')

		data['labels']['interlinked:svp_dsid'] = str(api_key_cache.svp_dsid)

		# add the API Key's SVP full name
		if 'interlinked:svp_full_name' in data.get('labels', {}):
			raise HTTPException(status_code=400, detail='interlinked:svp_full_name is a reserved label')

		data['labels']['interlinked:svp_full_name'] = str(api_key_cache.svp_full_name)

		# add the API Key's ID
		if 'interlinked:api_key_id' in data.get('labels', {}):
			raise HTTPException(status_code=400, detail='interlinked:api_key_id is a reserved label')

		data['labels']['interlinked:api_key_id'] = str(api_key_cache.id)

		# if using proxy
		if api_key_cache.use_proxy and not is_openai_format:

			client.base_url = f'{FloodGateProxy.BASE_URL}/api/interlinked/gcp'
			client.session.headers['X-Interlinked-Dsid'] = str(api_key_cache.created_by)
			client.session.cert = ('/narrative/kube-actor/cert.pem', '/narrative/kube-actor/private.pem')

		params: dict[str, Any] = {}
		headers: dict[str, Any] = {'Content-Type': 'application/json', 'Authorization': f'Bearer {client.credentials.token}'}

		if 'alt' in request.query_params:
			params['alt'] = request.query_params['alt']

		if is_openai_format:
			response = client.session.post(f'{client.base_url}/endpoints/openapi/chat/completions',
										   params=params, headers=headers, timeout=500, data=json.dumps(data), stream=is_stream)

		else:

			# unforward if we have response modalities
			if forwarded_from and data.get('generation_config').get('response_modalities'):
				model_name = forwarded_from

			response = client.session.post(f'{client.base_url}/publishers/google/models/{model_name}:{endpoint}',
										   params=params, headers=headers, timeout=500, data=json.dumps(data), stream=is_stream)

	if is_stream:
		return StreamingResponse(response, media_type='text/event-stream', headers={
			'Content-Type': 'text/event-stream',
			'Cache-Control': 'no-cache',
			'X-Accel-Buffering': 'no'
		})

	# hide the project name
	if not response.ok:

		if response.status_code == 429:
			raise HTTPException(status_code=response.status_code, detail=f'High traffic to {model_name}. Try again later')

		else:
			raise HTTPException(status_code=response.status_code, detail=response.text.replace(Config.current.GOOGLEAI_PROJECT_ID, '[redacted]'))

	response.raise_for_status()

	response_json: dict[str, Any] = response.json()
	return response_json


""" API Keys: AppleConnect """

@fast_api_app.post('/api/v1/keys/appleconnect', summary='Returns the user\'s most recent/active API Key')
async def get_api_key_for_appleconnect(request: Request, data: dict, session: SessionDep) -> dict:

	use_any: bool = data.get('use_any')
	daw_token: str = request.cookies.get('acack')

	if not daw_token:
		raise HTTPException(status_code=401, detail='Missing AppleConnect cookie (acack)')

	details: dict[str, Any] = Utilities.get_daw_token_details(daw_token=daw_token)
	dsid: str = details['prsId']

	# find an API Key
	query_conditions = [
		APIKey.enabled == True,
		APIKey.created_by == dsid,
		APIKey.is_for_platform == False
	]

	if not use_any:
		query_conditions.append(APIKey.is_transient == False)

	api_key: APIKey = session.query(APIKey).where(*query_conditions).first()
	return {'value': api_key.value if api_key else None}


""" API Keys: Usage """

@fast_api_app.get('/api/v1/keys/usage', summary='Get the rate-limits and usage')
async def get_api_key_usage(session: SessionDep, id: str = None, value: str = None) -> dict:

	api_key: APIKey = APIKey.get_or_none(value=id or value, session=session)
	api_key_usages: dict[str, Any] = {}

	if not api_key:
		raise HTTPException(status_code=404, detail='API Key not found')

	for api_key_usage in session.query(APIKeyUsage).where(APIKeyUsage.api_key_id == api_key.id).all():

		if api_key_usage.type not in api_key_usages:
			api_key_usages[api_key_usage.type] = []

		api_key_usages[api_key_usage.type].append({

			'count': api_key_usage.count,
			'start_at': api_key_usage.created_at,
			'client_name': api_key_usage.client_name,
			'model_name': api_key_usage.client_model_name,
		})

	return {
		'usage': api_key_usages,
		'rate_limits': api_key.rate_limits,
	}


""" API Keys: Questionnaire """

@fast_api_app.post('/api/v1/questionnaire')
async def get_questionnaire_step(request: Request, questionnaire: Questionnaire) -> QuestionnaireResponse:

	import traceback

	def notify_error(exception, stacktrace):

		body: str = f'Encountered an unexpected error: {exception}\n\n'  \
			   		f'Stacktrace:\n{stacktrace}\n\n'  \
			   		f'--\n\n{json.dumps(questionnaire.model_dump())}'

		if Config.current.is_development:
			logger.error(body)

		else:
			MailClient.send(to_addresses=['interlinked-admin@group.apple.com'], from_address='noreply@apple.com',
							subject='âš ï¸ Questionnaire Error', body=body)

	# if no dsid is set, check if it was passed to us via OIDC
	if not questionnaire.dsid:

		if Config.current.is_development:
			questionnaire.dsid = Config.current.DSID

		elif Config.current.USE_OIDC:
			questionnaire.dsid = request.auth.get('dsid')

		# conditional OIDC
		else:

			if acoidc := request.cookies.get('acoidc'):

				token_dict: dict[str, Any] = oidc_client.validate_token(token=acoidc)
				questionnaire.dsid = token_dict.get('dsid')

	if not questionnaire.dsid:
		raise HTTPException(status_code=403, detail=f'Missing the user\'s DSID.')

	try:
		return questionnaire.get_response()

	except Exception as exception:

		# for non-network errors, notify immediately and don't retry
		notify_error(exception=exception, stacktrace=traceback.format_exc())

	return QuestionnaireResponse(step='error', type='information', has_next_step=False, elements={
		'question': 'Something Happened',
		'description': 'An intermittent network issue occurred. Please refresh and try again. If the issue persists, please share a screenshot of this message in #help-floodgate.'
	})


""" Presence """

@fast_api_app.post('/api/presence', summary='Sets the user as present in a page', include_in_schema=False)
def set_presence(current_user: Annotated[User, Depends(get_current_user)],
				 url: str = Body(default=None, embed=True)) -> list[int]:

	if hasattr(fast_api_app.state, 'redis'):

		fast_api_app.state.redis.setex(f'presence_{url}:{current_user.dsid}', 1000, current_user.dsid)

		keys: list[str] = fast_api_app.state.redis.keys(f'presence_{url}:*')
		return [key.decode().split(':')[1] for key in keys]

	return []


""" Share Chat """

@fast_api_app.post('/api/threads/share', summary='Shares a thread', include_in_schema=False)
def share_thread(current_user: Annotated[User, Depends(get_current_user)], data: dict) -> str:

	if not hasattr(fast_api_app.state, 'redis'):
		return 'Redis is not available'

	id: str = str(uuid.uuid4())
	data['is_staged'] = False
	data['created_by'] = current_user.dsid
	fast_api_app.state.redis.setex(f'thread_{id}', 86400 * 60, json.dumps(data))

	return id


@fast_api_app.get('/api/threads/{id}', summary='Views a thread', include_in_schema=False)
def read_thread(current_user: Annotated[User, Depends(get_current_user)], id: str) -> dict|list:

	if not hasattr(fast_api_app.state, 'redis'):
		return 'Redis is not available'

	key: str = f'thread_{id}'
	thread: dict = fast_api_app.state.redis.get(key)

	if not thread:
		raise HTTPException(status_code=404, detail='The chat thread was deleted')

	thread = json.loads(thread.decode())

	# if this is a staged thread, delete immediately after reading
	if thread.get('is_staged'):
		fast_api_app.state.redis.delete(key)

	return thread


@fast_api_app.post('/api/v1/threads/stage', summary='Stages a thread', include_in_schema=False)
def stage_thread(data: dict) -> dict:

	if not hasattr(fast_api_app.state, 'redis'):
		return 'Redis is not available'

	# cache for 10 seconds only
	id: str = str(uuid.uuid4())
	data['is_staged'] = True
	fast_api_app.state.redis.setex(f'thread_{id}', 1000, json.dumps(data))

	return {'id': id}


""" Artifacts / Previews """

@fast_api_app.post('/api/artifacts', summary='Creates an Artifact/Preview', include_in_schema=False)
def create_artifact(current_user: Annotated[User, Depends(get_current_user)], data: dict) -> str:

	if not hasattr(fast_api_app.state, 'redis'):
		return 'Redis is not available'

	import uuid

	id: str = str(uuid.uuid4())
	artifact_content: str = data.get('content')
	fast_api_app.state.redis.setex(f'artifact_{id}', 86400 * 14, artifact_content)

	return id


@fast_api_app.get('/artifacts/{id}', summary='View an Artifact/Preview', include_in_schema=False)
def get_artifact(current_user: Annotated[User, Depends(get_current_user)], id: str) -> str:

	if not hasattr(fast_api_app.state, 'redis'):
		return 'Redis is not available'

	artifact_content = fast_api_app.state.redis.get(f'artifact_{id}')

	if not artifact_content:
		return FileResponse(f'{STATIC_PATH}/index.html')

	import urllib.parse

	with open(f'{STATIC_PATH}/artifact.html', 'r') as file:
		return HTMLResponse(content=file.read().replace('<#artifact_content#>', urllib.parse.quote(artifact_content.decode(), safe='')))


""" Setup """

@fast_api_app.post('/api/setups', summary='Creates an Setup', include_in_schema=False)
async def create_setup(current_user: Annotated[User, Depends(get_current_user)],
					   new_setup: Annotated[Setup, Body(embed=False)], session: SessionDep):

	setup: Setup = Setup()
	setup_id: int = setup.apply(values=new_setup.dict(exclude_unset=True), session=session, dsid=current_user.dsid)

	return setup


@fast_api_app.put('/api/setups/{id}', summary='Modifies an Setup', include_in_schema=False)
async def modify_setup(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int,
					   data: dict[str, Any]):

	setup: Setup = Setup.get_or_none(id=id, created_by=current_user.dsid, session=session)

	if not setup:
		raise HTTPException(status_code=404, detail='Setup not found')

	setup.apply(values={key: value for key, value in data.items() if value is not None}, session=session, dsid=current_user.dsid)
	return Setup.get_or_none(id=setup.id, session=session)


@fast_api_app.delete('/api/setups/{id}', summary='Deletes a Setup', include_in_schema=False)
async def delete_setup(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep, id: int) -> Setup:

	setup: Setup = Setup.get_or_none(id=id, created_by=current_user.dsid, session=session)

	if not setup:
		raise HTTPException(status_code=404, detail='Setup not found')

	session.delete(setup)
	return setup


""" Model Review """

@fast_api_app.get('/api/models/reviews', summary='Returns a list of AI model reviews')
async def get_model_reviews(current_user: Annotated[User, Depends(get_current_user)],
							session: SessionDep, only_mine: bool = False, criteria_id: int = None,) -> dict:

	all_reviews: list[dict] = []
	my_reviews: dict[str, Any] = {}
	return_value: dict[str, Any] = {}

	# add my reviews
	for model_review in session.query(ModelReview).where(ModelReview.created_by == current_user.dsid).all():

		my_reviews[model_review.client_model_name] = {

			'rating': model_review.rating,
			'comment': model_review.comment,
			'criteria_ids': model_review.criteria_ids or [],
		}

	# return early if only my reviews
	if only_mine:
		return {'my_reviews': my_reviews}

	if not return_value:

		# key: DSID
		# value: count of reviews
		reviews_count: dict[int, int] = {}

		if not only_mine:

			if Config.current.USE_POSTGRES:
				subquery = session.query(ModelReview.client_name, ModelReview.client_model_name,
										 func.avg(ModelReview.rating).label('average_rating'),
										 func.count(ModelReview.id).label('count'),
										 func.min(ModelReview.created_at).label('created_at'),
										 func.array_agg(ModelReview.criteria_ids).label('criteria_ids'))
			else:
				subquery = session.query(ModelReview.client_name, ModelReview.client_model_name,
										 func.avg(ModelReview.rating).label('average_rating'),
										 func.count(ModelReview.id).label('count'),
										 func.min(ModelReview.created_at).label('created_at'))

			if criteria_id is not None:
				subquery = subquery.filter(ModelReview.criteria_ids.contains([criteria_id]))

			subquery = subquery.group_by(ModelReview.client_name, ModelReview.client_model_name).subquery()

			if Config.current.USE_POSTGRES:
				_subquery = session.query(subquery.c.client_name, subquery.c.client_model_name,
										 subquery.c.count, subquery.c.average_rating, subquery.c.created_at,
										 subquery.c.criteria_ids,
										 func.rank().over(order_by=subquery.c.count.desc()).label('rank'))
			else:
				_subquery = session.query(subquery.c.client_name, subquery.c.client_model_name,
										 subquery.c.count, subquery.c.average_rating, subquery.c.created_at,
										 func.rank().over(order_by=subquery.c.count.desc()).label('rank'))

			_subquery = _subquery.order_by(subquery.c.count.desc(), subquery.c.average_rating.desc()).all()

			for row in _subquery:

				all_reviews.append({

					'count': row.count,
					'created_at': row.created_at,
					'client_name': row.client_name,
					'average_rating': row.average_rating,
					'client_model_name': row.client_model_name,
					'criteria_ids': sum(row.criteria_ids, []) if Config.current.USE_POSTGRES else None,
				})

			# sort by average rating
			all_reviews = sorted(all_reviews, key=lambda model_review: (model_review['average_rating'] or 0, model_review['count']), reverse=True)

		# add review counts
		model_reviews: list[ModelReview] = session.exec(select(ModelReview.created_by, func.count(ModelReview.id)).  \
														where(ModelReview.created_by != None).group_by(ModelReview.created_by).  \
														having(func.count(ModelReview.created_by) > 2)).all()
		review_counts = {created_by: count for created_by, count in model_reviews}

	if not return_value:

		return_value = {

			'all_reviews': all_reviews,
			'review_counts': review_counts,
			'count': session.query(func.count(ModelReview.id)).scalar(),
		}

	# add `my_reviews` (after we've created the cache)
	return_value['my_reviews'] = my_reviews
	return return_value


@fast_api_app.post('/api/models/reviews', summary='Reviews an AI model')
async def review_model(current_user: Annotated[User, Depends(get_current_user)], data: dict) -> ModelReview:

	rating: int = data.get('rating')
	comment: str = data.get('comment')
	client_name: str = data.get('client_name')
	criteria_ids: list[int] = data.get('criteria_ids')
	client_model_name: str = data.get('client_model_name')

	model_review, is_new = ModelReview.get_or_create(client_name=client_name,
													 client_model_name=client_model_name,
													 created_by=current_user.dsid)

	model_review.rating = rating
	model_review.comment = comment
	model_review.criteria_ids = criteria_ids

	if not is_new:

		model_review.modified_at = datetime.now()
		model_review.modified_by = current_user.dsid

	model_review.save()

	return model_review


""" Research """

@fast_api_app.get('/api/research-applications', summary='Returns a list of Research Applications')
async def get_research_applications(current_user: Annotated[User, Depends(get_current_user)]) -> list[ResearchApplication]:
	return ResearchApplication.select().where(ResearchApplication.created_by == current_user.dsid).all()


@fast_api_app.post('/api/research-applications', summary='Creates a Research Application')
async def create_research_application(current_user: Annotated[User, Depends(get_current_user)], session: SessionDep,
									  research_application: Annotated[ResearchApplication, Body(embed=False)]) -> ResearchApplication:

	if ResearchApplication.select().where(ResearchApplication.created_by == current_user.dsid).first():
		raise HTTPException(status_code=400, detail='You already submitted an application')

	research_application.created_by = current_user.dsid

	session.add(research_application)
	session.commit()
	session.refresh(research_application)

	return research_application


@fast_api_app.put('/api/research-applications/{id}', summary='Updates a Research Application')
async def update_research_application(id: int, current_user: Annotated[User, Depends(get_current_user)], session: SessionDep,
									  updated_application: Annotated[ResearchApplication, Body(embed=False)]) -> ResearchApplication:

	application = session.query(ResearchApplication).filter(ResearchApplication.id == id, ResearchApplication.created_by == current_user.dsid).first()

	if not application:
		raise HTTPException(status_code=404, detail='Research application not found')

	# update the fields (excluding id and created_by)
	for key, value in updated_application.__dict__.items():

		if key.startswith('_') or key.endswith(('_at', '_by')) or key in {'id'}:
			continue

		setattr(application, key, value)

	session.commit()
	session.refresh(application)

	return application


""" Update """

@fast_api_app.get('/api/update', include_in_schema=False)
async def update():

	if not Config.current.is_development:
		return

	Utilities.install_package(name='apple-interlinked')


""" Debugging """

@fast_api_app.get('/__health', include_in_schema=False)
async def health(request: Request):
	return {'status': 'ok'}


@fast_api_app.get('/api/debug', include_in_schema=False)
async def debug(request: Request):
	print(request.__dict__)
	return str(request.__dict__)


@fast_api_app.get('/api/debug-noauth', include_in_schema=False)
async def debug_noauth(request: Request):
	print(request.__dict__)
	return str(request.__dict__)


@fast_api_app.get('/api/debug-502', include_in_schema=False)
async def debug_502(request: Request):
	raise HTTPException(status_code=502)


@fast_api_app.get('/api/stream-test', summary='Asks AI then returns the response', include_in_schema=False)
def stream_test() -> dict:

	def _get_response_generator():

		for i in range(0, 10):

			from time import sleep
			sleep(1)
			yield f'Lorem {i} '

	return StreamingResponse(_get_response_generator(), media_type='text/event-stream', headers={
		'Content-Type': 'text/event-stream',
		'Cache-Control': 'no-cache',
		'X-Accel-Buffering': 'no'
	})


""" Utilities """

def get_counts_in_messages(messages: list[dict], client_name: str, is_openai_format: bool = False) -> dict[str, int]:
	"""
	Returns the count of images in a list of messages, along with audio and video lengths

	Files, such as PDFs, are counted towards images, where each page is an image
	"""

	# dynamically install `pymediainfo`
	try:
		from pymediainfo import MediaInfo

	except ImportError as error:

		Utilities.install_package('pymediainfo')
		from pymediainfo import MediaInfo

	if client_name != 'googleaiclient':
		raise NotImplementedError('Not implemented yet for other AI clients')

	image_count: int = 0
	audio_length: int = 0
	video_length: int = 0

	for message in messages:

		if is_openai_format:

			content = message.get('content')

			if isinstance(content, str):
				continue

			content_parts = content if isinstance(content, list) else [content]

			for part in content_parts:
				if isinstance(part, dict) and 'image_url' in part:
					image_count += 1

			continue

		parts: list[dict] = message.get('parts', [])

		for part in parts:

			if 'fileData' in part:

				image_count += 1
				continue

			binary_data: bytes = None
			mime_type: str = None

			if 'inline_data' in part:

				binary_data = base64.b64decode(part['inline_data']['data'])
				mime_type = part['inline_data']['mime_type']
				size_mb = len(binary_data) / (1024 * 1024)

				if size_mb > 20:
					raise HTTPException(status_code=400, detail='The file size is too big. It should be equal or less than 20mb.')

			elif 'file_data' in part:

				url = part['file_data']['file_uri']

				with requests.head(url) as response:

					content_length: int = int(response.headers.get('content-length')) / (1024 * 1024)

					if content_length > 20:
						raise HTTPException(status_code=400, detail='The file size is too big. It should be equal or less than 20mb.')

				mime_type, _ = mimetypes.guess_type(url)
				response = requests.get(url, stream=True)
				binary_data =  response.content

			else:
				continue

			# pdf
			if mime_type.startswith('application/pdf'):

				# AI models consider each page as an image: counting limit based on number of pages
				text: str = binary_data.decode('latin1', errors='ignore')
				pages: list[str] = re.findall(r'/Type\s*/Page', text)
				image_count += max(0, len(pages) - 1)

			# images
			elif mime_type.startswith('image'):
				image_count += 1

			# audio and video
			elif mime_type.startswith(('audio', 'video')):

				binary_stream: io.BytesIO = io.BytesIO(binary_data)
				media_info = MediaInfo.parse(filename=binary_stream)
				duration_seconds: int = None

				# find the longest track (sometimes audio is longer)
				longest_track: 'Track' = max(media_info.tracks, key=lambda track: track.duration)

				# convert to seconds
				duration_seconds = longest_track.duration / 1000.0

				if longest_track.type == 'Audio':
					audio_length += int(duration_seconds)

				else:
					video_length += int(duration_seconds)
	return {

		'image_count': image_count,
		'audio_length': audio_length,
		'video_length': video_length,
	}



""" Routes """

@fast_api_app.get('/', include_in_schema=False)
@fast_api_app.get('/chat', include_in_schema=False)
@fast_api_app.get('/docs', include_in_schema=False)
@fast_api_app.get('/learn', include_in_schema=False)
@fast_api_app.get('/events', include_in_schema=False)
@fast_api_app.get('/models', include_in_schema=False)
@fast_api_app.get('/chatx_', include_in_schema=False)
@fast_api_app.get('/search', include_in_schema=False)
@fast_api_app.get('/applets', include_in_schema=False)
@fast_api_app.get('/research', include_in_schema=False)
@fast_api_app.get('/screener', include_in_schema=False)
@fast_api_app.get('/community', include_in_schema=False)
@fast_api_app.get('/workflows', include_in_schema=False)
@fast_api_app.get('/chat/{id}', include_in_schema=False)
@fast_api_app.get('/playground', include_in_schema=False)
@fast_api_app.get('/events/{id}', include_in_schema=False)
@fast_api_app.get('/documentation', include_in_schema=False)
@fast_api_app.get('/screener/{path:path}', include_in_schema=False)
@fast_api_app.get('/community/{path:path}', include_in_schema=False)
@fast_api_app.get('/workflows/{path:path}', include_in_schema=False)
async def home(current_user: Annotated[User, Depends(get_current_user)], request: Request):

	# check access to next
	if '-next.' in str(request.url):

		has_next_access: bool = 13165695 in current_user.group_dsids

		if not Config.current.is_development and not has_next_access:

			MailClient.send(to_addresses=['interlinked-admin@group.apple.com'], from_address='noreply@apple.com',
							subject='âš ï¸ Unauthorized access to Next', body=f'{current_user.first_name} {current_user.last_name} ({current_user.email}) attempted to access Next')
			return RedirectResponse('https://interlinked.apple.com')

	return FileResponse(f'{STATIC_PATH}/index.html')