import os
import json
import base64
import logging
import pathlib
import importlib
from datetime import datetime, timedelta
from typing import Any, Optional, ClassVar
from pydantic import BaseModel, ConfigDict, SkipValidation,  \
					 Field as PydanticField, PrivateAttr

from interlinked.ui.tools import Tools
from interlinked.core.tool import Tool
from interlinked.core.ai import AI, Pause
from interlinked.core.config import Config
from interlinked.core.clients.baseaiclient import BaseAIClient, ToolCall, File
from interlinked.core.clients.appledirectoryclient import AppleDirectoryClient, Person
from interlinked.ui.models import Suggestion, SuggestionResultEnum, Configuration, APIKey, APIKeyUsage

logger = logging.getLogger(__name__)


class Attachment(BaseModel):

	name: str
	url: str
	size: str = None

	# if this attachment is a folder
	attachments: list['Attachment'] = []


class Entry(BaseModel):

	text: str
	created_by: Person
	created_at: datetime = None


""" Suggestion """

class SuggestionUpdate(BaseModel):

	id: int = None

	# the value AI suggested for this field
	value: str|int = None
	display_value: str | None = None

	# the user's action on this suggestion
	result: SuggestionResultEnum = SuggestionResultEnum.valid


""" Item """

class Item(BaseModel):

	title: str = None
	created_at: datetime = None
	external_id: str | int = None

	state: str = None
	resolution: str = None
	classification: str = None

	assignee: Person = None
	security_dri: Person = None

	entries: list[Entry] = []
	attachments: list[Attachment] = []
	suggestions: dict[str, Suggestion] = {}

	component: dict = None


class ItemUpdate(BaseModel):

	suggestions: dict[str, SuggestionUpdate] = None

	internal_comment: str = None
	external_comment: str = None


""" Configuration """

class ConfigurationUsage(BaseModel):

	configuration: Configuration

	# total number of suggestions in this configuration
	suggestions: list[Suggestion]


class ConfigurationUpdate(BaseModel):

	# the Workflow this configuration is for
	workflow_name: str

	# the Apple Directory group that can access this configuration
	group_dsid: int

	data: dict


""" AI """

class AskRequest(BaseModel):

	model_config = ConfigDict(arbitrary_types_allowed=True)

	prompt: str | dict[str, str] | None = None
	template: str = None

	messages: list[dict] | None = []
	system_message: str | None = None

	# matches the `AI.ask` default
	max_iterations: int = 3
	temperature: float = 0.6

	client_name: str = None
	client_model_name: str = None

	stream: bool = False
	track: bool = False

	daw_token: str = None
	openai_api_key: str = None
	googleai_api_key: str = None

	# when on, requests are stored for donation
	# PII is removed from the content of each message
	donate_data: bool = False

	# used if user turns of `@AI.track` to view Steps
	# in Playground
	external_id: str | None = None

	# thinking
	is_thinking_enabled: bool | None = False
	setup_id: int | None = None

	files: list[bytes|str|dict] = None

	is_web_search: bool = False
	is_image_only: bool = False
	is_video_only: bool = False

	tool_names: list[str] | None = None
	tool_kwargs: dict | None = None

	# tool names to skip `Pause` on
	skip_pause_on: list[str] | None = None

	# optional options for each client
	options: dict | None = None

	""" Internal """
	_tools: SkipValidation['Tool'] = PrivateAttr(default=None)
	_client: SkipValidation['BaseAIClient'] = PrivateAttr(default=None)

	@property
	def client(self) -> BaseAIClient:
		"""
		Returns the AI client set in this request
		"""

		if self._client:
			return self._client

		options: dict = self.options or {}

		if self.client_name == 'endorclient':
			options['options'] = {'max_tokens': 4000}

		if self.client_name == 'ajaxclient' and not Config.current.is_development:

			if not self.daw_token or not self.daw_token.strip().startswith('DAW'):
				raise Exception('Please add your AppleConnect token')

			self._client = AI.CLIENTS[self.client_name](model_name=self.client_model_name, daw_token=self.daw_token, **options)

		elif self.client_model_name:
			self._client = AI.CLIENTS[self.client_name](model_name=self.client_model_name, **options)

		elif self.client_name:
			self._client = AI.CLIENTS[self.client_name](**options)

		return self._client

	@property
	def messages_for_client(self) -> list[dict]:
		"""
		Returns a list of messages that work with any AI client
		"""
		client: BaseAIClient = self.client

		if self.system_message and (system_message := self.system_message.strip()):
			if not self.messages:
				self.messages = []

			system_message: dict[str, Any] = client.create_message(role=client.ROLE_SYSTEM, content=system_message)

			if self.messages and self.messages[0].get('role') == self.client.ROLE_SYSTEM:
				self.messages[0] = system_message
			else:
				self.messages.insert(0, system_message)

		messages: list[dict] = []

		for message in (self.messages or []):

			if message.get('should_convert'):

				files = []

				# process files from the message
				for raw_file in message.get('files', []):

					if self.client_name == 'googleaiclient':

						# parse the data URL
						file_mime_type, file_content = raw_file.removeprefix('data:').replace(';base64', '').split(',')
						files.append(File(content_type=file_mime_type, content=base64.b64decode(file_content)))

					# for other clients, decode base64 content
					else:
						files.append(base64.b64decode(raw_file.split(',')[1]))

				# handle role mapping
				role: str = message['role']

				if role in {client.ROLE_ASSISTANT, 'model', 'assistant'}:
					role = client.ROLE_ASSISTANT

				content: str = message['content']

				# skip messages with no content
				# which could occur in chat.apple.com, if the response was empty and
				# the user replies instead of retrying, which is an expected flow
				if not content and not files:
					continue

				messages.append(client.create_message(role=role, content=content, files=files))

			else:
				messages.append(message)

		return messages

	@property
	def files_for_client(self) -> list[dict]:
		"""
		Returns a list of files that work with any AI client
		"""

		# TODO: all clients should support `File`
		if self.client_name == 'googleaiclient':

			files: list[File] = []

			for file in self.files:

				file_mime_type, file_content = file.removeprefix('data:').replace(';base64', '').split(',')
				files.append(File(content_type=file_mime_type, content=base64.b64decode(file_content)))

			return files

		return [base64.b64decode(file.split(',')[1]) for file in self.files]

	@property
	def tools(self) -> list[Tool]:
		"""
		Returns a list of tools based on the `tool_names` argument

		@return (list): a list of `Tool`
		"""

		if self._tools is not None:
			return self._tools

		if not self.tool_names:
			return []

		self._tools = []

		for tool_name in set(self.tool_names):

			if '..' in tool_name or tool_name.startswith('_'):
				continue

			if tool_name == 'run_code':
				tool_name = 'run_python_code'

			elif ':' in tool_name:

				tool: Tool = None
				file_path, function_name = tool_name.split(':')

				# built-in examples has a special path
				if file_path == 'examples.py':

					BASE_PATH: str = pathlib.Path(__file__).resolve().parent
					STATIC_PATH: str = pathlib.Path(f'{BASE_PATH}/static')
					file_path = f'{STATIC_PATH}/playground/{file_path if Config.current.is_development else "examples_production.py"}'

				elif not Config.current.is_development:
					raise Exception('Please run Playground locally to use any function')

				tool = Tool.get_tool_from_file(path=file_path, function_name=function_name)

			tool.function_kwargs = (self.tool_kwargs or {}).get(tool_name, {})

			# add default arguments
			if has_daw_token_argument := any(argument for argument in tool.function.signature.arguments if argument.get('name') == 'daw_token'):
				tool.function_kwargs['daw_token'] = self.daw_token

			if has_client_argument := any(argument for argument in tool.function.signature.arguments if argument.get('name') == 'client'):
				tool.function_kwargs['client'] = self.client

			self._tools.append(tool)

		return self._tools

	def tool_call_validator(self, tool_call: ToolCall, messages: list[dict, any], **kwargs) -> None:
		"""
		Pauses every time to confirm with the user before AI calls a Tool/function
		"""

		# don't pause if the user unchecked "Always ask me"
		if self.skip_pause_on and tool_call.name in self.skip_pause_on:
			return

		raise Pause(messages=messages)


""" Tool """

class ToolUpdate(BaseModel):

	# name: str
	code: str

	# the Apple Directory group that can access this Tool
	group_dsid: int


""" APIKey Cache """

class APIKeyCache(BaseModel):

	model_config = ConfigDict(populate_by_name=True)

	__FORWARDED_MODEL_NAME_MAP__: dict[str, str] = {

		'gemini-2.0-flash-exp': 'gemini-2.0-flash',
		'gemini-2.5-flash-preview-04-17': 'gemini-2.5-flash',
		'gemini-2.5-flash-preview-05-20': 'gemini-2.5-flash',

		'gemini-2.5-pro-exp-03-25': 'gemini-2.5-pro',
		'gemini-2.5-pro-preview-05-06': 'gemini-2.5-pro',
		'veo-3.0-generate-preview': 'veo-3.0-fast-generate-preview',
	}

	__CACHE_KEY_PREFIX__: str = 'api_key:'
	__DEFAULT_GOOGLE_AI_RATE_LIMITS__: dict[str, Any] = {
		'chat': {

			# flash
			'googleaiclient:gemini-2.0-flash': {'hour': 1800, 'day': 1800, 'max_input_length': 70000, 'max_output_length': 1649, 'day_image_count': 200, 'day_audio_length': 120, 'day_video_length': 120},
			'googleaiclient:gemini-2.5-flash': {'hour': 1800, 'day': 1800, 'max_input_length': 70000, 'max_output_length': 1649, 'day_image_count': 200, 'day_audio_length': 120, 'day_video_length': 120},

			# pro
			'googleaiclient:gemini-2.5-pro': {'hour': 60, 'day': 60, 'max_input_length': 20000, 'max_output_length': 942, 'day_image_count': 20, 'day_audio_length': 0, 'day_video_length': 0},

			# imagen
			'googleaiclient:imagen-3.0-generate-002': {'hour': 1, 'day': 1, 'week': 2},
			'googleaiclient:imagen-3.0-fast-generate-001': {'hour': 2, 'day': 2, 'week': 4},

			# veo (video generation)
			'googleaiclient:veo-2.0-generate-001': {'hour': 2, 'day': 2, 'week': 3},
			'googleaiclient:veo-3.0-fast-generate-preview': {'hour': 2, 'day': 2, 'week': 3},

		},
		'embedding': {
			'googleaiclient:text-embedding-004': {'hour': 100, 'day': 150, 'max_input_length': 6000, 'day_image_count': 0, 'day_audio_length': 0, 'day_video_length': 0},
			'googleaiclient:text-embedding-005': {'hour': 500, 'day': 500, 'max_input_length': 9000, 'day_image_count': 0, 'day_audio_length': 0, 'day_video_length': 0},
			'googleaiclient:text-multilingual-embedding-002': {'hour': 100, 'day': 150, 'max_input_length': 6000, 'day_image_count': 0, 'day_audio_length': 0, 'day_video_length': 0},
		},
		'platform': {
			'googleaiclient:google_search': {'hour': 10, 'day': 10, 'week': 20},
		},
		'token_count': {'*': {'hour': 400, 'day': 1000}},
	}

	__DEFAULT_ANTHROPIC_RATE_LIMITS__: dict = {
		'chat': {

			# anthropic
			'anthropicclient:claude-3-5-haiku-latest': {'hour': 10, 'day': 10, 'max_input_length': 10000, 'max_output_length': 942, 'day_image_count': 0},
			'anthropicclient:claude-3-7-sonnet-latest': {'hour': 10, 'day': 10, 'max_input_length': 10000, 'max_output_length': 942, 'day_image_count': 0},

		},
		'embedding': {},
		'platform': {},
		'token_count': {'*': {'hour': 400, 'day': 1000}},
	}

	id: int

	# key: endpoint name
	#	- sub-keys: <AI client name:model name> (e.g. `googleaiclient:gemini-2.5-pro`)
	# value: number of requests allowed
	rate_limits: dict | None = PydanticField(alias='rl', default={})

	created_by: int = PydanticField(alias='cb')
	department_number: str | None = PydanticField(alias='dn')
	svp_dsid: int | None = PydanticField(alias='sd')
	svp_full_name: str | None = PydanticField(alias='sf')

	enabled: bool = PydanticField(alias='ed', default=True)
	value: str = PydanticField(exclude=True)

	use_proxy: Optional[bool] = PydanticField(alias='up', default=False)

	permissions: Optional[list] = PydanticField(alias='pr', default=[])

	@classmethod
	def get_has_usage(cls, dsid: int, redis: 'Redis', skipped_api_key_id: int = None) -> bool:
		"""
		[Internal]
		Returns whether a given API Key has any usage in the past 24 hours
		"""

		def _get_has_usage(api_key_id: int) -> bool:

			if api_key_id == skipped_api_key_id:
				return False

			# check Redis first
			cursor = 0

			while True:

				cursor, keys = redis.scan(cursor, match=f'aku:{api_key_id}:*', count=10)

				if keys:
					return True

				if cursor == 0:
					break

			# check the database
			if APIKeyUsage.select().join(APIKey).where(APIKeyUsage.api_key_id == api_key_id, APIKey.created_by == dsid,
													   APIKeyUsage.created_at > datetime.now() - timedelta(hours=24),
													   APIKey.is_for_platform == False, APIKey.is_transient == False).all():
				return True

			return False

		return any(_get_has_usage(api_key_id=api_key.id) for api_key in
								  APIKey.select().where(APIKey.created_by == dsid, APIKey.is_for_platform == False,
								  APIKey.is_transient == False).all())

	@classmethod
	def get_default_rate_limits(cls) -> dict[str, Any]:
		"""
		Returns a merged dictionary of all rate-limits
		"""

		return cls.get_merged_rate_limits(cls.__DEFAULT_GOOGLE_AI_RATE_LIMITS__, cls.__DEFAULT_ANTHROPIC_RATE_LIMITS__)

	""" Utilities """

	@classmethod
	def get_as_markdown(cls, rate_limits: dict[str, Any]) -> str:
		"""
		Converts the rate limits into a Markdown table format
		"""

		header = '| | | Per-Hour | Per-Day | Max Characters | Max Images (per-day)\\n'
		separator = '|---|---|---|---|---|---|\\n'
		rows = []

		for category, limits in rate_limits.items():

			if category == 'platform':
				continue

			display_category: str = {

				'chat': '`AI.ask(…)`',
				'embedding': '`AI.learn(…)`',
				'token_count': '`.get_token_count(…)`',
			}[category]

			get_formatted_number: Callable = lambda number: f"{number:,}" if isinstance(number, (int, float)) else str(number)

			for rate_limit_index, (model_name, rate_limit) in enumerate(limits.items()):

				# remove client name
				display_model_name: str = model_name.split(':', 1)[-1]

				day = get_formatted_number(number=rate_limit.get('day', 'N/A'))
				hour = get_formatted_number(number=rate_limit.get('hour', 'N/A'))

				day_image_count = get_formatted_number(number=rate_limit.get('day_image_count', 'N/A'))
				max_input_length = get_formatted_number(number=rate_limit.get('max_input_length', 'N/A'))

				rows.append(f'| {display_category if rate_limit_index == 0 else ""} | {display_model_name} | {hour} | {day} | {max_input_length} | {day_image_count} |')

		# add token count separately
		token_count_limits = rate_limits.get('token_count', {}).get('*', {})
		token_count_hour = token_count_limits.get('hour', 'N/A')
		token_count_day = token_count_limits.get('day', 'N/A')
		rows.append(f'| `.get_models(…)` | ∞ | ∞ | | | | |')
		rows = '\\n'.join(rows)

		return f'{header}{separator}{rows}'

	@classmethod
	def get_merged_rate_limits(cls, rate_limits_a: dict, rate_limits_b: dict) -> dict:
		"""
		Merges two rate limit dictionaries. `rate_limits_b` takes precedence
		in case of overlap.
		"""
		merged_rate_limits: dict[str, Any] = {}

		# iterate through all main categories (e.g. `chat`, `embedding`, etc.)
		all_categories = set(rate_limits_a.keys()) | set(rate_limits_b.keys())

		for category in all_categories:

			merged_rate_limits[category] = {}

			sub_a = rate_limits_a.get(category, {})
			sub_b = rate_limits_b.get(category, {})

			merged_rate_limits[category].update(sub_a)
			merged_rate_limits[category].update(sub_b)

		return merged_rate_limits

class QuestionnaireResponse(BaseModel):

	step: str
	type: str
	has_next_step: bool = True
	subtitle: Optional[str] = None
	back_page: Optional[str] = None
	elements: Optional[dict[str, Any]] = None
	options: Optional[list[dict[str, Any]]] = None


class Questionnaire(BaseModel):

	# the DSID of the user onboarding
	# if not set, the DSID is expected to be in the headers
	dsid: Optional[int] = None

	# the name of the AI client
	# any of `['anthropicclient', 'googleaiclient']`
	client_name: Optional[str] = None

	# list of AI client names to check access for all
	# any combination of `['anthropicclient', 'googleaiclient']`
	client_names: Optional[list[str]] = None

	# used to track which version of the questionnaire
	# to return
	version: Optional[str|dict] = None

	current_page: Optional[str] = None
	questionnaire: Optional[dict[str, Any]] = None

	# [internal]
	# used for caching the response function
	_get_response_map: ClassVar[dict[str, callable]] = {}
	_questionnaire_definitions: ClassVar[dict[str, dict[str, Any]]] = {}

	# [internal]
	BETA_GROUP_DSID: ClassVar[int] = 13334940

	# [internal]
	# used for backup access to Anthropic
	BACKUP_GROUP_DSID: ClassVar[int] = 13084924
	CACHED_BETA_DSIDS: ClassVar[list[int]] = []
	LAST_CACHED_BETA_DSIDS: ClassVar[datetime] = None

	# [internal]
	# caching for access groups
	CACHED_ACCESS_GROUPS: ClassVar[list[dict]] = []
	LAST_CACHED_ACCESS_GROUPS: ClassVar[datetime] = None

	@classmethod
	def get_access_groups(cls) -> list[dict]:
		"""
		Returns a list of access groups from GitHub
		Caches for 15 minutes
		"""

		if not cls.LAST_CACHED_ACCESS_GROUPS or cls.LAST_CACHED_ACCESS_GROUPS < datetime.now() - timedelta(minutes=15):

			try:

				from interlinked.core.clients.githubclient import GitHubClient

				github_client: GitHubClient = GitHubClient()
				repository: 'Repository' = GitHubClient.shared.get_repository(owner_name='Interlinked', name='interlinked')
				blobs: list['Blob'] = repository.get_blobs(branch_name='main', recursive=True)

				json_file_blob = None

				for blob in blobs:

					if blob.path == 'interlinked/ui/questionnaire/access_groups.json':

						json_file_blob = blob
						break

				if json_file_blob:

					file_content = json_file_blob.content

					if isinstance(file_content, bytes):
						file_content = file_content.decode('utf-8')

					json_data = json.loads(file_content)
					cls.CACHED_ACCESS_GROUPS = json.loads(file_content)
					cls.LAST_CACHED_ACCESS_GROUPS = datetime.now()

			except Exception as exception:

				logger.error(f'Could not load access groups from GitHub: {exception}')

				# keep using cached data if available, otherwise use one locally
				if not cls.CACHED_ACCESS_GROUPS:

					BASE_PATH: str = pathlib.Path(__file__).resolve().parent
					STATIC_PATH: str = pathlib.Path(f'{BASE_PATH}/static')

					with open(f'{BASE_PATH}/questionnaire/access_groups.json', 'r') as file:
						cls.CACHED_ACCESS_GROUPS = json.loads(file.read())

		return cls.CACHED_ACCESS_GROUPS

	def get_response(self) -> QuestionnaireResponse:
		"""
		Returns the response based on the `version`
		"""

		if not self.dsid:
			raise Exception('Missing `.dsid` of the user')

		if not self.version:

			# without a `version` + (`client_name` or `client_names`), we do not know which version to auto-pick
			if not self.client_name and not self.client_names:
				raise Exception('When no `version` is set, `client_name` or `client_names` is required')

			# Default to the new any_v3 for most cases
			if self.client_name == 'anthropicclient':
				self.version = 'any_v3'

			elif self.client_name == 'googleaiclient':
				self.version = 'googleclient_v1'
			
			# Fallback to old versions if needed
			elif self.client_names:
				self.version = 'any_v3'

		# used for checking if a user is part of an org that
		# limits access to certain members
		access_group_maps: list[dict] = Questionnaire.get_access_groups()

		# 1️⃣ check if user is in beta group (can access the wizard)
		has_beta_access: bool = access_group_maps is not None

		if not has_beta_access:

			try:
				# check nested groups if Open Directory sync is not complete yet
				# use the cached list and make sure to update every 2 minutes
				if not Questionnaire.LAST_CACHED_BETA_DSIDS or Questionnaire.LAST_CACHED_BETA_DSIDS < datetime.now() - timedelta(minutes=4):

					Questionnaire.LAST_CACHED_BETA_DSIDS = datetime.now()
					Questionnaire.CACHED_BETA_DSIDS = [person.dsid for person in AppleDirectoryClient.shared.get_persons_for_group_dsid(dsid=Questionnaire.BETA_GROUP_DSID, nested=True)]

				has_beta_access = any(dsid for dsid in Questionnaire.CACHED_BETA_DSIDS if dsid == self.dsid)

			except Exception as exception:
				logger.error('could not get_persons_for_group_dsid for onboarding')

		# if this is `googleclient` and the user is part of HWT, show an additional screen
		# if type is None and AppleDirectoryClient.shared.get_person_for_dsid(dsid=dsid).svp.dsid == 22094965:
		# 	return QuestionnaireResponse(step='existing_access', type='information',
		# 								 elements={'question': 'You already have access',
		# 								 		   'description': 'You can begin using AI today. If you would like to update previous answers you have given, press "Next".'})

		# 	elif self.current_page == 'existing_access':
		# 		self.current_page = None

		# if user is in beta group, show the wizard
		if has_beta_access:

			# check if the user already has access
			if self.current_page is None:

				accessible_client_names: list[str] = []

				if not self.client_names:
					self.client_names = [self.client_name]

				# if client_names is set, check if user has access to all specified clients
				for client_name in self.client_names:

					if client_name == 'anthropicclient':

						if Questionnaire.get_has_anthropic_access(dsid=self.dsid):
							accessible_client_names.append(client_name)

					elif client_name == 'googleaiclient':

						if Questionnaire.get_has_googleai_access(dsid=self.dsid):
							accessible_client_names.append(client_name)

				if len(accessible_client_names) != len(self.client_names):

					client_name_map: dict[str, str] = {

						'test': 'Test AI',
						'googleaiclient': 'Google AI',
						'anthropicclient': 'Anthropic',
					}

					# e.g. [Google AI, Anthropic]
					display_accessible_client_names: list[str] = [client_name_map[client_name] for client_name in accessible_client_names]
					display_missing_client_names: list[str] = [client_name_map[client_name] for client_name in self.client_names if client_name not in accessible_client_names]
					return QuestionnaireResponse(step='existing_access', type='information',
												 elements={'question': 'Update your access',
												 		   'description': f'You can begin using {", and ".join(display_accessible_client_names)} today.'  \
												 		   				  f' If you would like to get access to {", and ".join(display_missing_client_names)}, press "Next".'})

				else:
					return QuestionnaireResponse(step='existing_access', type='information',
												 elements={'question': 'You already have access',
												 		   'description': 'You can begin using AI today. If you would like to update previous answers you have given, press "Next".'})

			elif self.current_page == 'existing_access':
				self.current_page = None

			if isinstance(self.version, dict):
				return self.get_response_from_json(self.version)

			elif self.version in Questionnaire._get_response_map:
				return Questionnaire._get_response_map[self.version](questionnaire=self)

			# load and process questionnaire
			else:
				return self.get_questionnaire_response()

		# 2️⃣ check if user is in a controlled access group
		controlled_group_dsids: list[int] = [access_group['dsid'] for access_group in access_group_maps]
		matching_group_dsids: list[int] = AppleDirectoryClient.shared.get_matching_group_dsids(group_dsids=controlled_group_dsids, person_dsid=self.dsid)

		if matching_group_dsids:

			matching_group_dsid = matching_group_dsids[0]
			matching_access_group = next(access_group_map for access_group_map in access_group_maps if access_group_map['dsid'] == matching_group_dsid)

			return QuestionnaireResponse(step='no_access', type='information',
										 has_next_step=False, elements={
											 'question': matching_access_group['question'],
											 'description': matching_access_group['description']
										 })

		# the user is not in beta or controlled group - show default message
		return QuestionnaireResponse(step='no_access', type='information',
									 has_next_step=False, elements={
										'question': 'Beta Access Required',
										 'description': 'To get added to the beta waitlist, please reach out to the onboarding team at proj-genai-beta-onboarding@group.apple.com.'
									 })

	def get_questionnaire_response(self) -> dict[str, Any]:
		"""
		Helper method to load questionnaire response
		"""

		module_name: str = f'interlinked.ui.questionnaire.{self.version}'
		spec = importlib.util.find_spec(module_name)

		if spec:
			Questionnaire._get_response_map[self.version] = getattr(importlib.import_module(module_name), 'get_response')

		else:

			BASE_PATH = pathlib.Path(__file__).resolve().parent

			json_path = f'{BASE_PATH}/questionnaire/{self.version}.json'

			if os.path.exists(json_path):

				with open(json_path, 'r') as file:
					Questionnaire._questionnaire_definitions[self.version] = json.load(file)

				Questionnaire._get_response_map[self.version] = lambda questionnaire: \
					questionnaire.get_response_from_json(Questionnaire._questionnaire_definitions[questionnaire.version])
			else:
				raise Exception(f'Could not find {json_path}')

		return Questionnaire._get_response_map[self.version](questionnaire=self)

	""" Access """

	@classmethod
	def get_has_anthropic_access(cls, dsid: int) -> bool:
		"""
		Returns whether the the user has access to Anthropic
		"""
		return len(AppleDirectoryClient.shared.get_matching_group_dsids(group_dsids=[13084779], person_dsid=dsid)) > 0

	@classmethod
	def get_has_googleai_access(cls, dsid: int) -> bool:
		"""
		Returns whether the the user has access to Google AI
		"""
		return len(AppleDirectoryClient.shared.get_matching_group_dsids(group_dsids=[13320800], person_dsid=dsid)) > 0

	""" JSON Questionnaire """

	def get_response_from_json(self, json: dict[str, Any]) -> QuestionnaireResponse:
		"""
		Process a JSON-defined questionnaire and return the appropriate response.
		This is a consolidated function that handles all questionnaire processing.

		Supports both v2 (questions) and v3 (nodes) format.

		@param json(dict): The questionnaire definition loaded from JSON
		@return(QuestionnaireResponse): A `QuestionnaireResponse` object for the current state
		"""

		next_page: str = None
		back_page: str = None
		
		# Determine if this is v3 (nodes) or v2 (questions) format
		is_v3_format = 'nodes' in json
		questions_or_nodes = json.get('nodes' if is_v3_format else 'questions', {})
		start_key = 'start_node_id' if is_v3_format else 'start_question_id'
		next_key = 'next_node_id' if is_v3_format else 'next_question_id'
		

		# If no current page, start at the beginning
		if self.current_page is None:
			next_page = json.get(start_key)

		else:

			# get the current question/node data
			current_data = questions_or_nodes.get(self.current_page, {})
			
			if is_v3_format:
				next_page = self._process_v3_node(current_data, next_key)
			else:
				next_page = self._process_v2_question(current_data, next_key)
			
			# Check if we got a final result
			if isinstance(next_page, QuestionnaireResponse):
				return next_page
		
		# if we don't have a next page, something went wrong
		if not next_page:
			return QuestionnaireResponse(step='error', type='information', has_next_step=False, elements={
				'question': 'Something Happened',
				'description': 'Could not determine the next question. Please take a screenshot of this screen and share it in #help-interlinked. '
							   f'Current page: {self.current_page}, Answer: {self.questionnaire.get(self.current_page) if self.questionnaire else None}'
			})

		# For v3 format, resolve if_group nodes before building response
		if is_v3_format:
			next_page = self._resolve_if_group_chain(next_page, questions_or_nodes)
			
			# Check if resolution resulted in a final response
			if isinstance(next_page, QuestionnaireResponse):
				return next_page

		# update the current page to the next page
		node_data: dict[str, Any] = questions_or_nodes.get(next_page, {})

		if is_v3_format:
			return self._build_v3_response(next_page, node_data, next_key)
		else:
			return self._build_v2_response(next_page, node_data, next_key)

	def _resolve_if_group_chain(self, node_id: str, nodes: dict[str, Any], max_depth: int = 20) -> str | QuestionnaireResponse:
		"""
		Resolve a chain of if_group nodes until we reach a displayable node.
		This prevents "-branch" nodes from being shown as content when they should auto-navigate.
		
		@param node_id: The starting node ID to resolve
		@param nodes: The nodes dictionary from the questionnaire JSON
		@param max_depth: Maximum depth to prevent infinite loops
		@return: Final node ID or QuestionnaireResponse for terminal nodes
		"""
		
		current_node_id = node_id
		depth = 0
		
		while depth < max_depth:
			node_data = nodes.get(current_node_id, {})
			
			# If this is an if_group node, resolve it
			if 'if_group' in node_data:
				group_info = node_data['if_group']
				group_name = group_info.get('group')
				
				# Check group membership
				is_member = self._check_group_membership(group_name)
				
				# Get the next node based on membership
				current_node_id = group_info.get('member_node_id' if is_member else 'non_member_node_id')
				
				if not current_node_id:
					logger.error(f'if_group node missing next_node_id: {node_data}')
					break
					
				depth += 1
				continue
			
			# If this is a terminal node, handle it immediately
			elif 'not_permitted' in node_data:
				perm_data = node_data['not_permitted']
				return QuestionnaireResponse(
					step='result', type='information', has_next_step=False,
					elements={
						'question': perm_data.get('text', 'Not Permitted'),
						'description': perm_data.get('detail', 'Based on your answers, you are not permitted to proceed'),
						'permitted': False
					}
				)
			
			elif 'models' in node_data:
				models_data = node_data['models']
				return QuestionnaireResponse(
					step='result', type='information', has_next_step=False,
					elements={
						'question': 'Access Granted',
						'description': 'You now have access to the requested models',
						'models': models_data
					}
				)
			
			# If it's not an if_group or terminal node, we can display it
			else:
				break
		
		if depth >= max_depth:
			logger.error(f'Hit max depth resolving if_group chain starting from {node_id}')
			return QuestionnaireResponse(step='error', type='information', has_next_step=False, elements={
				'question': 'Navigation Error',
				'description': 'Too many nested group checks. Please contact support.'
			})
		
		return current_node_id

	def _process_v3_node(self, current_data: dict[str, Any], next_key: str) -> str | QuestionnaireResponse:
		"""Process a v3 format node and return next node ID or final response"""
		
		# if_group nodes should have been resolved earlier in the chain
		# If we encounter one here, it indicates a logic error
		if 'if_group' in current_data:
			logger.error(f'Unexpected if_group node during processing: {self.current_page}')
			group_info = current_data['if_group']
			group_name = group_info.get('group')
			is_member = self._check_group_membership(group_name)
			return group_info.get('member_node_id' if is_member else 'non_member_node_id')
		
		# Handle other node types based on current user input
		current_answer = self.questionnaire.get(self.current_page) if self.questionnaire else None
		
		# Handle select nodes (similar to old questions with answers)
		if 'select' in current_data:
			select_data = current_data['select']
			
			# Handle case where answer is a list and use the first answer
			if isinstance(current_answer, list) and len(current_answer) > 0:
				current_answer = current_answer[0]
			
			# If no answer yet, check for default
			if current_answer is None:
				for option in select_data.get('options', []):
					if option.get('default'):
						current_answer = option.get('text')
						break
			
			# Find matching option and get next node
			for option in select_data.get('options', []):
				if option.get('text') == current_answer:
					return option.get(next_key)
			
			# If it's an information screen with only one option, proceed
			if len(select_data.get('options', [])) == 1:
				return select_data['options'][0].get(next_key)
		
		# Handle input nodes (text entry)
		elif 'input' in current_data:
			input_data = current_data['input']
			return input_data.get(next_key)
		
		# Handle not_permitted nodes (should be handled in _resolve_if_group_chain now)
		elif 'not_permitted' in current_data:
			perm_data = current_data['not_permitted']
			return QuestionnaireResponse(
				step='result', type='information', has_next_step=False,
				elements={
					'question': perm_data.get('text', 'Not Permitted'),
					'description': perm_data.get('detail', 'Based on your answers, you are not permitted to proceed'),
					'permitted': False
				}
			)
		
		# Handle models nodes (should be handled in _resolve_if_group_chain now)
		elif 'models' in current_data:
			models_data = current_data['models']
			return QuestionnaireResponse(
				step='result', type='information', has_next_step=False,
				elements={
					'question': 'Access Granted',
					'description': 'You now have access to the requested models',
					'models': models_data
				}
			)
		
		return None

	def _check_group_membership(self, group_name: str) -> bool:
		"""
		Check if the current user is a member of the specified group.
		Maps group names to known group DSIDs and checks membership.
		"""
		
		# Map group names to their corresponding DSIDs
		# This mapping is based on the groups used in any_v3.json
		group_dsid_map = {
			# Legacy/Beta groups
			'sear-proj-floodgate-access': self.BETA_GROUP_DSID,  # 13334940
			'genai-onboarding-pressure-valve': 13334940,  # Pressure valve group
			
			# Access control groups for different organizations
			'genai-onboarding-access-swe': 1000168827,
			'genai-onboarding-access-hwt': 1000152538,
			'genai-onboarding-access-aiml': 1000543527,
			'genai-onboarding-access-ais': 1000543527,  # AIS uses same as AIML
			'genai-onboarding-access-ist': 1000364597,
			'genai-onboarding-access-ase': 1000113595,
			'genai-onboarding-access-maps': 11995765,
			'genai-onboarding-access-apps': 1000083067,
			'genai-onboarding-access-wpc': 1000394264,
			'genai-onboarding-access-hwe': 1000129599,
			'genai-onboarding-access-watch': 1000503220,
			'genai-onboarding-access-hi': 1000376793,
			'genai-onboarding-access-health': 1000472406,
			'genai-onboarding-access-corpdev': 1000374363,
			'genai-onboarding-access-ads': 1000599028,
			
			# Self-onboard groups
			'floodgate-self-onboard-aiml': 13372886,
			'floodgate-self-onboard-apps': 1000083067,
			'floodgate-self-onboard-ap': 13585294,
			'floodgate-self-onboard-hweng': 1000129599,
			'floodgate-self-onboard-hwtech': 13372895,
			'floodgate-self-onboard-ist': 1000364597,
			'floodgate-self-onboard-hi': 13502686,
			'floodgate-self-onboard-watch': 13469100,
			'floodgate-self-onboard-wpc': 13540968,
			
			# Model access groups
			'anthropic-models-onboarded': 13084779,  # Anthropic access group
			'google-models-onboarded': 13320800,    # Google AI access group
			
			# Specific org access groups
			'adrian-org-floodgate-access': 1000374363,  # Corp Dev
			'apple_genai_3pc_models_-_ase': 6574142,   # ASE specific
			'hsw-gen-ai-access': 13541222,              # Health SW
		}
		
		group_dsid = group_dsid_map.get(group_name)
		
		if not group_dsid:
			logger.warning(f'Unknown group name in any_v3 questionnaire: {group_name}')
			return False
		
		try:
			# Check if user is in the group
			matching_groups = AppleDirectoryClient.shared.get_matching_group_dsids(
				group_dsids=[group_dsid], person_dsid=self.dsid
			)
			return len(matching_groups) > 0
		except Exception as exception:
			logger.error(f'Error checking group membership for {group_name}: {exception}')
			return False

	def _process_v2_question(self, current_question: dict[str, Any], next_key: str) -> str | QuestionnaireResponse:
		"""Process a v2 format question and return next question ID or final response"""
		
		current_page_type: str = self.get_page_type(question_data=current_question)
		current_answer = self.questionnaire.get(self.current_page) if self.questionnaire else None

		# if the answer is None, use the default from the JSON (if available)
		if current_answer is None and (default_answer_id := current_question.get('default')):
			# find the default answer
			current_answer = next((answer for answer in current_question.get('answers', []) if answer.get(next_key) == default_answer_id), None)
			if current_answer:
				current_answer = current_answer.get('id', current_answer.get('answer'))

		# if the question was information/review/freeform textarea
		elif current_page_type in {'information', 'review', 'textarea'}:
			return current_question.get(next_key)

		# handle case where answer is a list (e.g. `["Employees"]`) and use the first answer
		elif isinstance(current_answer, list) and len(current_answer) > 0:
			current_answer = current_answer[0]

		# find the matching answer and determine next step
		if current_answer is not None:
			for answer_option in current_question.get('answers', []):
				# some answers in the current JSON do not have `id`
				if answer_option.get('id', answer_option.get('answer')) != current_answer:
					continue

				# get next question or result
				next_page = answer_option.get(next_key)
				result = answer_option.get('result')
				
				# if we have a result and no next page, return the result
				if result and not next_page:
					question: str = result.get('message')
					description: str = result.get('detail')

					# sometimes `detail` should've been `message`, because it's too long
					if question and len(question) > 15 and not description:
						description = question
						question = None

					if not question:
						if result.get('permitted') is False:
							question = 'Not Permitted'
							description = 'Based on your answers, you are not permitted to onboard'

					if not question:
						question = 'Questionnaire Completed'
						description = result.get('message', str(result))

					return QuestionnaireResponse(step='result', type='information', has_next_step=False, elements={
												 **result, 'question': question, 'description': description})
				
				return next_page
		
		return None

	def _build_v3_response(self, next_page: str, node_data: dict[str, Any], next_key: str) -> QuestionnaireResponse:
		"""Build response for v3 format nodes"""
		
		# if_group nodes should never reach this point
		# If they do, it indicates a bug in the resolution logic
		if 'if_group' in node_data:
			logger.error(f'if_group node reached _build_v3_response: {next_page}')
			return QuestionnaireResponse(step='error', type='information', has_next_step=False, elements={
				'question': 'Navigation Error',
				'description': f'Encountered an unresolved branch node ({next_page}). Please contact support.'
			})
		
		subtitle: str = None
		options: list[dict] = []
		elements: dict[str, Any] = {}
		has_next_step: bool = True
		question_type: str = 'information'
		
		# Handle select nodes
		if 'select' in node_data:
			select_data = node_data['select']
			question_type = self._determine_v3_question_type(select_data)
			
			elements = {
				'question': select_data.get('text', ''),
				'description': select_data.get('detail', '')
			}
			
			# Handle special case for coding agreement - only for actual agreement nodes
			if next_page in ['coding-agreement-anthropic-gemini', 'coding-agreement-gemini']:
				BASE_PATH: str = pathlib.Path(__file__).resolve().parent
				STATIC_PATH: str = pathlib.Path(f'{BASE_PATH}/static')
				
				try:
					with open(f'{STATIC_PATH}/third_party_coding_agreement.md', 'r') as file:
						elements['question'] = select_data.get('text', 'Third Party Model Generative Coding Agreement')
						elements['description'] = file.read()
						question_type = 'confirmation'
						# Coding agreements are terminal - UI will provision access
						has_next_step = False
				except FileNotFoundError:
					# Fallback to using the detail from JSON
					pass
			
			# Handle items (bullet points)
			if 'items' in select_data:
				elements['statements'] = [
					{
						'id': idx,
						'title': item.get('text', ''),
						'description': item.get('detail', '')
					}
					for idx, item in enumerate(select_data['items'])
				]
				question_type = 'review'
			
			# Build options
			for option in select_data.get('options', []):
				opt = {
					'label': option.get('text', ''),
					'key': option.get('text', ''),
				}
				
				if 'detail' in option:
					opt['description'] = option['detail']
				
				if option.get('default'):
					opt['default'] = True
				
				options.append(opt)
			
			# Only update has_next_step if it hasn't been explicitly set (e.g., for coding agreements)
			if has_next_step:
				has_next_step = next_key in select_data or any(next_key in opt for opt in select_data.get('options', []))
		
		# Handle input nodes
		elif 'input' in node_data:
			input_data = node_data['input']
			question_type = 'textarea'
			
			elements = {
				'question': input_data.get('text', ''),
				'description': input_data.get('detail', ''),
				'placeholder': input_data.get('placeholder', '')
			}
			
			has_next_step = next_key in input_data
		
		# Handle other node types as information screens
		else:
			elements = {
				'question': node_data.get('text', next_page),
				'description': node_data.get('detail', '')
			}
		
		if not subtitle and isinstance(node_data.get('detail'), str):
			subtitle = node_data['detail']
		
		return QuestionnaireResponse(
			step=next_page, elements=elements, type=question_type,
			has_next_step=has_next_step, back_page=None, subtitle=subtitle,
			options=options if options else None
		)

	def _build_v2_response(self, next_page: str, question_data: dict[str, Any], next_key: str) -> QuestionnaireResponse:
		"""Build response for v2 format questions (backward compatibility)"""
		
		subtitle: str = None
		options: list[dict] = []
		elements: dict[str, Any] = {}
		has_next_step: bool = True
		question_type = self.get_page_type(question_data=question_data)

		if question_type == 'textarea':
			elements = {
				'question': question_data.get('message'),
				'description': question_data.get('detail'),
				'placeholder': question_data.get('placeholder'),
			}

		elif question_type == 'confirmation' and 'agreement' in question_data:
			BASE_PATH: str = pathlib.Path(__file__).resolve().parent
			STATIC_PATH: str = pathlib.Path(f'{BASE_PATH}/static')

			with open(f'{STATIC_PATH}/third_party_coding_agreement.md', 'r') as file:
				elements['question'] = 'Third Party Model Generative Coding Agreement'
				elements['description'] = file.read()

			has_next_step = next_key in question_data

		elif question_type in {'information', 'review'}:
			if question_type == 'review':
				subtitle = 'Please review these additional statements'

			elements['statements'] = [{'id': detail.get('id'), 'title': detail.get('title'), 'description': detail.get('message')}
									  for detail in question_data.get('details', [])]

		else:
			for answer in question_data.get('answers', []):
				option = {
					'label': answer['answer'],
					'description': answer.get('detail'),
					'key': answer.get('id', answer['answer']),
				}

				if 'detail' in answer:
					option['description'] = answer['detail']

				if answer.get('default'):
					option['default'] = True

				options.append(option)

			elements = {
				'question': question_data.get('question', ''),
				'description': question_data.get('detail', '')
			}

		if not subtitle and isinstance(question_data.get('detail'), str):
			subtitle = question_data['detail']

		return QuestionnaireResponse(step=next_page, elements=elements, type=question_type,
									 has_next_step=has_next_step, back_page=None, subtitle=subtitle,
									 options=options if options else None)

	def _determine_v3_question_type(self, select_data: dict[str, Any]) -> str:
		"""Determine the appropriate question type for v3 select nodes"""
		
		tags = select_data.get('tags', [])
		options = select_data.get('options', [])
		
		# Check for review/information screens
		if 'items' in select_data:
			return 'review'
		
		# If there are no options, it's an information screen
		if not options:
			return 'information'
		
		# If there are exactly two options, use tabs
		if len(options) == 2:
			return 'tabs'
		
		# If there are multiple options (more than 2), use dropdown
		if len(options) > 1:
			return 'dropdown'
		
		return 'information'

	def get_page_type(self, question_data: dict[str, Any]) -> str:
		"""
		Determine the appropriate question type based on the question data structure

		@param question_data(dict): The question data from the JSON definition
		@return(str): The determined question type
		"""

		if isinstance(question_data.get('details'), list):
			return 'review'

		if 'text_entry' in question_data:
			return 'textarea'

		if 'agreement' in question_data:
			return 'confirmation'

		answers = question_data.get('answers', [])

		# if there are no answers, it's an information screen
		if not answers:
			return 'information'

		# get the question text to look for confirmation patterns
		question_text = question_data.get('question', '').lower()
		description = question_data.get('detail', '').lower() if question_data.get('detail') else ''

		# if there are exactly two options (yes/no, true/false), use tabs
		if len(answers) == 2:
			return 'tabs'

		# if there are multiple options (more than 2), use dropdown
		if len(answers) > 1:
			return 'dropdown'

		# check for confirmation patterns in question or description
		confirmation_keywords = ['agree', 'confirm', 'accept', 'acknowledge']
		if any(keyword in question_text for keyword in confirmation_keywords) or \
		   any(keyword in description for keyword in confirmation_keywords):
			return 'confirmation'

		return 'information'